<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[mysql——索引（三）]]></title>
    <url>%2F2020%2F11%2F02%2Fmysql%E2%80%94%E2%80%94%E7%B4%A2%E5%BC%95%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[mysql为什么有时候选错索引在mysql中一张表可以支持多个索引，但是你在写sql的时候并没有主动选择使用哪个索引，那是因为mysql的优化器帮你选择了。竟然这样，mysql的优化器在选择时是否会出错呢？答案是 —— 会。 选错索引的例子1让我们一起来看以下的例子，假如有一个简单的表，表里有a、b两个字段，并分别建上索引：12345678CREATE TABLE `t` ( `id` int(11) NOT NULL, `a` int(11) DEFAULT NULL, `b` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `a` (`a`), KEY `b` (`b`)) ENGINE=InnoDB; 然后，我们往表t中插入10万行记录，取值按整数递增，即：(1,1,1)，(2,2,2)，(3,3,3) 直到(100000,100000,100000)，插入的代码如下：123456789101112delimiter ;;create procedure idata()begin declare i int; set i=1; while(i&lt;=100000)do insert into t values(i, i, i); set i=i+1; end while;end;;delimiter ;call idata(); 接下来我们来分析这个sql：select * from t where a between 10000 and 20000; 你一定会说，这个语句还用分析吗，很简单呀，a上有索引，肯定是要使用索引a的，使用explain进行分析也确实如你所说：在刚才的基础上我们再进行如下操作，模拟不断地删除历史数据和新增数据的场景：遗憾的是，笔者按照以上的方式并没有复现出选错索引的场景。例子来源参考10讲MySQL为什么有时候会选错索引如果你复现了以上的场景，最终会发现在session B中explain显示rows大概为100000，即扫描了全表。 选错索引例子1补充如果你能复现出上面选错索引的场景，你执行：explain select * from t where a between 10000 and 20000;及explain select * from t force index(a) where a between 10000 and 20000;会显示：其中force index(a)表示让sql语句强制使用索引a。这时你一定会产生疑问：mysql为何放着扫描3w+行的方案不执行，反而去执行扫描10w+的方案呢？这是因为优化器觉得如果使用索引a，还要算上回表的代价，权衡之下优化器选择直接去主键id索引上执行。但是在还没有模拟不断地删除历史数据和新增数据之前，优化器是选择了索引a的（参考第一张图），这里是因为索引基数不准确导致的预计扫描行数不准确，引起的优化器选错了索引（从语句上看应该是扫描10001行，但最终显示了37116行）。 在知道因为索引基数不对导致选错索引的情况下，可以执行：analyze table t;重新统计索引基数： 以上的说明都是基于你已经复现出了mysql选错索引例子1的场景。 优化器逻辑选择索引是优化器的工作，优化器选择索引的目的，是为了找到一个最优的执行方案，并用最小的代价去执行语句。其中扫描行数是影响选择的因素之一。那么扫描行数是如何得到的呢？mysql在执行sql的时候，并不能精确地知道具体有多少行，只能根据统计的信息来估计记录数。这个统计的信息就是索引的区分度：一个索引上不同的值越多，这个索引的区分度就越好，而索引上不同值的个数就是索引基数，即基数越大区分度越好。 mysql如何统计索引基数的？如果将表一行行读取出来进行统计，虽然能获得精确的数据，但是这代价太大了，所以mysql使用的是采样统计。采样统计时，InnoDB会选取N个数据页，统计这些页上的不同值，获得一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。（所以页数增多会引起基数变大） 到此就可以解释另一个问题：模拟不断地删除历史数据和新增数据后，索引统计的基数为何增加了？mysql是使用标记删除来删除记录的,并不从索引和数据文件中真正的删除。在REPEATABLE-READ（可重复读）事务隔离级别下，session B要支持session A的可重复读，所以不能占用之前被删除的数据的空间，新插入的数据只能重新申请数据页空间，所以导致索引页面数的增加，导致统计的基数增大。 执行：show index from t; 可以显示表所有索引的基数： 当然，扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断，这里不再展开。 选错索引的例子2这里再补充一个因为排序引起的选错索引的场景：explain select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b limit 1;从你的角度看，优化器应该选择索引a，这样只需要扫描1000行就能查出结果了。但是上图明确指明了优化器偏偏选择了索引b，这里就是因为后面要对b进行排序，优化器认为选择索引b可以避免排序（因为B+树的有序性，索引b上逐个搜索出来的数据是自带排序的）。 这里将sql改写一下：explain select * from t where (a between 1 and 1000) and (b between 50000 and 100000) order by b,a limit 1;告诉优化器a也是需要排序的，这样扫描行数成了决策的唯一因素，优化器又重新选择了索引a： 索引选择异常和处理1.像我们第一个例子一样，采用force index强行选择一个索引。但是这样会侵入我们的代码，如果索引修改了，代码没有做相应的修改，sql执行会报错：ERROR 1176 (42000): Key ‘a’ doesn’t exist in table ‘t’。2.我们可以考虑修改语句，引导MySQL使用我们期望的索引，比如选错索引例子2。3.如果那个误选择的索引确实不需要，可以删除，当然这需要从业务上进行分析。 explain使用在验证上面的例子时，我们都通过explain命令来判断sql执行是否符合我们的预期。explain是mysql中用来分析sql语句执行效率的重要命令。从上图可以看到explain展示的信息很多，其中比较重要的是type，key以及Extra，可以作为判断sql执行是否高效的指标。对于type，官方文档中有如下的效率排序：NULL &gt; system &gt; const &gt; eq_ref &gt; ref &gt; fulltext &gt; ref_or_null &gt; index_merge &gt; unique_subquery &gt; index_subquery &gt; range &gt; index &gt; ALL。一般只要掌握其中常用的10种即可：NULL &gt; system &gt; const &gt; eq_ref &gt; ref &gt; ref_or_null &gt; index_merge &gt; range &gt; index &gt; ALL。 key代表的是sql执行时最终选择的索引，根据这个你可以判断mysql是否选错了索引。 Extra表示一些额外信息，比如是否用到临时内存，排序时是否用到临时文件等等。暂时从缺，还没有研究透。 SQL语句逻辑相同，性能却差异巨大有时候会遇到这样的情况：这个表明明有索引，但是执行sql时并没有按照我们预期的通过索引树进行查找。这里举例了3个在日常中容易被忽略的例子。 对条件字段进行了函数操作假如有表t1，并且在字符字段a上建立了索引：12345678CREATE TABLE `t1` ( `id` int(11) NOT NULL, `a` varchar(64) DEFAULT NULL, `b` varchar(64) DEFAULT NULL, `c` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `idx_a` (`a`)) ENGINE=InnoDB; 有时候为了满足业务的需求，我们可能会对查询字段做函数操作，理所当然的认为这样的查询方式也会走索引。比如查询出a的前3位等于’100’的记录，explain结果如下：从上图可以知道sql并没有走 idx_a 索引，而是进行了全表扫描。因为当你对查询条件应用函数时，mysql优化器认为substr(a,1,3)得出的结果在idx_a索引树上是无序的，因此放弃了走索引。 隐式类型转换有时候你的DBA或者组长告诉你：对字符串进行搜索时加上单引号，而不要单纯地写个数字。这里涉及到隐式类型转换：和java一样，当mysql发现where条件中字段的类型和条件值类型不符合时，会将其中一边进行类型转换。还是以表t1为例，分别执行以下sql：从上图中可以知道，mysql将字符串转换成了数字，再进行了比较。对于优化器而言：执行select * from t1 where a = 100;就相当于执行了select * from t1 where cast(a AS int) = 100; 正好符合了对条件字段进行了函数操作。接下来对同一个表中的字段c增加索引，并且执行：explain select * from t1 where c = ‘100’;从上图得到了不同的结果，优化器正确地选择了索引c。此时对于优化器而言相当于执行了select * from t1 where c = cast(‘100’ AS int); 函数时应用在条件值上的。所以：1.函数应用在where条件值上是没有问题的，但切忌应用在条件字段上2.int类型字段where条件使用字符串比较也是可以的 隐式字符编码转换使用join的方式，驱动表字段使用utf8，被驱动表使用utf8mb4。比如 select b.* from a,b where a.t = b.k and a.id = 10; 表a有索引t，表b有索引k，t是utf8，k是utf8mb4暂时没有复现出来]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql——索引（二）]]></title>
    <url>%2F2020%2F11%2F02%2Fmysql%E2%80%94%E2%80%94%E7%B4%A2%E5%BC%95%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[什么是回表在mysql——索引（一）中我们知道了索引的数据结构是B+树，要描述回表的过程，还得提前了解索引的数据结构。还是以索引(一)中的例子为例，假设，我们有一个主键列为id的表，表中有字段k，并且在k上有索引，建表语句如下：12345mysql&gt; create table t(id int primary key, k int not null, name varchar(16),index (k))engine=InnoDB; 表中row1~row5的(id,k)值仍然为(100,1)、(200,2)、(300,3)、(500,5)和(600,6)则基于主键索引和k值索引的结构图如下：回答什么是回表之前，我们来讨论一个问题：基于主键索引的查询和普通索引（此处是k值索引）的查询有什么区别？ 方式一：执行select * from t where id = 500，即为主键查询，则只需要搜索主键id这棵B+树。 方式二：执行select * from t where k = 5，即为普通索引查询，则先需要搜索k值B+树，得到id=500，再搜索主键id这颗B+树。 方式二中先到普通索引进行搜索获取主键id后，仍需要回到主键id这棵B+树进行搜索获取数据的行为就是回表。 为什么搜索完k值树后还要回主键id树呢？因为前面我们已经知道了：只有主键索引的叶子节点上存储了全量的数据，普通索引叶子节点上存储的是主键id。所以一般建议使用主键id查询效率会更高一些，毕竟少了回表的过程。 索引覆盖从上面描述我们知道：回表会影响查询的性能。如果将方式二的语句改写成：select id from t where k = 5，还需要进行回表吗？这里的答案是：不需要进行回表。因为k值搜索树中存储的是主键id，已经覆盖了查询所需的数据。我们再将上面的sql改写成以下3中方式，判断一下是否需要回表？ select id, k from t where k = 5; select k, name from t where k = 5; select id, name from t where k = 5;从索引覆盖的角度出发，第一种方式是不需要进行回表的，后面两种方式仍然需要回表，因为k值搜索树中只有k和id，并没有name数据。 索引覆盖：如果查询所需的数据在非主键索引中已经可以满足，就不用再回到主键id搜索树中获取额外数据。由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段。 最左前缀原则有索引的查询就可以防止全表扫描，所以理论上每一种查询都建立一个索引是最保险的。如果真的为每一种查询都设计一个索引，索引是不是太多了？实际上并不用为每一种查询设计索引，因为B+树这种索引结构，可以利用索引的“最左前缀”来定位记录。只要调整成合理的顺序，一个索引是可以应用在多种业务查询上的。为了直观地说明这个概念，我们用(a,b)这个联合索引来进行分析。假设，我们有一个表t1，表中有业务字段(a,b,c)，并且在(a,b)上有联合索引：12345678CREATE TABLE `t1` ( `id` int(11) NOT NULL, `a` varchar(64) DEFAULT NULL, `b` varchar(64) DEFAULT NULL, `c` int(11) DEFAULT NULL, PRIMARY KEY (`id`), INDEX `idx_a_b` (`a`,`b`)) ENGINE=InnoDB; 这里我们使用explain命令来分析不同的条件查询是否有使用到索引。 下面贴出explain执行的5种条件查询以及分析结果： 使用a和b联合查询 使用a等值查询 使用b等值查询 使用c等值查询 使用a模糊查询从以上的结果可知：最左前缀原则，既可以作用于联合索引的最左N个字段，也可以作用于字符串索引的最左M个字符。 如果将t1表看作一张业务表，那么很明显，根据字段a和b的联合查询以及单独根据字段a的查询一定是高频业务。这里就涉及到一个问题：在建立联合索引的时候，如何安排索引内的字段顺序？利用索引的最左前缀原则，我们可以制定如下标准：根据业务优先考虑索引的复用，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的。比如上面的例子，在得知字段a查询比字段b高频，所以将a放在联合索引前面，这样不用在给字段a单独建立索引。 索引下推对于联合索引的查询，mysql还引入了索引下推的优化，减少回表。（这里版本要&gt;=MySQL 5.6）还是以表t1举例，有这样的业务查询：select * from t1 where a like ‘a%’ and b = ‘b’;如果是小于MySQL 5.6的版本是这样处理的：1.从ab联合搜索树中找到第一个以a开头的元素存储的id的值。2.拿这个id去主键id搜索树中获取数据，并再根据后续条件进行筛选。3.重复第一步，直到不符合 a like ‘a%’为止。 如果是MySQL 5.6及以上版本会将这个过程优化：1.从ab联合搜索树中找到第一个以a开头的元素，并将条件b下推，直接进行筛选。2.重复第一步，直到不符合 a like ‘a%’为止。 假设t1表中存在大量字段a以a开头但是字段b!=’b’的数据，则需要进行大量回表。如果有了索引的下推，在ab联合搜索树中我们直接可以判断后续条件，不用进行任何的回表操作。 小结今天在索引数据结构的基础上，阐述了什么是回表，然后以减少回表为出发点，讲了索引覆盖和索引下推两种优化（前者是业务优化，后者是数据库自身优化）。中间还插入了索引的最左前缀原则以及建立索引的原则。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql——索引（一）]]></title>
    <url>%2F2020%2F10%2F29%2Fmysql%E2%80%94%E2%80%94%E7%B4%A2%E5%BC%95%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[InnoDB引擎下索引的数据结构给一张表创建索引的时候，mysql究竟在做什么？创建索引时，为什么表数据越大，执行时间越长？想要知道答案，就必须了解mysql中索引的数据结构。 InnoDB引擎下，索引其实是一棵B+树（这里明确指出是InnoDB，因为mysql支持多种存储引擎，比如还有MYISAM，Memory等。不同的存储引擎，索引的数据结构也是不同的。）那么B+树到底是怎样的数据结构呢？如下图: 一个节点中可以包含多个元素，所有的叶子节点位于同一层 每个父节点的元素都出现在子节点中，是子节点的最大或最小元素 其他上层节点不存储数据，只作索引，只在叶子节点存储数据 叶子节点和节点之间通过指针依次按从小到大顺序连接 InnoDB为何选择B+树存储引擎无非要提供存储和查询的功能，有很多的数据结构都具备这样的功能，为何选择B+树。想要解释这个问题，可以将B+树和其他的数据结构进行比较，阐述其他数据结构的缺点。 假设，你现在维护着一个身份证信息和姓名的表，需要根据身份证号查找对应的名字哈希表 通过hash算法得到不同的下标进行存储 hash出来的key可能相同，所以相同key值的value用链表串起来 链表中的value值是无序的哈希表对于数据存储是很快的（因为只要将前一个节点next指针指向插入节点，插入节点next指向前一个节点next先前指向的节点即可。），但对于范围查找，范围中的每一个值，哈希表都要扫描对应key的链表中全部的值（因为是无序的，即使查找到第一个不符合范围的value，也无法保证后面的value是否在范围内）。所以，哈希表这种结构适用于只有等值查询的场景。 有序数组相比于哈希表，有序数组对于等值查找或者范围查找的支持更加优秀。这里是假设身份证不会重复的情况下（即索引值不重复），如果索引值是可重复的，有序数组难以支持；为了维护索引有序性，有序数组在数据存储时要付出很大代价，比如在中间插入数据，需要移动所有后面的数组，效率很差。 平衡二叉树二叉树无论是查询还是存储效率都是非常高的，时间复杂度都是O(log(N))。mysql为何没有选择二叉树作为索引的数据结构？这里主要涉及到IO访问次数的问题。（索引不止在内存中，要持久化到磁盘上。）申请磁盘空间时都是以块申请的，所以平衡二叉树每多增加一层，就要开辟一块磁盘空间。由于二叉树每个节点下只能有两个元素的限制，导致数据量大时树会非常高（这样导致存储的数据块也比较多），这就导致在查询一个值时可能需要进行多次的IO访问（磁盘相比与内存，访问速度可是要慢的多）。 B树B树没有二叉树这样树会很高的问题，因为和B+树一样，B树也是支持一个节点下有多个元素。但是，B树可以在非叶子节点中存储数据，导致每个节点的大小就增大了，由于每次从磁盘读的数据量固定，所以IO的交互会增多；并且由于数据并非都在叶子节点上，无法用指针将所有数据都串起来，这样每次搜索都得从root树根开始。 到此，我们可以回答上面的第一个问题了：当你给一张表创建索引的时候，mysql实际上在给这个表创建一棵B+树。 索引的维护索引分为主键索引(聚簇索引)和非主键索引(二级索引)：主键索引的叶子节点存储的是整行的数据；非主键索引的叶子节点存储的是主键的值。假设，我们有一个主键列为id的表，表中有字段k，并且在k上有索引：12345mysql&gt; create table T(id int primary key, k int not null, name varchar(16),index (k))engine=InnoDB; 表中row1~row5的(id,k)值分别为(100,1)、(200,2)、(300,3)、(500,5)和(600,6)，两棵树的示例示意图如下： 建表规范中为何一定要有自增主键B+树为了索引的有序性，在插入新数据的时候需要做必要的维护。以上面这个图为例，如果新插入的ID值为400，相对麻烦，需要逻辑上挪动后面的数据，空出位置；更糟的，如果R5所在的数据页已经满了，根据B+树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去（页分裂）。这种情况下，性能自然会受到影响。除了性能外，页的分裂还会影响空间的利用率，原本一页的数据被分成了两页，整体空间利用率降低大约50%。当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程。 基于索引维护的过程，我们不难猜到：如果新增进去的数据主键都是单调递增的，就不会出现页分裂的情况，既提高了插入的效率，又让数据紧凑（即提高空间利用率）。而用业务上的字段做主键（比如身份证），则往往不容易保证有序插入（谁能保证在业务上后插入的身份证一定排在后面），但是自增的主键id就能保证。 再看另一个业务场景：假设你的表中确实有一个唯一字段，比如字符串类型的身份证号，那应该用身份证号做主键，还是用自增字段做主键呢？因为非主键索引叶子节点存储的是主键的值，如果使用身份证号作为主键，会增加非主键索引的存储（因为无论整型-int 还是长整型-bigint字节都比身份证号字符串小）。 所以建表时要使用自增主键，既是从性能出发，也是从存储成本出发。 小结今天主要讲了数据库索引的数据结构，介绍了InnoDB采用了B+树结构，然后从搜索，更新，IO访问等角度阐明了InnoDB为什么要这样选择。由于InnoDB是索引组织表（表都是根据主键顺序以索引的形式存放的），所以建议在创建表时一定要有自增主键。当你给一张表创建索引的时候，mysql实际上在给这个表创建一棵B+树；并且表的数据量越大，要创建的叶子节点就越多，申请的空间也越大，所以执行的也越慢。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm——虚拟机栈]]></title>
    <url>%2F2020%2F08%2F09%2Fjvm%E2%80%94%E2%80%94%E8%99%9A%E6%8B%9F%E6%9C%BA%E6%A0%88%2F</url>
    <content type="text"><![CDATA[基于栈的设计对于数据的操作，有基于cpu寄存器的设计和基于栈的设计两种。jvm为何选择了基于栈的设计呢？如果是基于cpu寄存器的设计，则不得不依赖底层的操作系统，这样就无法满足跨平台的特性。为了满足跨平台，基于栈的设计相比第一种(基于cpu寄存器的架构)性能是有所损失的。 虚拟机栈异常java虚拟机规范中对于虚拟机栈的错误定义了两种：StackOverflowError和OutOfMemoryError。栈如果设置的固定大小，当栈的深度超过了设置的大小，即抛第一种错误异常，一般发生在递归调用或死循环中。栈如果设置的动态可扩展，当栈深过深会申请内存，内存申请不到，即抛第二种错误异常。 栈帧虚拟机栈是线程私有的，基本单位是栈帧(stack frame)，由以下4个部分组成： 局部变量表 操作数栈 动态链接 方法返回地址 虚拟机栈只有入栈和出栈两种操作，jvm对于虚拟机栈没有垃圾回收操作。入栈相当于java代码中的方法调用，出栈相当于方法调用的结束。 局部变量表虚拟机栈中有一个或多个栈帧组成，而每个栈帧中都包含局部变量表。局部变量表本质上的数据结构是数组，其大小是在编译的时候就决定了，并且在运行期间不会改变(在idea中可以使用jclasslib插件查看，找到.class文件，View &gt; Show Bytecode With Jclasslib)。局部变量表用于存储方法的入参，定义在方法中的基本数据类型，引用类型以及方法返回的地址。局部变量表基本单位是slot，其中long和double占用了两个连续基本单位(在取数时使用下标小的取)。slot基本单位是可以重复利用的，如果一个本地变量在操作后不再使用，它在局部变量表中的位置可以被后续定义的变量替换。 操作数栈每个栈帧中也都包含一个操作数栈，其也是用数组实现的，其深度也在编译器的时候就决定了。在每个栈帧刚开始创建的时候，操作数栈数组是空的。和局部变量表一样，long和double占用了两个单位的深度，其他基本类型只占用一个。操作数栈也只有入栈和出栈两种行为，操作数栈将局部变量，字段中的常量或者值压入栈中，供jvm的其他指令取出(即出栈)进行操作(比如加减乘除)，然后将结果再次入栈。对于有返回值的方法，操作数栈会将最终的执行结果压入栈顶，传递给调用方。 动态链接每个栈帧都包含了运行时常量池的引用(每个class文件都有一份常量池，记录了class中定义的方法和变量的符号引用，并且在class加载后放到运行时常量池中)，动态链接用于在解析时将符号引用替换成直接的地址(关于符号引用需要看.class的字节码)。静态链接(早期绑定)，当一个字节码文件被加载进jvm内部时，如果被调用的目标方法在编译期可知，且运行期保持不变时，这种情况下将调用方法的符号引用转换为直接引用的过程称之为静态链接。动态链接(晚期绑定)，如果被调用的方法在编译期间无法被确定下来，也就是说，只能够在程序运行期将调用方法的符号引用转换为直接引用，由于这种引用转换过程具备动态性，因此也被称之为动态链接。 方法返回地址java虚拟机规范中，只规定了正常的方法返回（Normal Method Invocation Completion）和异常的方法返回（Abrupt Method Invocation Completion）两种。如果方法是正常返回的，存放该方法调用处的PC寄存器的值。如果方法是异常退出的，则通过异常表进行处理。]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm——PC寄存器]]></title>
    <url>%2F2020%2F07%2F26%2Fjvm%E2%80%94%E2%80%94PC%E5%AF%84%E5%AD%98%E5%99%A8%2F</url>
    <content type="text"><![CDATA[jvm运行时数据区jvm区域划分，主要分为5个重要组成部分 堆 java虚拟机栈 元空间(jdk7以前叫方法区) 本地方法栈 pc寄存器这边重点讲解pc寄存器 pc寄存器 概念jvm中pc寄存器，又叫程序计数器。不同于cpu中的寄存器用于存储计算的中间结果，pc寄存器用来存储下一条指令的地址。 生命周期pc寄存器是线程私有的，每个线程在创建时都会分配一个pc寄存器，所以pc寄存器的生命周期与线程绑定。 作用pc寄存器是程序控制流的指示器，用于分支，循环，跳转和异常处理以及线程恢复等功能。因cpu要不停地切换线程执行，pc寄存器正是记录了上次执行的地方。 异常规定由于pc寄存器只是存储下一行指令的地址，所以pc寄存器的内存空间可以忽略不计，所以pc寄存器是唯一没有指定oom异常的区域，并且不会被垃圾回收。 代码说明代码如下，定义了i和j，并相加赋给了k 命令行执行javap -v PcRegisterTest.class 查看字节码。其中左边序号是pc寄存器存储的地址，右边是jvm的指令，执行引擎会读pc寄存器中的地址(即下一个指令)翻译成机器语言，并交给cpu执行。]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm——类加载器]]></title>
    <url>%2F2020%2F07%2F05%2Fjvm%E2%80%94%E2%80%94%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%2F</url>
    <content type="text"><![CDATA[类的生命周期加载目的将.class文件导入到jvm中 流程 jvm读取.class文件的二进制文件 将class的文件描述存放到方法区中 同时生成一个唯一对应的java.lang.Class对象描述数据结构(jvm规范并没有明确规定该对象必须放在何处，主流的hotspot虚拟机是生成在堆上的，方法区存放的只是元数据描述) 加载方式 从本地的classpath路径下加载 从网络上下载.class文件加载 从.zip或.jar等压缩包中加载 动态加载.class文件(动态代理) 从特定的数据库加载.class文件，没有找到好的例子说明 最终产品位于堆中的class对象，该对象封装了类在方法区的数据结构，并且为程序员提供了访问方法区数据的接口，是一切反射的入口 连接验证 类文件的结构检查 语义上的检查 字节码检查 二进制兼容性检查 准备为类的静态变量分配内存，并初始化为默认的值(这里不是代码中赋予的值，比如int默认是0，boolean默认是false，引用类型默认是null)补充说明：jdk1.7及之前，静态变量分配内存在方法区，但是1.8之后为了配合永久代的移除，将静态变量分配在了Class对象中，即堆内存中 解析将代码中的符号连接转换成直接连接(地址) 初始化目的为类的静态变量赋予正确的初始值，即程序员代码中设置的值 概念jvm规范规定，所有java虚拟机必须在每个类或接口被java程序首次主动使用时初始化 类的使用 主动使用 实例化一个类，即new关键字调用一个类或接口的静态字段调用一个类的静态方法使用反射实例化一个子类，主动使用要求子类初始化时要求父类必须已初始化一个类是一个启动类，即含有main方法jdk1.7+中支持的动态语言的调用 被动使用 除以上7种以外的其他方式都属于被动使用 结论 对于静态变量的调用，只有直接定义了该静态变量的类才会被初始化 当jvm初始化一个类的时候，要求其父类已经初始化完毕，但这并不适用于接口 一个接口不要求其子接口或实现类在初始化时将其初始化 final修饰的常量如果是确定的值，在编译期就已经初始化并分配在了常量池，如果编译期无法确定则需要运行期初始化 假如一个类存在静态变量，jvm按照文件中定义的顺序将其初始化 以上证明放在文末 使用比如new一个对象 卸载暂时缺省 类加载器类型根加载器Bootstrap，属于jvm自带的加载器，无父类加载器；负责加载jvm的核心类库，如java.lang.*，Object类就是由根加载器加载进来的；实现依赖底层的操作系统，属于jvm实现的一部分，并不继承ClassLoader 扩展加载器Extension，父加载器是Bootstrap；加载java.ext.dirs指定目录下的类库，或者从jdk的安装目录jre/lib/ext下加载类库；ClassLoader的子类 系统/应用加载器System，父加载器是Extension；从classpath路径下加载类；是所有自定义加载器的默认父加载器 自定义加载器自定义加载器一定是属于ClassLoader的子类，其父类加载器默认是System 扩展知识类加载类加载器并不需要等某个类首次主动使用时再加载它。jvm规范允许类加载器在预料某个类将要被使用的时候就预先加载它，如果在预先加载的时候遇到错误（class文件丢失），类加载器必须在程序首次主动使用的时候报错，如果这个类一直没有被使用，那么不会报错 vm optionjvm参数形式只有3种： -XX: +option 开启option选项 -XX: -option 关闭option选项 -XX: option=value 设置option选项的值为value比如：设置-XX:+TraceClassLoading可以打印jvm加载类的情况 助记符.class文件字节码中java虚拟机指令，通过javap反编译即可看到 ldc，表示将int,float,String类型的常量值从常量池推送至栈顶 bipush，表示将单字节（-128～+127）的常量值推送至栈顶 sipush，表示将一个短整型（-32768～+32767）常量值推送至栈顶 iconst_1，表示将int类型1推送至栈顶（iconst_m1～iconst_5）jvm表示-1～5比较常用 代码证明结论1对于静态变量的调用，只有直接定义了该静态变量的类才会被初始化1234567891011121314151617181920212223242526272829public class MyTest1 &#123; public static void main(String[] args) &#123; System.out.println(MyChild1.STR2); &#125;&#125;class MyParent1 &#123; public static String STR = &quot;hello world&quot;; static &#123; System.out.println(&quot;MyParent1 static block&quot;); &#125;&#125;class MyChild1 extends MyParent1 &#123; public static String STR2 = &quot;welcome&quot;; static &#123; System.out.println(&quot;MyChild1 static block&quot;); &#125;&#125;执行结果：MyParent1 static blockMyChild1 static blockwelcome 12345678910111213141516171819202122232425262728public class MyTest1 &#123; public static void main(String[] args) &#123; System.out.println(MyChild1.STR); &#125;&#125;class MyParent1 &#123; public static String STR = &quot;hello world&quot;; static &#123; System.out.println(&quot;MyParent1 static block&quot;); &#125;&#125;class MyChild1 extends MyParent1 &#123; public static String STR2 = &quot;welcome&quot;; static &#123; System.out.println(&quot;MyChild1 static block&quot;); &#125;&#125;执行结果：MyParent1 static blockhello world 结论2当jvm初始化一个类的时候，要求其父类已经初始化完毕，但这并不适用于接口可以参考结论1当中的代码 结论3一个接口不要求其子接口或实现类在初始化时将其初始化123456789101112131415161718192021222324252627282930public class MyInterfaceInitTest &#123; public static void main(String[] args) &#123; System.out.println(MyClass.b); &#125;&#125;interface MyParent &#123; public static final Thread thread = new Thread() &#123; &#123; System.out.println(&quot;MyParent block&quot;); &#125; &#125;;&#125;interface MyInterface extends MyParent &#123; public static final Thread thread = new Thread() &#123; &#123; System.out.println(&quot;MyInterface block&quot;); &#125; &#125;;&#125;class MyClass implements MyInterface &#123; public static final int b = 5;&#125;执行结果：5 12345678910111213141516171819202122232425262728293031public class MyInterfaceInitTest &#123; public static void main(String[] args) &#123; System.out.println(MyInterface.thread); &#125;&#125;interface MyParent &#123; public static final Thread thread = new Thread() &#123; &#123; System.out.println(&quot;MyParent block&quot;); &#125; &#125;;&#125;interface MyInterface extends MyParent &#123; public static final Thread thread = new Thread() &#123; &#123; System.out.println(&quot;MyInterface block&quot;); &#125; &#125;;&#125;class MyClass implements MyInterface &#123; public static final int b = 5;&#125;执行结果：MyInterface blockThread[Thread-0,5,main] 证明4final修饰的常量如果是确定的值，在编译期就已经初始化并分配在了常量池，如果编译期无法确定则需要运行期初始化123456789101112131415161718public class MyTest3 &#123; public static void main(String[] args) &#123; System.out.println(MyParent3.STR); &#125;&#125;class MyParent3 &#123; public static final String STR = &quot;hello world&quot;; static &#123; System.out.println(&quot;MyParent3 static block&quot;); &#125;&#125;执行结果：hello world 执行后，删除MyParent3.class文件，再次执行结果一样。因为在编译期的时候，STR已经分配在了MyTest3类对应的java.lang.Class对象的常量池中，之后MyParent3.class文件存不存在就没有关系了。 证明5假如一个类存在静态变量，jvm按照文件中定义的顺序将其初始化12345678910111213141516171819202122232425262728public class MyTest6 &#123; public static void main(String[] args) &#123; System.out.println(&quot;count1:&quot; + Singleton.count1); System.out.println(&quot;count2:&quot; + Singleton.count2); &#125;&#125;class Singleton &#123; public static int count1; public static Singleton singleton = new Singleton(); private Singleton() &#123; count1++; count2++; // jvm准备阶段的意义，还没有初始化，即有了一个默认值 &#125; public static int count2 = 0; public static Singleton getInstance() &#123; return singleton; &#125;&#125;执行结果：count1:1count2:0]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何永久激活jetbrains产品]]></title>
    <url>%2F2020%2F05%2F25%2F%E5%A6%82%E4%BD%95%E6%B0%B8%E4%B9%85%E6%BF%80%E6%B4%BBjetbrains%E4%BA%A7%E5%93%81%2F</url>
    <content type="text"><![CDATA[免费教育许可证如果你拥有学校的注册的邮箱，你可以申请免费的激活码。参考 破解jar下载 先下载jetbrains产品，地址 再下载jetbrains-agent，地址 关于历史的版本对应，可能访问会比较慢，地址 破解实际很简单网上很多破解流程需要修改 *.vmoptions文件，配置-javaagent=jetbrains-agent下载的绝对路径，然后重新启动，输入激活码。但是往往网上的激活码都已经是不可用的。实际上破解很简单： 打开刚下载的idea，选择免费30天 直接将jetbrains-agent拖拽到idea中，会提示重新启动，直接启动即可。启动后会提示下载激活码，点击下载即可，再次重启旧成功了。]]></content>
      <categories>
        <category>jetbrains</category>
      </categories>
      <tags>
        <tag>破解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解java虚拟机——编译openjdk12]]></title>
    <url>%2F2020%2F05%2F21%2Fopenjdk%E2%80%94%E2%80%94%E7%BC%96%E8%AF%91openjdk12%2F</url>
    <content type="text"><![CDATA[环境准备和下载依赖最近看了《深入理解java虚拟机 第3版》，要继续阅读下去，需要下载jdk源码并编译。官方下载地址：，在书中也有提供，但是非常慢这边笔者推荐百度云盘下载：，提取码：c6mn 书中推荐在macOS或者Linux系统上安装编译，由于笔者是macOS，所以其他系统编译不涉及。 系统：MacBook Pro 10.14.6 依赖： 安装xcode，并安装Command Line Tools， 用于编译openjdk中c/c++实现 安装ccache，加速编译，使用brew安装(国内会有网络慢问题，参考##遇到的问题) 安装freetype，尽量upgrade到最新的版本 n-1版本的jdk，用于编译openjdk中java实现(这里n指将编译的openjdk版本)，所以这里要下载jdk11，下载地址 编译命令cd 到 openjdk12的目录，可以使用 bash configure –help 查看configure参数信息。参数说明：123456--with-debug-level=slowdebug 启用slowdebug级别调试--with-jvm-variants=server 编译server类型JVM--enable-ccache 启用ccache，加快编译--with-freetype=/usr/local/Cellar/freetype/2.10.1(你的版本)--disable-warnings-as-errors 忽略警告--with-boot-jdk=/Users/.../Downloads/openjdk11/Contents/Home 启动jdk的设置 有了以上的参数说明，我们可以使用以下命令验证当前的环境是否能编译成功：12&gt;&gt;bash configure --with-debug-level=slowdebug --with-jvm-variants=server --enable-ccache --with-freetype=/usr/local/Cellar/freetype/2.10.1 --with-boot-jdk=/Users/.../Downloads/openjdk11/Contents/Home --disable-warnings-as-errors按照书中执行会出现问题，参考##遇到的问题 如果能编译成功，会返回如下结果：1234567891011121314151617181920212223====================================================A new configuration has been successfully created in/Users/wyc/company/jdk12-06222165c35f/build/macosx-x86_64-server-slowdebugusing configure arguments &apos;--with-debug-level=slowdebug --with-jvm-variants=server --enable-ccache --with-freetype-include=/usr/local/Cellar/freetype/2.9.1/include --with-freetype-lib=/usr/local/Cellar/freetype/2.9.1/lib --with-boot-jdk=/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home --disable-warnings-as-errors&apos;.Configuration summary:* Debug level: slowdebug* HS debug level: debug* JVM variants: server* JVM features: server: &apos;aot cds cmsgc compiler1 compiler2 dtrace epsilongc g1gc graal jfr jni-check jvmci jvmti management nmt parallelgc serialgc services shenandoahgc vm-structs&apos; * OpenJDK target: OS: macosx, CPU architecture: x86, address length: 64* Version string: 12-internal+0-adhoc.wyc.jdk12-06222165c35f (12-internal)Tools summary:* Boot JDK: openjdk version &quot;11.0.7&quot; 2020-04-14 OpenJDK Runtime Environment AdoptOpenJDK (build 11.0.7+10) OpenJDK 64-Bit Server VM AdoptOpenJDK (build 11.0.7+10, mixed mode) (at /Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home)* Toolchain: clang (clang/LLVM from Xcode 11.3.1)* C Compiler: Version 11.0.0 (at /usr/bin/clang)* C++ Compiler: Version 11.0.0 (at /usr/bin/clang++)Build performance summary:* Cores to use: 4* Memory limit: 8192 MB* ccache status: Active (3.6) 同一个目录下，执行以下命令编译：1&gt;&gt;make images 如果编译成功，会返回如下结果：123456789101112131415......Creating support/demos/image/jfc/CodePointIM/CodePointIM.jarCreating support/demos/image/jfc/FileChooserDemo/FileChooserDemo.jarCreating support/demos/image/jfc/SwingSet2/SwingSet2.jarCreating support/demos/image/jfc/Font2DTest/Font2DTest.jarCreating support/demos/image/jfc/J2Ddemo/J2Ddemo.jarCreating support/demos/image/jfc/Metalworks/Metalworks.jarCreating support/demos/image/jfc/Notepad/Notepad.jarCreating support/demos/image/jfc/Stylepad/Stylepad.jarCreating support/demos/image/jfc/SampleTree/SampleTree.jarCreating support/demos/image/jfc/TableExample/TableExample.jarCreating support/demos/image/jfc/TransparentRuler/TransparentRuler.jarCreating jdk imageCreating CDS archive for jdk imageFinished building target &apos;images&apos; in configuration &apos;macosx-x86_64-server-slowdebug&apos; 遇到的问题1.brew install 时遇到网络很慢的问题，需要切源，这边推荐稳定的中科大源。参考2.bash configure 验证时出现第一个错误：123&gt;&gt;bash configure --with-debug-level=slowdebug --with-jvm-variants=server --enable-ccache --with-freetype=/usr/local/Cellar/freetype/2.9.1 --with-boot-jdk=/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home --disable-warnings-as-errors......&gt;&gt;configure: error: &apos;valid values for --with-freetype are &apos;system&apos; and &apos;bundled&apos; 需要将freetype分开成include和lib，即 –with-freetype=/usr/local/Cellar/freetype/2.9.1 修改成–with-freetype-include=/usr/local/Cellar/freetype/2.9.1/include –with-freetype-lib=/usr/local/Cellar/freetype/2.9.1/lib参考3.修改后 bash configure 验证时出现第二个错误：123&gt;&gt;bash configure --with-debug-level=slowdebug --with-jvm-variants=server --enable-ccache --with-freetype-include=/usr/local/Cellar/freetype/2.9.1/include --with-freetype-lib=/usr/local/Cellar/freetype/2.9.1/lib --with-boot-jdk=/Library/Java/JavaVirtualMachines/adoptopenjdk-11.jdk/Contents/Home --disable-warnings-as-errors......&gt;&gt;configure: error: Only bundled freetype can be specified on Mac and Windows 需要注释 openjdk12目录/make/autoconf/lib-freetype.m4中的一段程序：123456......#if (test &quot;x$FREETYPE_TO_USE&quot; = &quot;xsystem&quot;) &amp;&amp; \# (test &quot;x$OPENJDK_TARGET_OS&quot; = &quot;xwindows&quot; || test &quot;x$OPENJDK_TARGET_OS&quot; = &quot;xmacosx&quot;); then# AC_MSG_ERROR([Only bundled freetype can be specified on Mac and Windows])#fi...... 参考]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>深入理解java虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是缓存穿透，击穿，雪崩]]></title>
    <url>%2F2020%2F05%2F05%2F%E4%BB%80%E4%B9%88%E6%98%AF%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%2F</url>
    <content type="text"><![CDATA[缓存的一般操作方式针对热点访问的数据，如果每次都从数据库取数，并发量上来了会增加DB的压力，从而影响其他业务的数据访问。所以缓存应运而生，将这些热点的数据放在内存中，并设置失效时间，在有效的时间内直接返回内存中的数据不再从DB中取数，从而减轻DB的压力。 缓存引入业务的一般逻辑如下：外部传入的key是否存在缓存中，存在则直接返回，不存在则从DB中取数并放入缓存中，再返回。以上逻辑在特殊情况下存在问题。 缓存穿透针对检测传入的key是否存在缓存中的逻辑，如果外部传入的key一定不存在(即查询一个不存在的数据)会出现缓存穿透问题。攻击者可以拿着这个key不停的访问，进而不停访问DB，DB消耗大量的资源处理无意义的请求，进而影响整个系统。 解决方案：1.对于key做非法的校验，例如，业务中key一定是大于0的或者以xxx开头；2.对于非法的key，既使从DB取的数据为空，也将这个空的结果放入到缓存中。 缓存击穿针对同一个key，过了失效时间，如果此时系统有千万级的并发同时访问这个key会出现缓存击穿问题。千万级的访问会同时访问DB，瞬间增大DB压力甚至导致DB崩溃，进而影响其他业务。 解决方案：1.同步锁，从DB中取数增加同步锁，这样只有一个请求是访问的DB，其他访问仍然从缓存中取数，但需要设置等待，所以此方式存在性能问题。2.永不过期，业务上判断这个热点数据是否真的永不变更，如果是则不设置失效的事件，如果会变更将变更操作异步处理(即发现缓存即将过期，异步线程访问DB取数(这边还是要设置同步)重新设置缓存)。 缓存雪崩如果缓存数据的失效时间设置一样，过了失效时间，如果此时系统针对不同key的大量访问会出现缓存雪崩问题。不同key的请求会转到DB，同击穿一样，会在瞬时增加DB的压力。 解决方案：对于不同的key，失效时间设置合理的随机值，降低key失效时间的重复率，进而减少key集体失效的事件发生。]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重构-重构手法列表]]></title>
    <url>%2F2020%2F03%2F18%2F%E9%87%8D%E6%9E%84-%E9%87%8D%E6%9E%84%E6%89%8B%E6%B3%95%E5%88%97%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[重构技巧列表重新组织函数 技巧名称 说明 动机 备注 extract method 将代码放进独立的函数中，并让函数名解释该函数的用途 1.如果每个函数的粒度都很小，那么被复用的概率更大；2.高层的函数读起来像一系列注释(注释后的代码更容易提炼)；3.函数粒度小也更容易被覆写 提炼函数 inline method 函数本体和名称一样通俗易懂，在函数调用点插入函数本体 提取函数固然好，但非必要的间接性会增加阅读成本 内联函数 inline temp 将所有对某个临时变量的引用替换为对他赋值的查询语句(不适合耗费性能的操作) 多半可视为replace temp with query 的一部分使用；妨碍其他重构手法，使用内联 内联临时变量 replace temp with query 程序以一个临时变量来保存某一表达式的运行结果，将表达式提炼到独立函数中去，将这个临时变量替换为对新函数的调用 1.临时变量都是暂时的，所以只能在所属函数内使用；2.因为使用域的限制导致想要再次访问该临时变量则需要重复劳作；3.该技巧可以给 extract method 做准备 已查询取代临时变量 introduce explaining variable 将一个复杂表达式（或其中一部分）的结果放进一个临时变量，以此变量名称来解释表达式用途 1.表达式有可能非常复杂而难以阅读，临时变量可以将表达式分解为比较容易管理的形式；2.在条件语句中，可以用此重构手法将条件子句进行提炼 引入解释性变量 split temporary variable 你的程序有某个临时变量被赋值超过一次，它既不是循环变量也不是用于收集统计的，针对每次赋值创建一个独立变量 如果一个临时变量被赋值了多次，应该拆分成多个临时变量，确保只有一个单一职责；同一个临时变量承担多个职责会令阅读者糊涂 分解临时变量 remove assignments to parameters 代码对一个参数进行赋值，以一个临时变量取代该参数的位置 对参数进行赋值，会降低代码的清晰度 移除对参数的赋值 replace method with mehtod object 你有一个大型函数，其中对局部变量的使用使你无法采用extract method，将函数移动到一个类中，并将局部变量变成对象内字段，在同一个类中拆解函数 针对某个函数中局部变量泛滥成灾的现象 以函数对象取代函数 substitute algorithm 将函数本体替换成另一种算法 解决问题有好几种方法，解决一个问题有更清晰简单的方式时，替换成新的方法吧 替换算法 在对象之间搬移特性 技巧名称 说明 动机 备注 move method 有个函数与所驻类之外的另一个类交流更多，在最常引用该函数的类型建立一个行为一样的函数，将旧函数变成一个委托函数 重构理论的重要支柱，如果一个类的行为太多或者某个行为和另外的类高度耦合，那么这些行为就要考虑移动了（寻找这样的函数：使用另一个领域比使用所驻类领域对象还要多） 搬移函数 move field 程序中的某个字段被另一个类使用的次数更加频繁，将该字段搬移到使用次数更频繁的类中 随着系统的发展，你会发现自己需要新的类，并将现有的工作责任拖到新的类中 搬移字段 extract class 某个类做了两个类应该做的事情，新建一个类将字段和方法移动到新类中 一个类应该是清楚的抽象，处理一些明确的责任，随着系统的不断状态，类会变得过于复杂（先考虑职责，在将不同职责的字段和函数提炼到新的类中） 提炼类 inline class 某个类没有做太多的事情，将这种类所得的特性搬移到另一个类中 一个类不在承担足够的责任，不在有独立存在的理由（一开始庞大，可能由于重构导致单薄），将这个类的特性迁移到使用它最多的类中 将类内联化 hide delegate 客户通过一个委托类来调用另一个对象 调用客户不想因委托的变化而进行修改，在委托类上建立客户所需要的所有行数，客户可通过委托类调用另一个类 隐藏委托关系 remove middle man 某个类做了太多的简单委托，让客户直接调用受托类 针对 hide delegate 付出的代价，每当客户要使用受托类的新特性，我们不得不在中间服务中增加简单的函数，随着受托类越来越庞大，这一过程痛苦不堪 移除中间人 introduce local extension 你需要为服务提供一些额外函数，但是你无法修改这个类 继承这个类，并且利用父类的函数，提供额外的功能 引入本地扩展 重新组织数据 技巧名称 说明 动机 备注 self encapsulate field 即使在类内，为这个字段设置取值/设值方法，并以这些函数访问这些字段 截然不同的观点：1.类内可以自由访问，2.即使在类内也应该通过函数访问；先使用观点1，当访问存在装饰时使用观点2 自封装字段 replace data value with object 你有一个数据项，需要与其他数据和行为一起使用才有意义，将数据项变成对象 如果某个字段出现了特殊的行为，短时间可以分散在各个类中，但是坏味道从此开始，你需要将这个字段封装成类，将特殊的行为包装在内 以对象取代数据值 chanage value to reference 从一个类衍生出来的各种相等的实例，将他们替换成相同的对象 并不好区分，引用对象在系统中就是独一份的 将值对象改为引用对象 replace array to object 你有一个数组，其中的元素各自代表不同的东西 数组应该只用于以某种顺序容纳一组相似对象 以对象替换数组 duplicate observed data 有一些数据置于gui组件中，而领域函数需要访问这些数据，将数据复制一份到领域对象中，使用观察者模式更新这些数据 一个良好的系统，需要将界面数据和处理业务的代码分开，但有时候数据无法分割（比如基于swing的界面编程） 复制被监制的数据 replace magic number with sysbolic constant 魔法值，即有特殊意义，却又不能明确表现出这种意义的数字 以字面常量替换魔法值 replace type code with class 针对类型码数值的传参，编译器看到并进行检查的始终是背后的那个code，大大降低代码的可读性，从而成为bug之源 以类取代类型码 replace type code with subclass 有一个不可变的类型码，它会影响类的行为，以子类取代这个类型码 如果类型码不会影响行为，用上面方法（replace type code with class）即可；如果类型码会影响行为，最好是借助多态来处理变化行为 以子类取代类型码 replace type code with state/strategy 有一个影响类行为的类型码，在对象的生命周期中发生变化，无法通过继承去消除它 本重构和replace type code with subclass相似，但是类型码在对象生命周期中变化/宿主类无法被继承，可以使用该重构 以状态模式/策略模式替换类型代码 replace subclass with field 各个子类的唯一差别只在 返回常量函数 的差别上，在超类中增加一个相应的字段，然后销毁子类 增加子类的目的是为了增加新的特性/改变其行为，如果子类中只有常量函数，那么考虑去除这样的子类避免因继承带来的额外复杂性 以字段取代子类 简化条件表达式 技巧名称 说明 动机 备注 decompose conditional 你有一个复杂的条件语句，从段落中分别提炼出独立的函数 1.复杂的条件语句是导致复杂度上升的原因之一；2.针对不同的条件分支，我们可以写测试来执行不同的分支行为，但是不明白为什么这样执行；3.分解出独立的函数，并根据分支行为给函数命名，这样可以大大提升可读性 分解条件表达式 consolidate conditional expression 你有一系列条件表达式，都等到相同的结果 检查条件各不相同，但最终行为却一致，合并成一个条件并提炼成函数表达式 合并条件表达式 consolidate duplicate conditional fragments 在各条件分支中有着相同的一段代码 一组条件表达式的所有分支上都执行了同一段代码，将这段代码搬移到条件表达式外面 合并重复的条件片段 remove control flag 在一系列布尔表达式中，某个变量带有控制标记的作用（这个变量可以结束某个流程） java中可以使用continue/break语句替换 移除控制标记 replace nested conditional with guard clauses 函数中的条件逻辑使人难以看清正常的执行路径（条件语句嵌套） 条件表达有两种形式：1.条件语句所有分支都属于正常行为；2.条件中只有一种情况是正常行为，其他情况不常见；其中后者可以使用该重构 使用卫语句处理特殊情况 replace conditional with polymorphism 有个条件表达式，根据对象类型的不同而选择不同的行为 多态的存在可以使你不必编写明显的条件语句（但是继承也会增加复杂性，要预测行为是否经常增加减少来判断是否真的适合多态来替换） 以多态取代条件表达式 introduce assertion 某一段代码需要对程序状态作出某种假设 当条件为真时某段代码才能正常运行，引入断言来排除不正常的情况（先行判断不正常情况发生时是否确实当作异常处理） 引入断言 简化函数调用 技巧名称 说明 动机 备注 rename method 函数的名称未能揭示函数的用途 极力提倡的一种编程风格，将复杂的处理过程分解成小函数，但是如果处理不好使你费尽周折却弄不清楚这些小函数各自的职责，所以给函数一个通俗易懂的名字 函数改名 add parameter 某个函数的改动需要更多的信息 不多说 添加参数 remove parameter 某个函数已经不再使用某个参数 程序员不太愿意删除无用参数。千万不要打这样的如意算盘：多余的参数在程序中不会引发任何的问题，未来的时间可能还用得上 移除参数 separate query from modifier 某个函数既返回了对象状态值又修改了对象状态值 如果一个函数没有任何副作用，即只有单一职责的接口（对于某些业务查询和更新行为确实会存在并存），我们可以任意调用它而不用操心太多地方 分离查询函数和修改函数 paramaterize method 若干函数做了类似的工作 你可能会发现这样的函数：有着类似的行为，但是因为某些值导致行为些许的不同，将这些值作为参数传入合并这些行为类似的函数 令函数携带参数 replace paramater with explicit methods 有一个函数，行为完全取决与传入的参数 针对不同的分支行为建立不同的函数，调用方不用再考虑传入赋予参数什么值调用（与paramaterize method 恰恰相反） 以明确函数取代参数 preserve whole object 你从某个对象中取出若干值作为某个函数的参数 改用传整个对象，1.减少参数列表，长的参数列表也是降低代码可读性的祸首；2.函数需要多余信息时参数列表可以不用跟着变更； 保持对象完整 replace paramater with methods 对象调用某个函数，并将获得结果作为函数的参数 如果获取到的结果只是服务于该函数（后续程序中并没有使用到），那么去掉参数，函数内部直接调用前一个函数 以函数取代参数 introduce paramater object 某些参数总是自然的同时出现 如果某些参数总是出现的频率非常高，那么他们有可能是属于同一个类属性的，用对象包装它们 引入参数对象 hide methods 如果一个函数从来没有被其他类调用过 函数的可见度应该符合其作用域范围 隐藏函数 处理概括关系 技巧名称 说明 动机 备注 pull up field 两个子类拥有相同的字段 如果子类是分别开发的，或者在重构过程中组合起来的，可能存在相同的字段；1.字段名字相同，这种很好区分，2.字段的使用方式相同，这种需要观察字段在函数中的作用。既能够减少重复的字段，还可以将重复的行为提到超类中 字段上移 pull up method 有些函数在各个子类中产生相同的结果 避免重复行为是很重要的，虽然重复的行为也能照常运行，但是你就会面临修改了一个但是遗漏了另一个的风险 函数上移 push down method 某个超类中的函数只与部分的子类有关 – 函数下移 push down field 某个超类中的字段只与部分的子类有关 – 字段下移 extract subclass 类中的某些特性只被某些实例用到 类中的某些行为只被一部分实例用到，提炼子类也能更清晰地划分出业务行为 提炼子类 extract superclass 两个类有相似的特性 重复的代码是系统糟糕的东西。不同的地方重复同样的行为，你也会面临修改了一个但是遗漏了另一个的风险 提炼超类 extract interface 若干客户使用类接口的同一子集，或两个类实现的接口有相同的部分 – 提炼接口 collapse hierarchy 超类和子类无太大的区别 继承体系的存在可能使其变得过分复杂，如果一个继承体系没有存在的价值，合并是很好的处理方式 折叠继承体系 form template method 有若干子类，存在相似的行为，只是有些细节不相同 这个重构又是为了消除重复代码。使用模版方法模式，在超类中制定函数的统一行为，行为细节定义为抽象方法，子类各自实现 塑造模版方法 replace inheritance with delegation 某个子类只使用超类接口中的一部分，或者根本不需要继承而来的数据 子类可以只使用超类功能的一部分。但是这样代码传达的信息和你的意图南辕北辙；继承是有很好的东西，设计模式中强调多用包含关系，少用继承关系（继承体系庞大会造成难以增加新特性/行为） 以委托替代继承 replace delegation with inheritance 和上述重构手法相反 一般遇到某个类使用了受托类的所有函数，并且花了很大的力气编写了极其简单的委托函数 以继承替代委托 大型重构 技巧名称 说明 动机 备注 tease apert inheritance 少用继承，多用委托 庞大复杂的继承体系要小心（java 7 由于 Collections 复杂的继承体系，导致无法新增加特性，只能使用 default 关键字） 梳理并分解继承体系 converter procedural design to object – – 将过程化设计转为对象设计 separate domain from presentation mvc模式就是很好的例子，jsp就是反例 – 将领域对象和表述/显示分离]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven基本命令]]></title>
    <url>%2F2019%2F04%2F27%2Fmaven%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[命令介绍 clean validate compile test package verify install site deploy以上整体命令构成了maven的整个生命周期 创建一个简单的项目123#可以使用mvn命令创建一个简单的项目$mvn archetype:generate -DgroupId=cn.com.wyc -DartifactId=simple -DpackageName=cn.com.wyc.maven.test#可以使用开发集成idea创建简单的项目 mvn clean1234567891011121314151617#清理之前构件的工程，说白了就是删除target包下的内容#执行与显示结果$ mvn clean[INFO] Scanning for projects...[INFO] [INFO] ------------------------------------------------------------------------[INFO] Building maven-test 1.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ maven-test ---[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 0.355 s[INFO] Finished at: 2019-04-28T11:09:29+08:00[INFO] Final Memory: 6M/123M[INFO] ------------------------------------------------------------------------ mvn validate123456789101112131415161718192021222324252627282930313233#验证工程是否正确，所有需要的资源是否可用#这里我们自定义一个pom文件的错误&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.com.wyc&lt;/groupId&gt; &lt;artifactId&gt;maven-test&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;xxx&gt;&lt;/xxx&gt;&lt;/project&gt;#执行结果如下：$ mvn validate[INFO] Scanning for projects...[ERROR] [ERROR] Some problems were encountered while processing the POMs:[ERROR] Malformed POM /Users/wyc/company/project/xqy-tax-11-29/maven-test/pom.xml: Unrecognised tag: &apos;xxx&apos; (position: START_TAG seen ...&lt;/version&gt;\n\n &lt;xxx&gt;... @11:10) @ /Users/wyc/company/project/xqy-tax-11-29/maven-test/pom.xml, line 11, column 10 @ [ERROR] The build could not read 1 project -&gt; [Help 1][ERROR] [ERROR] The project cn.com.wyc:maven-test:1.0-SNAPSHOT (/Users/wyc/company/project/xqy-tax-11-29/maven-test/pom.xml) has 1 error[ERROR] Malformed POM /Users/wyc/company/project/xqy-tax-11-29/maven-test/pom.xml: Unrecognised tag: &apos;xxx&apos; (position: START_TAG seen ...&lt;/version&gt;\n\n &lt;xxx&gt;... @11:10) @ /Users/wyc/company/project/xqy-tax-11-29/maven-test/pom.xml, line 11, column 10 -&gt; [Help 2][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException[ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/ModelParseException#一般可以用mvn validate验证pom文件是否正确 mvn compile123456789101112131415161718192021222324#测试执行结果如下：$mvn compile[INFO] Scanning for projects...[INFO] [INFO] ------------------------------------------------------------------------[INFO] Building maven-test 1.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ maven-test ---[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent![INFO] Copying 0 resource[INFO] [INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ maven-test ---[INFO] Changes detected - recompiling the module![WARNING] File encoding has not been set, using platform encoding UTF-8, i.e. build is platform dependent![INFO] Compiling 1 source file to /Users/wyc/company/project/xqy-tax-11-29/maven-test/target/classes[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 1.798 s[INFO] Finished at: 2019-04-28T11:21:24+08:00[INFO] Final Memory: 13M/165M[INFO] ------------------------------------------------------------------------#BUILD SUCCESS说明编译成功了，将 项目路径/src 中的源码文件和配置文件 编译并生成到了 项目路径/target/ 下 mvn test1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#执行 项目路径/src/test中的单元测试$ mvn test[INFO] Scanning for projects...[INFO] [INFO] ------------------------------------------------------------------------[INFO] Building maven-test 1.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ maven-test ---[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent![INFO] Copying 0 resource[INFO] [INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ maven-test ---[INFO] Nothing to compile - all classes are up to date[INFO] [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ maven-test ---[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent![INFO] skip non existing resourceDirectory /Users/wyc/company/project/xqy-tax-11-29/maven-test/src/test/resources[INFO] [INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ maven-test ---[INFO] Changes detected - recompiling the module![WARNING] File encoding has not been set, using platform encoding UTF-8, i.e. build is platform dependent![INFO] Compiling 1 source file to /Users/wyc/company/project/xqy-tax-11-29/maven-test/target/test-classes[INFO] [INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ maven-test ---[INFO] Surefire report directory: /Users/wyc/company/project/xqy-tax-11-29/maven-test/target/surefire-reportswycdeMacBook-Pro:maven-test wyc$ mvn test[INFO] Scanning for projects...[INFO] [INFO] ------------------------------------------------------------------------[INFO] Building maven-test 1.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ maven-test ---[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent![INFO] Copying 0 resource[INFO] [INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ maven-test ---[INFO] Nothing to compile - all classes are up to date[INFO] [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ maven-test ---[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent![INFO] skip non existing resourceDirectory /Users/wyc/company/project/xqy-tax-11-29/maven-test/src/test/resources[INFO] [INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ maven-test ---[INFO] Nothing to compile - all classes are up to date[INFO] [INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ maven-test ---[INFO] Surefire report directory: /Users/wyc/company/project/xqy-tax-11-29/maven-test/target/surefire-reports------------------------------------------------------- T E S T S-------------------------------------------------------Running cn.com.wyc.MavenTestTestTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.103 secResults :Tests run: 1, Failures: 0, Errors: 0, Skipped: 0[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 1.843 s[INFO] Finished at: 2019-04-28T11:32:58+08:00[INFO] Final Memory: 10M/220M[INFO] ------------------------------------------------------------------------#集成测试可以用来验证代码逻辑的修改是否存在影响 mvn package1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#对项目进行打包，jar或者war，具体在pom文件中进行说明$ mvn package[INFO] Scanning for projects...[INFO] [INFO] ------------------------------------------------------------------------[INFO] Building maven-test 1.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ maven-test ---[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent![INFO] Copying 0 resource[INFO] [INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ maven-test ---[INFO] Nothing to compile - all classes are up to date[INFO] [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ maven-test ---[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent![INFO] skip non existing resourceDirectory /Users/wyc/company/project/xqy-tax-11-29/maven-test/src/test/resources[INFO] [INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ maven-test ---[INFO] Nothing to compile - all classes are up to date[INFO] [INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ maven-test ---[INFO] Surefire report directory: /Users/wyc/company/project/xqy-tax-11-29/maven-test/target/surefire-reports------------------------------------------------------- T E S T S-------------------------------------------------------Running cn.com.wyc.MavenTestTestTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.09 secResults :Tests run: 1, Failures: 0, Errors: 0, Skipped: 0[INFO] [INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ maven-test ---[INFO] Building jar: /Users/wyc/company/project/xqy-tax-11-29/maven-test/target/maven-test-1.0-SNAPSHOT.jar[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 2.227 s[INFO] Finished at: 2019-04-28T11:36:28+08:00[INFO] Final Memory: 11M/220M[INFO] ------------------------------------------------------------------------#打包命令，看日志可以发现需要执行验证，编译，测试等过程#有时候测试的会一直过不了，打包时你可能需要跳过测试，可以执行如下命令跳过$mvn package -DskipTests或$mvn package -Dmaven.test.skip=true mvn install123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#将jar包上传到本地仓库$ mvn install[INFO] Scanning for projects...[INFO] [INFO] ------------------------------------------------------------------------[INFO] Building maven-test 1.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ maven-test ---[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent![INFO] Copying 0 resource[INFO] [INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ maven-test ---[INFO] Nothing to compile - all classes are up to date[INFO] [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ maven-test ---[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent![INFO] skip non existing resourceDirectory /Users/wyc/company/project/xqy-tax-11-29/maven-test/src/test/resources[INFO] [INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ maven-test ---[INFO] Nothing to compile - all classes are up to date[INFO] [INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ maven-test ---[INFO] Surefire report directory: /Users/wyc/company/project/xqy-tax-11-29/maven-test/target/surefire-reports------------------------------------------------------- T E S T S-------------------------------------------------------Running cn.com.wyc.MavenTestTestTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.109 secResults :Tests run: 1, Failures: 0, Errors: 0, Skipped: 0[INFO] [INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ maven-test ---[INFO] [INFO] --- maven-install-plugin:2.4:install (default-install) @ maven-test ---[INFO] Installing /Users/wyc/company/project/xqy-tax-11-29/maven-test/target/maven-test-1.0-SNAPSHOT.jar to /Users/wyc/company/xxxx/cn/com/wyc/maven-test/1.0-SNAPSHOT/maven-test-1.0-SNAPSHOT.jar[INFO] Installing /Users/wyc/company/project/xqy-tax-11-29/maven-test/pom.xml to /Users/wyc/company/xxxx/cn/com/wyc/maven-test/1.0-SNAPSHOT/maven-test-1.0-SNAPSHOT.pom[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 2.311 s[INFO] Finished at: 2019-04-28T11:40:01+08:00[INFO] Final Memory: 11M/155M[INFO] ------------------------------------------------------------------------ mvn site1#生成站点报告 mvn deploy1#将生成的jar上传到本地仓库和远程仓库]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven-pom文件]]></title>
    <url>%2F2019%2F04%2F27%2Fmaven-pom%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[超级pom 超级pom 所有的maven项目的pom文件都扩展自超级pom。超级pom定义了一组被所有项目共享的默认设置。你可以在maven安装路径的lib目录的maven-model-builder.jar中找到 最简单的pom123456789101112#初始化的maven项目pom.xml内容如下：&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.com.wyc&lt;/groupId&gt; &lt;artifactId&gt;maven-test&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;&lt;/project&gt; pom语法项目坐标 groupId 定义了项目属于哪个组，这个组往往和项目所在的组织或者公司存在关联，比如cn.com.servyou artifactId 定义了maven项目在组中的唯一的id，可以认为是组中子项目唯一标识符 version 指定了子项目的当前版本 packaging 指定了项目的打包方式，默认情况下打成jar包 classifier 用来定义构件输出的一些附属构件，因为附属构件拥有相同的groupId，artifactId，version，需要和真正的项目构件区分开 添加依赖123456789#比如增加junit的依赖&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/&lt;dependency&gt;&lt;/dependencies&gt;#其实就是将第三方项目的坐标添加到&lt;dependency&gt;&lt;/&lt;dependency&gt;标签下 依赖排除123456789101112#依赖的jar还会依赖其他的jar，有时候会产生冲突，需要排除对其对依赖&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;排除项目的groupId&lt;/groupId&gt; &lt;artifactId&gt;排除项目的artifactId&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/&lt;dependency&gt; 依赖范围依赖的范围大致分成以下5种： compile - 编译依赖范围 test - 测试依赖范围 provided - 已提供依赖范围 runtime - 运行时依赖范围 system - 系统依赖范围依赖范围会对编译，测试，运行时的classpath产生影响： 依赖范围（scope） 对于编译classpath有效 对于测试classpath有效 对于运行时classpath有效 例子 compile(默认) Y Y Y Spring-core test – Y – Junit provided Y Y – servlet-api runtime – Y Y JDBC驱动实现 system Y Y – 本地的，Maven仓库之外的类库文件 依赖范围还会对传递性依赖产生影响： compile test provided runtime compile compile — — runtime test test — — test provided provided — provided provided runtime runtime — — runtime 1234567#比如对于Junit的依赖只针对测试的classpath：&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; pom文件继承和java中的继承一样，pom文件也可以继承：1234567&lt;parent&gt; &lt;groupId&gt;父模块groupId&lt;/groupId&gt; &lt;artifactId&gt;父模块artifactId&lt;/artifactId&gt; &lt;version&gt;父模块version&lt;/version&gt; &lt;relativePath&gt;相对路径&lt;/relativePath&gt;&lt;/parent&gt;#其中父模块packaging参数必须为pom，由于父模块只是为了消除配置的重复，因此也就不需要src/main/java等目录了 模块聚合当一个项目有多个模块的时候，我们需要到各个模块去执行mvn命令进行构建，如何才能在一个模块下构建所有的模块呢？模块聚合配置就是为了解决这个问题：123456&lt;modules&gt; &lt;module&gt;模块1 name&lt;/module&gt; &lt;module&gt;模块2 name&lt;/module&gt; ....&lt;/modules&gt;#该配置配置在parent pom文件中，只需对parent模块进行maven构建等操作，maven会根据modules配置计算出构建的反应堆，分别进行各个模块的构建 统一的版本管理项目多模块的情况下，每个模块可能会依赖同一个jar包，为了防止版本的泛滥，需要有一个版本管理：12345678910111213141516&lt;properties&gt; &lt;junit.version&gt;4.0.0-RELEASE&lt;/tax.common.version&gt; ....&lt;/properties&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;$&#123;junit.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; .... &lt;dependencies&gt;&lt;dependencyManagement&gt;#maven使用dependencyManagement标签来约束依赖的使用，在dependencyManagement声明的依赖不会实际引入，即能让子模块继承到父模块的依赖配置，又能保证子模块使用的灵活性]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven自定义插件]]></title>
    <url>%2F2019%2F04%2F27%2Fmaven%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[编写插件代码 创建一个maven工程 打包方式配置成maven-plugin 1&lt;packaging&gt;maven-plugin&lt;/packaging&gt; 增加maven插件接口的依赖 12345678910111213&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.maven&lt;/groupId&gt; &lt;artifactId&gt;maven-plugin-api&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.maven.plugin-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-plugin-annotations&lt;/artifactId&gt; &lt;version&gt;3.4&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建插件类，继承AbstractMojo类 重写execute方法 添加标签@Mojo(name = “自定义名称”) 引入插件另外创建一个工程，在pom文件中绑定自定义的插件：12345678910111213141516171819&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;cn.com.wyc&lt;/groupId&gt; &lt;artifactId&gt;my-plugin&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;configuration/&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;clean&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;sayHello&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt;#即添加插件的坐标，配置插件需要绑定的生命周期阶段及插件的目标 插件注释 defaultPhase 默认绑定的生命周期阶段，pom文件不配置，则使用默认配置 requiresOnline :是否要求maven必须是在线状态，默认值为false requiresDirectInvocation :为true时，该目标就只能通过命令行直接调用。默认为false requiresDependencyResolution :在运行mojo之前必须解析所有指定范围的依赖，如maven-surefire-plugin的test目标带有requiresDependencyResolution test标注，表示执行测试前，所有测试范围的依赖必须得到解析 requiresDependencyCollection instantiationStrategy executionStrategy requiresProject :该目标是否必须在一个maven项目中运行（如测试插件用于测试其他项目），默认为true requiresReports :是否要求项目报告已经生成，默认为false aggregator 当mojo在多模块项目上运行时，该标注表示目标只会在顶层模块运行 inheritByDefault configurator threadSafe]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven生命周期]]></title>
    <url>%2F2019%2F04%2F27%2Fmaven%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[生命周期在maven出现之前，项目构建的生命周期就已经存在了。你可以把他想象成项目构建的一套顺序流程；比如有些是手动进行构建，有些是编写自动化脚本，大家也是各写各的，能满足自身的项目即可，换个项目就需要重头再来maven将这套生命周期抽象化，总结了一套高度完善，易扩展的生命周期可以把他想象成设计模式——模版方法模式，maven只定义了生命周期的算法，本身的工作交由插件来完成，子类可以通过重写其中的方法来控制实际的行为，这样即满足了扩展性，又能够严格控制生命周期的整体结构 三套生命周期maven有三套相互独立的生命周期： clean生命周期 pre-clean 执行清理前需要完成的工作 clean post-clean 执行清理后需要完成的工作 default生命周期 validate initialize generate-source process-source 处理项目主资源文件 generate-resource process-resource compile 编译项目的主源码 process-classes generate-test-source process-test-source generate-test-resource process-test-resource test-compile 编译项目的测试代码 process-test-classes test 使用单元测试框架运行测试 prepare-package package 接受编译好的代码，打包成可发布的格式 pre-integration-test integration-test post-integration-test verify install 安装到本地仓库 deploy 将最终的包复制到远程的仓库 sit生命周期 pre-site 执行一些在生成项目站点之前需要完成的工作 site 生成项目的站点文档 post-site 执行一些在生成项目站点之后需要完成的工作 site-deploy 将生成的项目站点发布到服务器上 命令行和生命周期 $mvn clean 实际执行了clean生命周期的pre-clean和clean阶段 $mvn test 实际执行了default生命周期的validate，initialize等，直到test $mvn clean package 可以联合执行两种类型的生命周期命令行和生命周期不是所有阶段都绑定的，比如$mvn clean不会执行post-clean阶段 内置绑定maven的生命周期与插件相互绑定，用以完成实际的构建任务，具体而言就是生命周期的阶段与插件的目标相互绑定： clean maven-clean-plugin:clean site maven-site-plugin:site compile maven-compile-plugin:compile….]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven安装]]></title>
    <url>%2F2019%2F04%2F27%2Fmaven%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[基于zip压缩包安装直接官网上下载zip解压即可使用 配置环境变量123456789#使用maven之前，你必须得有java运行环境(具体安装不进行展开)#编辑当前用户的环境配置$vim /etc/profile#添加：export PATH=maven安装目录:$PATH#比如：export PATH=/Users/wyc/company/apache-maven-3.3.9/bin:$PATH#运行：mvn --version#检测安装是否成功 maven配置maven默认本地仓库路径：～/.m2/repositorymaven配置文件路径：maven安装目录/conf/setting.xml正如前面所说的，maven可以在配置文件setting.xml中更改本地库的配置：1&lt;localRepository&gt;自定义的路径&lt;/localRepository&gt; setting.xml中的配置项简单项 localRepository - 本地仓库配置 interactiveMode - 交互模式，默认是true，需要和用户交互来获取输入 usePluginRegistry - 是否使用maven默认的文件来管理插件版本，默认值是false offline - 构建系统是否需要在离线情况下运行，默认是false pluginGroups - 插件组，使用某个插件，但在命令行中没有提供groupId是maven会使用该列表中的插件组服务器12345678910#基本配置如下：&lt;servers&gt; &lt;server&gt; &lt;id&gt;server001&lt;/id&gt; &lt;username&gt;my_login&lt;/username&gt; &lt;password&gt;my_password&lt;/password&gt; &lt;/server&gt; &lt;/servers&gt;#server配置中的id与pom文件中distributionManagement中仓库配置的id一致#username/password分别是登录仓库所需的账号和密码 镜像12345678910111213#基本配置如下：&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;planetmirror.com&lt;/id&gt; &lt;name&gt;PlanetMirror Australia&lt;/name&gt; &lt;url&gt;http://downloads.planetmirror.com/pub/maven2&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;/mirror&gt; &lt;/mirrors&gt;#id表示镜像的唯一标识符#url表示镜像的地址，比如阿里的镜像：http://maven.aliyun.com/nexus/content/groups/public/#mirrorOf表示被镜像的服务器id，这边表示被镜像的是中央仓库#配置该配置，maven构建时会优先考虑从镜像下载构件 Profiles - 属性123456#基本配置如下：&lt;properties&gt; &lt;aaa&gt;1&lt;/aaa&gt; &lt;bbb&gt;2&lt;/bbb&gt;&lt;/properties&gt;#其他上下文可以通过$&#123;aaa&#125;获取属性的值，用法和pom文件中的用法一致 Profiles - 仓库和插件仓库123456789101112131415161718基本配置如下：&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;codehausSnapshots&lt;/id&gt; &lt;name&gt;Codehaus Snapshots&lt;/name&gt; &lt;releases&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;warn&lt;/checksumPolicy&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;never&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt; &lt;/snapshots&gt; &lt;url&gt;http://snapshots.maven.codehaus.org/maven2&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;/repository&gt; &lt;/repositories&gt; releases, snapshots 这里配置了两种构件，发布版（Release）和快照版（Snapshot）的策略。有了 这两组配置，POM就可以在每个单独的仓库中，为每种类型类型的构件采取不同 的策略。例如，可能有人会决定只为开发的目的开启对快照版本下载的支持 enabled true或者false表示该仓库是否为某种类型构件（发布版或者快照版）开启 updatePolicy 该元素指定更新发生的频率。Maven会比较本地POM和远程POM的时间戳。这里的 选项是：always（一直），daily（默认，每日），interval：X（这里X是以分 钟为单位的时间间隔），或者never（从不） checksumPolicy 当Maven将构件部署到仓库中时，它也会部署对应的校验和文件。当没有校验和 文件，或者该文件不正确时，你的选项有ignore（忽略），fail（失败），或者 warn（警告） layout 在上面的仓库描述中，它们都遵循一个共同的布局。大部分情况都是这样。Maven 2为其仓库提供了一个默认的布局；然而，Maven 1.x有一种不同的布 局。我们可以使用该元素指定布局是default（默认）还是legacy（遗留） maven仓库的概念笼统的说，maven仓库大致可以分为以下4种： 本地仓库 —— 用来服务本地所有pom工程 私服 —— 最典型的例子，nexus，用于服务局域网中的pom工程 中央仓库 —— 世界各地所有的pom工程 仓库镜像 —— 为了减小中央仓库的压力，和加快各地下载的速度]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven]]></title>
    <url>%2F2019%2F04%2F27%2Fmaven%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[maven的定义开发：maven是一个用来把源代码构建成可发布的构件的工具。项目经路：maven是一个管理工具。12一个更正式的 Apache Maven 的定义： Maven是一个项目管理工具，它包含了一个项 目对象模型 (Project Object Model)，一组标准集合，一个项目生命周期(Project Lifecycle)，一个依赖管理系统(Dependency Management System)，和用来运行定义在 生命周期阶段(phase)中插件(plugin)目标(goal)的逻辑。 约定优于配置系统，类库，框架应该假定合理的默认值，而非要 求提供不必要的配置。举个简单的例子：我们一般不太喜欢这样的软件，运行时提示该文件夹不存在，而更青睐于即使不存在也自动帮我们创建好的软件。 Maven 对约定优于配置的应用不仅仅是简单的默认配置，Maven 的核心插件使用了一组通用的约定，以用来编译源代码，打包可分发的 构件，生成 web 站点，还有许多其他的过程。 Maven 的力量来自它的”武断”，它有 一个定义好的生命周期和一组知道如何构建和装配软件的通用插件。如果你遵循这些约 定，Maven 只需要几乎为零的工作——仅仅是将你的源代码放到正确的目录，Maven 将会帮你处理剩下的事情。总结就是，你得按照规范做。 规范虽然往往让人有一种强迫的感觉，但是maven还是开放了一些配置增加自由度，比如你可以修改仓库的位置，可以自定义插件执行。 插件的全局重用当你创建一个maven工程的时候，在工程目录下默认生成pom.xml文件：12345678910111213初始化的pom文件格式统一如下：&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;自定义的groupId&lt;/groupId&gt; &lt;artifactId&gt;自定义的artifactId&lt;/artifactId&gt; &lt;version&gt;版本号&lt;/version&gt;&lt;/project&gt; 没有配置任何插件，你就可以对工程进行编译和构建了，因为4.0.0使得该工程已经去下载了maven编译构建等所需要的插件了，并且这个插件是全局重用的，另外的本地所有的maven项目都可以使用这些插件 比较maven和antant的build.xml构建配置如下：12345678910111213141516171819202122232425&lt;project name=&quot;my-project&quot; default=&quot;dist&quot; basedir=&quot;.&quot;&gt; &lt;description&gt; simple example build file &lt;/description&gt; &lt;!-- set global properties for this build --&gt; &lt;property name=&quot;src&quot; location=&quot;src/main/java&quot;/&gt; &lt;property name=&quot;build&quot; location=&quot;target/classes&quot;/&gt; &lt;property name=&quot;dist&quot; location=&quot;target&quot;/&gt; &lt;target name=&quot;init&quot;&gt; &lt;!-- Create the time stamp --&gt; &lt;tstamp/&gt; &lt;!-- Create the build directory structure used by compile --&gt; &lt;mkdir dir=&quot;org.apache.maven.model.Build@d7e661&quot;/&gt; &lt;/target&gt; &lt;target name=&quot;compile&quot; depends=&quot;init&quot; description=&quot;compile the source &quot; &gt; &lt;!-- Compile the java code from $&#123;src&#125; into org.apache.maven.model. &lt;javac srcdir=&quot;$&#123;src&#125;&quot; destdir=&quot;org.apache.maven.model.Build@d7e661 &lt;/target&gt; &lt;target name=&quot;dist&quot; depends=&quot;compile&quot; description=&quot;generate the distribution&quot; &gt; &lt;!-- Create the distribution directory --&gt; &lt;mkdir dir=&quot;$&#123;dist&#125;/lib&quot;/&gt; &lt;!-- Put everything in org.apache.maven.model.Build@d7e661 into the &lt;jar jarfile=&quot;$&#123;dist&#125;/lib/MyProject-$&#123;DSTAMP&#125;.jar&quot; basedir=&quot;org.apa &lt;/target&gt; &lt;target name=&quot;clean&quot; description=&quot;clean up&quot; &gt; &lt;!-- Delete the org.apache.maven.model.Build@d7e661 and maven的构建配置如下：12345&lt;project&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;org.sonatype.mavenbook&lt;/groupId&gt; &lt;artifactId&gt;my-project&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/project&gt; 以上两个功能都是一样的，明显发现，ant构建方式虽然更有自由度，但是相比maven更加繁琐。]]></content>
      <categories>
        <category>maven</category>
      </categories>
      <tags>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git分支]]></title>
    <url>%2F2019%2F04%2F26%2Fgit%E5%88%86%E6%94%AF%2F</url>
    <content type="text"><![CDATA[基本操作12345678910111213141516#查看分支$git branch -v#查看分支，并显示与远程的映射$git branch -vv#创建分支，并提交$git branch &lt;分支名&gt;$git push --set-upstream origin &lt;远程分支名&gt;#切换分支$git checkout &lt;分支名&gt;#合并分支git checkout &lt;被合并的分支&gt;git merge &lt;分支名&gt; 高效的多分支推进传统的瀑布开发模式无法快速占据市场，目前的敏捷开发模式才能快速地占据市场，拥抱变化，敏捷开发模式需要同样敏捷的分支管理正如git的图标所示，git有高效的分支管理技术12345678910#用实际的迭代场景来说明分支的高效性1.假如产品在01.01号计划要上线一个功能——程序员的咸鱼一天，预计02.14号上线2.项目经理从master上branch了一个分支——2.1.43.开发开始在2.1.4 branch上进行研发4.之后产品计划又要上线一个新功能——代码没写完无法咸鱼，预计02.08号上线5.项目经理从master上又branch了一个分支——2.0.86.开发在2.0.8上研发，并于02.08号成功上线7.运维将代码merge到master分支上，通知代码合并完了8.开发将 代码没写完无法咸鱼 功能从master上merge到2.1.4 branch上9.02.14号 代码没写完无法咸鱼，程序员的咸鱼一天上线 上面是一个真实的迭代场景，理论上在01.01——02.14之间，我们发布无数个除 程序员的咸鱼一天 的其他功能]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git撤销操作]]></title>
    <url>%2F2019%2F04%2F25%2Fgit%E6%92%A4%E9%94%80%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[撤销误添加暂存区文件12#无论是创建新增还是修改新增$git reset HEAD &lt;file&gt; commit覆盖123456789#处理少commit但不想多显示提交的记录$git commit -m 会被覆盖$git add &lt;file&gt;$git commit --amend#vim编辑前一笔提交信息#没有会被覆盖的提交记录$git reflog17bd775 (HEAD -&gt; master) HEAD@&#123;0&#125;: commit (amend): 忘记提交了代码，否则编译出错589173d HEAD@&#123;1&#125;: reset: moving to 589173d 撤销工作区文件修改12#这些操作是非常&lt;span color=&quot;red&quot;&gt;危险&lt;/span&gt;的，对这个文件的任何修改都会消失$git checkout -- &lt;file&gt; git reset命令要理解reset命令，先理解git的结构，具体可以参考git重置 soft 只移动HEAD指针位置，不改变暂存区和工作区的状态 mixed 移动HEAD指针位置，并重置暂存区状态和本地库一致，不改变工作区状态 hard 移动HEAD指针位置，并重置暂存区和工作区状态，保持3者一致12345678910111213141516171819202122232425262728293031323334353637#重置指针$git reset --soft HEAD^#如果是退两步HEAD^^#如果是退n步HEAD~n#重置指针和暂存区$git reset --mixed HEAD^或者$git reset HEAD^#重置指针，暂存区及工作区#危险，如果工作区代码没有commit，则再也找不回!!!!!!!$git reset --hard HEAD^#重置到指定的提交#这边需要用到之前的日志查看命令$git reflog07876bd (HEAD -&gt; master) HEAD@&#123;0&#125;: commit: 新提交2b104f55 HEAD@&#123;1&#125;: reset: moving to b104f5522a101d HEAD@&#123;2&#125;: commit: 新提交17bd775 (origin/master) HEAD@&#123;3&#125;: reset: moving to HEAD^0a4a26f HEAD@&#123;4&#125;: reset: moving to HEAD0a4a26f HEAD@&#123;5&#125;: commit: 提交17bd775 (origin/master) HEAD@&#123;6&#125;: reset: moving to 17bd775a45581a HEAD@&#123;7&#125;: revert: Revert 这个提交撤销17bd775 (origin/master) HEAD@&#123;8&#125;: commit (amend): 忘记提交了代码，否则编译出错589173d HEAD@&#123;9&#125;: reset: moving to 589173db104f55 HEAD@&#123;10&#125;: reset: moving to HEADb104f55 HEAD@&#123;11&#125;: commit: 忘记提交了589173d HEAD@&#123;12&#125;: commit: 提交16aa8104 HEAD@&#123;13&#125;: commit: 动了26364c5 HEAD@&#123;14&#125;: commit: 只是一次测试4a3753c HEAD@&#123;15&#125;: commit (initial): git测试#重置到17bd775$git reset --soft 17bd775 回滚代码123456789101112#暴力回滚$git reset --hard hash$git push(这个时候一般是提交不上去的，需要先git pull，可能还需要merge)#暴力回滚，会丢失本地工作区未commit的代码#对比历史commit$git diff hash#找到以前的代码，进行回滚，这种修改相对比较柔和，但是需要理解逻辑#找回删除的项目$git branch 最新一次提交的hash$git push]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git查看日志]]></title>
    <url>%2F2019%2F04%2F25%2Fgit%E6%9F%A5%E7%9C%8B%E6%97%A5%E5%BF%97%2F</url>
    <content type="text"><![CDATA[git log1234567891011121314151617181920$git logcommit 6aa81045796dfcbc53757454b10f6f89884f8515 (HEAD -&gt; master, origin/master)Author: 王鋆昌 &lt;wyc@servyou.com.cn&gt;Date: Thu Apr 25 17:17:46 2019 +0800 动了commit 26364c59ecde2db488b1c9554054484deba7f3a3Author: 王鋆昌 &lt;wyc@servyou.com.cn&gt;Date: Thu Apr 25 16:54:32 2019 +0800 只是一次测试commit 4a3753c16d279720d026a791d60d0138d30f4b2aAuthor: 王鋆昌 &lt;wyc@servyou.com.cn&gt;Date: Thu Apr 25 16:20:52 2019 +0800 git测试git尽可能的添加数据，而不是修改或删除数据git使用hash保证数据的完整性和一致性 简单参数12345678910111213141516#增加每次提交的差异点，并只显示最近一次提交$git log -p -1commit 6aa81045796dfcbc53757454b10f6f89884f8515 (HEAD -&gt; master, origin/master)Author: 王鋆昌 &lt;wyc@servyou.com.cn&gt;Date: Thu Apr 25 17:17:46 2019 +0800 动了diff --git a/wyc.txt b/wyc.txtindex a496e02..ba3d802 100644--- a/wyc.txt+++ b/wyc.txt@@ -1 +1,3 @@ 这个是数据wyc的文件，其他人不要动++我就动了，怎么滴，哈哈哈 12345678910#显示文件的修改统计，不显示明细$ git log --stat -1commit 6aa81045796dfcbc53757454b10f6f89884f8515 (HEAD -&gt; master, origin/master)Author: 王鋆昌 &lt;wyc@servyou.com.cn&gt;Date: Thu Apr 25 17:17:46 2019 +0800 动了 wyc.txt | 2 ++ 1 file changed, 2 insertions(+) 格式化展示12345678910111213141516171819202122232425262728293031#只展示hash值和对应的提交内容$ git log --pretty=oneline6aa81045796dfcbc53757454b10f6f89884f8515 (HEAD -&gt; master, origin/master) 动了26364c59ecde2db488b1c9554054484deba7f3a3 只是一次测试4a3753c16d279720d026a791d60d0138d30f4b2a git测试#展示hash码的一部分$ git reflog6aa8104 (HEAD -&gt; master, origin/master) HEAD@&#123;0&#125;: commit: 动了26364c5 HEAD@&#123;1&#125;: commit: 只是一次测试4a3753c HEAD@&#123;2&#125;: commit (initial): git测试#展示分支，合并历史$ git log --graph* commit 6aa81045796dfcbc53757454b10f6f89884f8515 (HEAD -&gt; master, origin/master)| Author: 王鋆昌 &lt;wyc@servyou.com.cn&gt;| Date: Thu Apr 25 17:17:46 2019 +0800| | 动了| * commit 26364c59ecde2db488b1c9554054484deba7f3a3| Author: 王鋆昌 &lt;wyc@servyou.com.cn&gt;| Date: Thu Apr 25 16:54:32 2019 +0800| | 只是一次测试| * commit 4a3753c16d279720d026a791d60d0138d30f4b2a Author: 王鋆昌 &lt;wyc@servyou.com.cn&gt; Date: Thu Apr 25 16:20:52 2019 +0800 git测试]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git仓库]]></title>
    <url>%2F2019%2F04%2F25%2Fgit%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[安装不详细说明，请参考安装教程123$ git --versiongit version 2.14.1恭喜你，安装成功了 git配置git config工具帮助你管理你的git配置，对于macOs系统，gitcongfig配置路径如下：– /etc/gitconfig 系统级的配置文件(基本不用)– ~/.gitconfig 当前用户的配置文件– .git/config 当前仓库的配置文件123456789101112131415对/etc/gitconfig进行读写需要加上--system$git config --system key value设置用户名和邮箱，如果不设置无法commit$git config --global user.name &quot;老狗&quot;(强制设置成自己的真实名称)$git config --global user.email wyc@servyou.com.cn查看全部的git配置$git config --listuser.name=王鋆昌user.email=wyc@servyou.com.cn通过git config key查看配置值$ git config user.name王鋆昌 git仓库获取仓库的方法目前有两种: 使用 git init 将本地现有的项目交由git管理 使用 git clone 从代码托管中心克隆仓库到本地123456789101112131415161718192021222324252627如果是通过 git init 初始化的本地项目，需要手动添加远程仓库读写远程仓库使用的 Git 保存的简写与其对应的 URL$git remote -vshortname 一般使用origin，因为和git clone默认命名$git remote add &lt;shortname&gt; &lt;url&gt;比如：$mkdir gittest$cd gittest$git init#此时查看发现生成了./git文件夹$ls -alldrwxr-xr-x 6 *** staff 192 Apr 25 16:18 .drwxr-xr-x 4 *** staff 128 Apr 25 13:39 ..drwxr-xr-x 9 *** staff 288 Apr 25 16:43 .git$touch wyc.txt#请观察文件颜色的变化$git add wyc.txt$git commit -m 做一次测试$git remote add origin git@192.168.2.107:wyc/gittest.git$git push origin master#如果不想每次都输 origin master 建立本地分支和远程分支映射$git push --set-upstream origin master或者$git branch --set-upstream-to origin/master$git push]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git发展史]]></title>
    <url>%2F2019%2F04%2F25%2Fgit%E5%8F%91%E5%B1%95%E5%8F%B2%2F</url>
    <content type="text"><![CDATA[brother and father说到git，不得不提他的两位举世闻名的家人brother——linuxfather——Linus这两位家人正是git产生的推动者。 简史 1991年，Linus编写了linux的内核代码，并于此年年底公开了部分代码 1991-2002年间，linux的维护成本都花费在了提交补丁和保存归档上，基本都是手动合并来自世界各地的代码 2002年，BitMover出于人道主义，将收费的分布式版本控制工具bitkeeper给linux开源社区来管理linux代码 2005年，samba开发者Andrew试图破解bitkeeper，然后被监测到，BitMover公司和社区闹掰，并回收的bitkeeper的使用权 同年，Linus闭关两星期，git诞生 git优势 相比svn这种传统的集中式版本管理，git没有单点故障的确定 分布式的版本管理，导致代码容灾能力超强 由于本地客户端存在分支版本，不需要联网即可工作 完整性保证，hash 尽可能的添加数据，而不是删除或者修改数据 与linux命令全面兼容 git结构– 工作区– 暂存区– 本地库– 代码托管中心 基本流程团队内部合作1234567a:git inita:git push添加权限给bb:git cloneb:git pusha:git pulla:git merge 跨团队合作123456外部c:fork外部c:git clone外部c:git push外部c:pull requesta:审核a:git merge]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[山水之间]]></title>
    <url>%2F2019%2F02%2F09%2F%E5%B1%B1%E6%B0%B4%E4%B9%8B%E9%97%B4%2F</url>
    <content type="text"><![CDATA[人生不过山水之间申屠古镇 这个古镇名字是我自定义的，之所以取成这样是因为那边的申屠祠堂的事迹给我的印象比较深，然后我已经忘记游览的古镇的名字了，但是申屠姓氏当之无愧。 山水之间 白云之巅景点，具体的名字我已经忘却了，但是与白云相关。松露遍开，漫天飞雪，当世之美景不过如此。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
        <tag>自然环境</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[征期跨度暴露的浙江爬取数据问题]]></title>
    <url>%2F2018%2F10%2F20%2F%E5%BE%81%E6%9C%9F%E8%B7%A8%E5%BA%A6%E5%BC%95%E8%B5%B7%E7%9A%84%E6%B5%99%E6%B1%9F%E7%88%AC%E5%8F%96%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[征期跨度上一期迭代，接触了网厅爬取数据的项目。出现了征期跨度引起的问题，31号发布的功能在本月份没有问题，但是跨越到7月份征期出现了严重的问题。(我们具体发布的时间是：2018-07-31)到了新的征期，税种的未申报都处理成了申报成功，无需缴款。 项目完成过程这个问题我认为是在项目迭代中慢慢积累起来的，最终演化成了一个严重的问题。1.爬取规则问题：浙江的爬取规则，是产品自己研究定的，但是出现了问题，所以最后紧急变更需求。(这里感谢浙江的现场的会计，最终的规则由她们提供)2.时间点问题：研究浙江网厅的时间是在6月末期了，已经过了6月征期，导致，一.提供的账号没有未缴款的数据，无法得知未缴款的数据机构；二.税种列表中都是已申报的状况，没有未申报的数据。3.自测问题：开发的可测试数据太少，原因由问题2引起；开发在进行自测的时候关注点过多集中在已申报缴款的状态，没有考虑到未申报的情况。 针对问题1：我认为如果可行，尽量在自己研究网厅的基础上再询问当地的会计，因为真实使用的客户永远比我们懂的多的多。针对问题2：开发主要责任，完全没有意识到因为征期税种报送完成可能获取不到一些状体的数据(比如未缴款状体)，所以网厅爬取要做好前期工作。针对问题3：在获取到爬取页面的数据结构前提下，开发其实可以模拟其他状态的数据，从而覆盖到更多的场景。 网厅爬取针对这一次的项目，我认为网厅爬取的最大难度在于攻破网厅的登录。浙江的网厅是界面上的国地税合并，登录方式还是分开的，随意本次迭代如何攻破登录确实花费了不少时间。 对于数据的爬取，理想的情况下是请求接口返回json格式的数据，糟糕的情况是返回html页面，后者需要对页面进行解析。此时爬取的规则显得比较重要了，相同的数据可能在不同的界面，返回的格式也有优劣，可以择优选择。 维护成本，爬取的方式维护成本一定是大的。我把他命名为：自己代码的命运掌握在别人的手中。 总结1.关于网厅爬取的任务，前提的工作一定要做好，主要是登录方式的研究和爬取页面数据的收集。2.爬取的规则尽可能找当地的会计进行确认。3.在没有足够数据的前提下，可以进行数据的模拟。4.做好心理准备，随时可能变更代码。 最后还是 心有余悸。如果我们验证的时间点没有跨越到8月份……。]]></content>
      <categories>
        <category>迭代错误</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[drds数据库脚本引发的问题——第一弹]]></title>
    <url>%2F2018%2F10%2F19%2Fdrds%E6%95%B0%E6%8D%AE%E5%BA%93%E8%84%9A%E6%9C%AC%E5%BC%95%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%80%E5%BC%B9%2F</url>
    <content type="text"><![CDATA[说明这篇文章内容比较敏感，所以不会发布到github上，只会保存在本地。 drds执行错误sql以下的sql执行在dev，test环境没有任何问题，问题暴露在release环境。sql如下：12341.update tx_employee_variation set is_martyr = is_disabled, is_bereaved_gaffer = is_disabled;2.update tx_employee_variation set is_no_domestic_residence = &apos;0&apos; where employee_id in (select id from tx_employee where area = &apos;1&apos;);3.update tx_employee_variation set is_no_domestic_residence = &apos;1&apos; where employee_id in (select id from tx_employee where area = &apos;2&apos;);4.update tx_need_declaration_setting set period_type = 1 where column_code in (&apos;100040&apos;,&apos;100050&apos;) and customer_id in (大概10w的数据量); sql.2和sql.3错误的类型一样，所以并在一起说。 release暴露问题release环境真很重要，至少可以在drds上暴露sql脚本的很多问题，不至于将问题遗留到真正发布的时刻。 关于drds sql执行的一些错误，阿里云有官方的帮助文档，有疑问请参考。上面官方可能会给出一些错误的解决方法。 sql.1第一次执行，报错：1[Err] 3009 - [d550cc4c3000000][10.98.6.41:3306][xqy_testportal]ERR-CODE: [TDDL-4620][ERR_FORBID_EXECUTE_DML_ALL] Forbid execute DELETE ALL or UPDATE ALL sql. 解释：drds对于全表的更改是不予许的。解决： 官方给出了解决方法，在执行的sql前面增加/!TDDL:FORBID_EXECUTE_DML_ALL=false*/，去掉无法执行全表的校验。注意，官方文档中给的惊叹号是中文的，请手动改成英文再执行。 DBA的意见是在sql后面加上where 1=1，增加条件语句即可跳过drds的检查。 sql.2和sql.3第一次执行，报错：1ERR-CODE: [TDDL-4617][ERR_SUBQUERY_LIMIT_PROTECTION] The number of rows returned by the subquery exceeds the maximum number of 100000. 解释：drds对于子查询中量级超过10万就会禁止。解决： 官方给出的解决办法是优化sql。 网络上找到了联表更新方法，将sql优化为： update tx_employee_variation as a,tx_employee as b set a.is_no_domestic_residence = ‘0’ where a.employee_id = b.id and b.area = ‘1’; 优化的sql第一次执行，报错： 1ERR-CODE: [TDDL-4601][ERR_EXECUTOR] not support cross db update. 解释：drds认定以上语句为跨库更新了。 解决： DBA出手解决了这个问题，两张表都是分库分表，在联表更新的情况下，必须加上分库分表健。于是改写成了以下sql：update tx_employee_variation as a,tx_employee as b set a.is_no_domestic_residence = ‘0’ where a.customer_id = b.customer_id and a.employee_id = b.id and b.area = ‘1’; sql.4比较特殊，10万的数据量，来源于cc_servyou这张表，xqy_portal和cc_servyou分属于不同的库，不能进行联动查询更新——DBA。来自DBA的建议： 10w的数据量肯定是不能放在一个in中的，drds执行会报错，支持大概0.5w左右 10w的数据量分成10w条sql跑，执行的速度会很慢 建议将10w的数据拆分成20*0.5的量放在执行语句in中 总结本次迭代的sql脚本也花费了相当一部分时间去解决的。值得好好总结一下。1.全表更新或者删除操作，可以按官方文档解决或者后面添加where 1=1条件，新测第二种方法执行效率更快。2.联表查询更新时，如果表是分库分表的，一定要加上分库分表健。3.in 语句 数据量大时，可以进行适当数量的拆分。 最后感谢，领导的英明神武——搭建了release环境；DBA—— jhh的专业指导；xp，wjn的sql测试和陪伴。 ps：release和线上的cc库是否能赋予我们查询的权利。]]></content>
      <categories>
        <category>drds</category>
      </categories>
      <tags>
        <tag>sql脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——低保真的可用性测试]]></title>
    <url>%2F2018%2F10%2F18%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E4%BD%8E%E4%BF%9D%E7%9C%9F%E7%9A%84%E5%8F%AF%E7%94%A8%E6%80%A7%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[可用性测试你怎么知道你的应用程序能正常工作。也许你的程序通过了编译，也许它通过了所有的单元测试。也许它还成功通过了QA的严酷考验。然而所有的这些都不能说明你的程序可以正常工作。如果你不找来真正的用户做可用性测试的话，你无法知道你的程序能否正常工作。 低保真测试演变基本理念：如果你想知道你的软件是否容易使用，那么在一些人尝试使用它们的时候观察他们，记下他们遇到的问题，然后修正这些问题，之后在度测试。 刚开始可用性测试非常昂贵，需要建立一个可用性测试的实验室，招募大量的测试用户，以得到有统计意义的结果，每次测试需要花费2～5万美元。因为成本原因不会经常使用。 1989年，jakob Nieob写了一篇文章，”打折扣的可用性测试”(中文翻译)。文章中指出可用性测试不是非得那样做不可。不需要可用性实验室，减少很多的测试用户，也能得到相同的结果。唯一的问题是聘请一个人主持一次测试仍然得花上5000～15000美元。 这里应该使用更加激进的方法。我们要合理利用客户这个群体资源。可行的话，找几个真实的用户来进行新上线的功能的检验。使用配置白名单的方式来进行小群体的测试也是可行的方案，之后可以逐步扩大开放范围。 关系图减少了测试人员的数量，你也可以获取可观的测试结果。从上图可知，少量的测试人员也可以发现相当的软件问题。(此图来自jakob Nieob的文章) 准则以下是低保真可用性测试的知道原则： 我该在什么时候测试理想情况下，在出现重大变更的时候，需要进行测试。比如更改用户的操作习惯等。 我需要找多少用户从图中的关系来看不需要太多。 要找什么样的用户一般情况下是随机抓一些人，只要会用电脑就行。但是对于一些专业性软件，需要找专职的人员测试。 测试要持续多长时间尽量保持简单，每个用户保持在45分钟～1个小时。 在哪里测试无所谓 测试人员应该具备怎样的特质需要一定的耐心，专业软件专业人才来测试。 我需要什么设备有些客户习惯在线直接测试，但是有些可能希望观看录制的视频，或者观看内部人员的直播。 我要为测试做什么准备提前想好要展示的东西，草拟一些步骤和说明。 测试需要花多少钱理想的情况是最好不要花费一分钱，找一些希望有这个功能的客户进行免费的测试。 我们应该怎样解读测试的结果这个比较重要。向开发团队和任何感兴趣的项目干系人汇报，或者可以以邮件的形式发送到相关的人员。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——结交混世魔猴]]></title>
    <url>%2F2018%2F09%2F13%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E7%BB%93%E4%BA%A4%E6%B7%B7%E4%B8%96%E9%AD%94%E7%8C%B4%2F</url>
    <content type="text"><![CDATA[起源2012年年底，Netflix技术博客上出现了一篇文章，题为”5 Lessons We’ve Learned Using AWS”(转向亚马逊网络服务过程中学到的5个教训)。亚马逊网络服务无疑是所谓”云计算”的杰出代表。因此，这篇文章实际上也可以说是给任何想要转向”云”网站的箴言。 疯狂的猴子下面是文章中的描述，”我们的工程师在AWS里最早创建的一个系统其实是”混世魔猴”。这只猴子的工作就是捣蛋，它要随机杀死我们系统架构里的组件或者服务。” 乍一看，肯定觉得这条建议太疯狂了！但是我们必须面对它。应该没有多少公司认同这样的做法，更别提会有多少公司真的去尝试了。如果在你工作的地方有人部署了一个后台服务，专门用于随机杀死自家服务器集群里的服务或进程，那么可以出门左转财务部。 没得选择“混世魔猴”不会安分守己，即使你不去结交，它也会自己找上门来。谁也无法预料在运行中，程序会不会出现各种诡异的问题。”如果我们不这样持续检验我们在失败面前自我恢复乃至成功的能力，那么这个系统很可能恰恰会在关键时刻掉链子”。 结交“混世魔猴”虽然讨厌，但是它对于服务来说也是有帮助的，它可能使得你的服务韧性十足。每件事情发生的背后总是有原因的。要避免失败，最好的办法就是不断地失败。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——费茨定律和无限宽度]]></title>
    <url>%2F2018%2F09%2F01%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E8%B4%B9%E8%8C%A8%E5%AE%9A%E5%BE%8B%E5%92%8C%E6%97%A0%E9%99%90%E5%AE%BD%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[费茨定律在人机交互领域，“费茨定律”被证明是一个最要的公式。它就是：Time = a + b log2(D/S + 1)其中，这里的 D 表示从光标的起始点到目标的距离，而 S 表示目标物体的宽度。这个理论完全是在运动轴线方向上的一个二维平面内讨论的。 “费茨定律” 在各种各样的条件下生效，这些条件包括肢体(手，脚，头戴式视觉，眼睛注视)，操作特征(输入设备)，物理环境(包括水下！)和用户群体(年轻人，老人，智障者，吸毒者)。注意，公式里的常数a和b是经验值，它们在不同的条件组合下的值各不相同。 Mac菜单“费茨定律” 决定了Mac平台上下拉菜单的获取速度应该比Windows上的下拉菜单获取快大约5倍。而且这已经被证明了。为了让导航变得更加容易一些，你要么把可点击的东西都挨着放在一起，要么就把可点击的区域做得大一些。或者双管齐下。事实上，Mac上的菜单并没有特别大。但Mac上的菜单没有依附在应用程序窗口上——它们永远停留在屏幕的顶端。既然光标停在屏幕的边缘，用”费茨定律“来计算的话，Mac上的菜单就有无穷大！因此，用户也就能更快地导航到Mac上的菜单。 总结“费茨定律”不仅仅是关于“把东西做得更大一点使得点击起来更容易”的理论，它还指导我们要尽最大的可能去利用屏幕边缘的自然边界：*“费茨定律”暗示，在任何电脑显示器上能被最快访问到的目标是屏幕的4个角落，然而，因为它们的图钉动作，设计师们似乎多年来都不惜一切代价地回避它们。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——分页显示该休矣]]></title>
    <url>%2F2018%2F08%2F31%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E5%88%86%E9%A1%B5%E6%98%BE%E7%A4%BA%E8%AF%A5%E4%BC%91%E7%9F%A3%2F</url>
    <content type="text"><![CDATA[并非抛弃当我第一次看到这个标题时，我是怀着好奇的心情去看待的，同时内心也是沉重的，因为我目前参与的项目中没有一个是抛弃分页显示的。这一小节所讲的并不是完全地抛弃分页显示的技术。 快速找到结果没错，我们应该避免没头没脑地生成一个包含有成千上万个条目的列表，然后用“一刀切”的方式把他们分页显示出来。这样就是把所有的负担都扔给了用户，这完全是不合理的！上面的内容用技术的术语来讲就是： 少量的数据，我们可以使用分页，不会阻碍用户寻找他想要知道的答案 数量达到一定的值，我们就需要考虑是否需要增加筛选，搜索和排序的功能 比如好的搜索引擎，返回的结果可能有十几页的数量，但是它一定会做很高效的排序让用户快速地进行选择——google，当然不是所有的搜索引擎都是那么优秀的，在不能翻墙的年代里，很难找到高质量的博客(一般都是无脑地转载)或者对于一个问题的解决方法。 除了排序，筛选和搜索的功能也能让用户在茫茫的数据中找出他想要的结果。 无穷分页法这种分页方式是没有分页按钮的，一般多用于手机App应用。也就是当用户看到页面底部的时候动态地加载更多的内容。传统的分页法对于用户来说不是特别友好(当然是你没有很好解决第二点的情况下)，但无穷分页法也不是完美无暇的——它自身也有缺点和陷阱： 用户永远也不知道下面还有多少的内容，所以在用无穷分页法的时候让用户觉得下面还有多少内容对用户来说不重要——参考头条。 无穷分页法不应该破坏深度链接。用户仍然能干净利落地链接到列表里任何一个特定的条目。 无穷分页的方法不受“网络蜘蛛“的青睐，无法去迎合网络搜索引擎。 当你动态加载新条目的时候，请提供一些用户看得见的反馈信息，这样用户就能自己判断出”内容还没有完，还有新的内容正在被加载，并不是程序死掉了…“。…]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——用户界面代表了软件]]></title>
    <url>%2F2018%2F08%2F27%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2%E4%BB%A3%E8%A1%A8%E4%BA%86%E8%BD%AF%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[难用即bug一般在项目收尾阶段拒绝任何改动，因为不管改动有多么正当，理性的理由，哪怕最小的改动都会带来更多的bug风险。以后我们可以给一个产品定性一个自带的bug属性，那就是是否受到用户的欢迎。这个bug是区别于功能的，区别于测试阶段发现的任何bug。如果你的系统有一个出色的界面，并且你在资金和时间方面预算充足，你就可以继续在这个系统上工作。即使这个系统有bug或者运行缓慢，你也可以去改进它。但如果你的系统有一个糟糕的界面，你基本上就一无所有了。所以这个自带的bug和界面有不可分割的关系。 用户并不关心当你的产品发布上去让用户使用的时候，你觉得用户关心的是什么？人们往往会更加关心他们所看到的，用户根本不想知道你产品服务端的界面有多么牛逼，你在visio画了多少了不起的架构设图，用户界面就代表了你的软件。系统的界面有多重要，想象一下，最高深的服务架构+超级糟糕的前端界面。有时候提升服务端的性能可能会花上很大的成本，而且客户可能并不买帐(因为客户并不关心他们不可触摸的东西)，然而改变一下你界面的一个小细节带来的成本或许少的多。用很小的成本换回用户超高的体验，我们何乐不为呢？ 纸上原型设计接下来的问题是，在不依赖我们的开发工具的前提下，我们怎么来为用户界面做原型设计呢？有个很简单的方法，就是在纸上做。从某种程度上来说，纸上原型设计是一种永远也不会过时的方法。*纸上的原型设计除了会影响你当前项目的质量以外，它有另外一个好处——它有助于你职业生涯的发展。在你学到的所有知识中，有多少在10年以后仍然有用？20年以后呢？技术的保存期跟香蕉一样。相比之下，纸上原型设计方法的保存期接近于纸张的保存期。一旦你学会了纸上原型设计，就可以在你接下来的职业生涯中参与的每一个项目里都使用它。20年之后什么用户界面技术会比较流行，但我相信，我们还是必须对那些设计做可用性方面的评估。 纸上原型设计的电子化对于软件开发来说是一种革新：原型设计感觉有点交互性了；相比于纸上更加容易修改；电子化更加适合于分布式开发的团队协作模式。Jensen Harris——作为office 2007团队的首席用户界面设计师，他介绍Power Point 做原型设计——呵呵。这个不是唯一的选择，对于一些简易的设计，网上有在线的工具可以进行原型设计。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——你永远不会有足够的奶酪]]></title>
    <url>%2F2018%2F08%2F22%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E4%BD%A0%E6%B0%B8%E8%BF%9C%E4%B8%8D%E4%BC%9A%E6%9C%89%E8%B6%B3%E5%A4%9F%E7%9A%84%E5%A5%B6%E9%85%AA%2F</url>
    <content type="text"><![CDATA[引言令人赏心悦目的产品和让人勉强容忍的产品之间的差别，就在于它们的细节有没有被做好。 哥伦比亚障碍装置哥伦比亚障碍装置，首度在 Human Factors International 公司(一个由商业心理学家和管理咨询师组成的咨询公司)做的一篇演讲文稿里被提及。实验装置如下图所示：实验装置说明：装置的一头是一只老鼠，另一头放置奶酪，中间用电网隔断。来研究奶酪的大小和电击的大小对老鼠是否会跨越电网的影响。 实验结果从上图看出，电击越小，奶酪越大，老鼠跨越电网的可能性越大。其实这个实验结论可以适用于用户是否会使用产品这个领域中。 没有足够的奶酪用户和可用性之间有着很自然的对应关系。要么放上最大的奶酪(让你的应用足够吸引人)，要么把电击降到最低(让你的应用容易被使用)。软件开发人员也许会认为自己的应用程序很吸引人，但是它们在用户眼里真的那么有吸引力吗？我们应该保持怀疑的态度——你的奶酪没有你想象中的那么大。要让用户真正地使用你的软件，如果你对此还抱有一丝希望的话，忘掉奶酪吧——确保你不会“电击”你的用户。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——程序员的高效工作场所]]></title>
    <url>%2F2018%2F08%2F05%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E9%AB%98%E6%95%88%E5%B7%A5%E4%BD%9C%E5%9C%BA%E6%89%80%2F</url>
    <content type="text"><![CDATA[程序员的&lt;&lt;权利法案&gt;&gt;如果一个公司愿意付给一个开发人员6万～10万美元的薪水，却用糟糕的工作条件以及破烂的硬件设施摧残他，这是令我难以置信的。提议我们应该制定一个针对程序员的&lt;&lt;权利法案&gt;&gt;，以防止公司拒绝给程序员提供取得成功所必要的基础条件，最终达到保护程序员权利的目的。 1.每个程序员都应该有两台显示器如果你想把开发者的生产力发挥到极致，请确保每个开发者都有两台显示器。 2.每个程序员都应该有一台快速的电脑首先电脑长时间运行不能卡顿，就这一点而言windows真的可以抛弃了，我觉得mac应该算是标配；有足够的内存支持，你可能需要运行很多的软件，没有足够的内存，运行的越多电脑越慢；电脑速度快，每次编译，调试的周期也就短，开发的效率高。 3.每个程序员都应该自己选择鼠标和键盘一般来说，公司配置的鼠标和键盘是标配，不一定适合每一个人。每个程序员手的手感和大小也都不一，对于键盘按下的感觉也不一致，追随自己的感觉，选择最合适自己的。 4.每个程序员都应该有一把舒适的椅子理论上，程序员靠屁股每天坐上8个小时，为何不在一把舒适的，设计优良的椅子上度过那8小时呢？ 5.每个程序员都应该能快速接入互联网软件开发环境中，网络一定要合格。不能光有有线网络，无限网络是必须的，因为移动办公是很常见的工作方式，做到开机即连。在国内，在补充一条，一定有途径做到翻墙。 6.每个程序员都应该有安静的工作环境编程需要全神贯注。程序员在一个嘈杂的环境下是很难高效工作的。 人体工程学人体工程学很重要，这门学科运用在软件开发中可以减缓程序员身体机能的衰退。如果你关注自己的身体健康，下面有几点注意事项： 显示器的顶端应该与眼睛齐平 桌子的表面应该和肚脐眼基本持平 脚应该在地板上平放，并且膝盖关节成90度(呵呵，不可能) 当你打字的时候，手腕应该和前臂成一条直线，不要弯上弯下，也不要侧弯 背景光的功效程序员有一样东西是无法容忍的，那就是从头顶上直射下来的光线，或者其他方向的光线导致屏幕不方便眼睛的查看，这也是程序员更加喜欢待在黑暗的环境中的原因。在大部分情况下，尽管不开灯的房间更适合阅读电脑显示屏上的东西，但待在这种黑暗的房间里本身也有问题。有研究表明，坐在一间黑屋子里长时间盯着一个超亮的矩形区域，这对眼睛同样有害。有效的解决方法是在电脑的背后制造背景光。背景光是间接光照和光线补偿的完美结合。它能减轻视力疲劳，并且创造一个更加美好，更加舒适的计算机显示体验。请参考mac电脑背后的信仰之灯。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——坏苹果]]></title>
    <url>%2F2018%2F08%2F04%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E5%9D%8F%E8%8B%B9%E6%9E%9C%2F</url>
    <content type="text"><![CDATA[坏苹果效应生活中的小实验，两堆新鲜的苹果，在其中一推中放入一个烂苹果。在等待中你会发现，放入烂苹果的那堆一定烂得更快。你可以把这个效应仿照到团队成员中，一个有问题的团队成员，也就是一个典型的“坏苹果”。 坏苹果的特征一些警报的信号，用以识别你的团队中是否有“坏苹果”存在： 他们掩饰自己的无知，而不是尽力去向他们的团队伙伴学习。 他们对个人的隐私有着过度的渴望，典型就是“我不需要任何人来查看我的代码”。 他们很在意自己的地盘。他们会说：”我代码里的问题没人能修复。但是我现在太忙了，没有时间去管它们。我打算下周处理它们。” 他们抱怨团队所做的决定，并且在团队已经继续前进了很久之后还会重拾旧题。典型就是“我还是认为，我们应该回过头去修改我们上个月讨论的那个设计。我们当初的那个是行不通的”。 所有其他的团队成员都在传说关于同一个人的俏皮话或者抱怨他(简称吐槽)。 他们不会积极投入团队的行动。 团队的毒药当你积极地对项目宣战并以你的团队成员为敌时，你就成为项目的一个负担。毫无疑问，有“坏苹果”的团队会表现得比较差。更加糟糕的是，“坏苹果”的效应，其他团队成员开始呈现“坏苹果“的特征。团队的绩效可以从团队里最差的那位成员身上准确地预测出来。 有趣的实验Will Felps——华盛顿大学的一位教授，曾经组织过一次社会学实验：大学生们每4个人一组，组建几个团队，并指派了同样的任务。为了激励团队，表现最好的那个团队每人都能得到100美元奖励。但是某些团队中安插着一些“坏苹果”，他们具有以下的特征： 沮丧的悲观主义者。他会抱怨任务太无趣，并且公开质疑团队的取胜能力。 混球。他会否定其他人的想法，但是自己从来不拿出想法。 懒鬼。他们会经常说“随便“或者”无所谓“。原本的结果预测是：团队会主导个人，而不是反过来。然而实验的结果表明，团队并没有传统观点上认为的那么强有力，毒害一个团队是多么的容易。他们往往表现为上一节中所描述的行为。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——会议是浪费工作时间的最佳去处]]></title>
    <url>%2F2018%2F08%2F03%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E4%BC%9A%E8%AE%AE%E6%98%AF%E6%B5%AA%E8%B4%B9%E5%B7%A5%E4%BD%9C%E6%97%B6%E9%97%B4%E7%9A%84%E6%9C%80%E4%BD%B3%E5%8E%BB%E5%A4%84%2F</url>
    <content type="text"><![CDATA[疑问现在你在自问一下，那些会议中有多少是值得参加的？如果把相同的时间用在工作上，你又能完成多少事情？这不禁让人想知道，我们究竟为什么要开会？ 会议的准则我们应该以怀疑的态度去看待会议，把它当作是一种降低工作效率的风险。我们举行会议是因为我们认为我们需要它们，但事实上，会议往往只是在浪费宝贵的工作时间。就我而言，我采用以下几个原则，以确保我的会议是真正有用的：1. 会议绝不应该超过一个小时，否则应该判死刑。 对于任何会议，第一个并且重要的是约束会议的时间。因为时间在任何公司都是宝贵的资源；会议的时间太长，参与会议的人员很难记住会议的重点。会议的时间太长说明这个会议可能讨论的范围太宽泛了，或者整体缺少一个必要的焦点。 2.每个会议都应该有一个清晰的目标声明 一个清晰的目标对于缩短会议的时间有很大的帮助，这样能使整体的会议有一个必要的焦点。只要确保每个人都很清楚会议的目的，剩下的事自然会水到渠成。 3.在开会之前预先做好功课 既然你的会议有了一个清晰的目标声明，那么每个与会者都应该提前知道他们将要讨论和分享的内容。主持会议的人需要整理资料文档，参加会议的人需要阅读资料文档。会议邮件的附件中尽量加入会议的资料文档。 4.把会议变成可选的 “强制”的会议是站不住脚的。每一个出现在会议上的人都应该是因为他们想要在那里，或者他们需要在那里。在组织会议的时候考虑团队中哪些成员需要参加，甚至途中哪些成员可以离开。 5.在会议结束时概括一下待办事项 任何一个真正有成效的会议都会做一些决定，然后就直接导致某些事情的发生。作为会议的住持者你需要记录会议中已经达成一致的约定，然后在会议结束的时刻进行总结和再声明。 总结在软件开发中，会议基本是你避无可避的流程(除非你是一个猛人，单核模式)。但是会议千万不要成为我们挥霍时间的地方，我们需要规划我们的流程会议。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——结对编程和代码审查]]></title>
    <url>%2F2018%2F08%2F02%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E7%BB%93%E5%AF%B9%E7%BC%96%E7%A8%8B%E5%92%8C%E4%BB%A3%E7%A0%81%E5%AE%A1%E6%9F%A5%2F</url>
    <content type="text"><![CDATA[结对编程首先来说一下结对编程的表象：两个开发者在同一台机器上工作。他们都有各自的键盘和鼠标。就好像飞机的驾驶员和领航员，驾驶员负责编码，领航员的职责是阅读，核对，拼写检查以及在脑子里测试代码。总结就是一个人在编写代码的同时另一个人在检查核对所写的代码。社区中对结对编程有一些正面的看法，我在这里整理和总结了一下： 缩减后期bug出现的概率，因为两个人多了一双眼睛。 两个人总有不同的技能，技能的传递往往发生在结对中。当一个人向另一个展示一些技巧的时候，实际上他们在进行临时的培训。 保证一个团队当中有超过一个的人熟悉这块代码。 交换的结对编程可以彼此熟悉，加深交流。 结对的成本结对编程固然有很多的好处，但是在软件开发中我们还需要考虑成本的问题。对于同一个业务开发你可能需要投入两倍的人力成本去完成这个结对编程。加入了人力成本的因素之后，结对编程的优点也就不是很突出了。 代码审查经过笔者工作的亲生经历，我可以毫无保留的担保代码评审的价值。其实，结对编程的很多好处都可以通过可靠的同级评审来获得。结对编程的优势在于它的即时性，代码审查是需要编码开发的后期来进行实现的。但是如今有许多的工具可以利用来节省代码审查的人力和时间： sonar扫描——发现编程中一些常见的隐藏bug checkstyle——代码格式检查和拼写检查 gerrit——同级评审制度 喜忧参半代码审查解决了结对编程中需要的高人力成本，同时也保证了代码的检查完成。但是，似乎没有人愿意花时间去真正理解那些并不简单的新代码，所以反馈通常是比较笼统的。而且那些所谓的工具只是发现常见通用的问题，对于具体的业务代码，你仍然需要花费时间去审查。人要在原有的代码基础上添加功能或者修改错误的时候，他们通常会有很多的反馈，甚至推到重来的冲动。所以代码审查真的是一项需要耐心和忍耐力的活。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——性能制胜]]></title>
    <url>%2F2018%2F08%2F01%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E6%80%A7%E8%83%BD%E5%88%B6%E8%83%9C%2F</url>
    <content type="text"><![CDATA[性能的重要性已经有大量的实验数据表明：网站载入和显示的速度越慢，使用它的人就会越少。比如：google发现，显示10个结果的网页需要0.4秒来生成，显示30个结果的网页需要0.9秒来生成。半秒钟的延迟会造成20%的流量下降。半秒钟的延迟就扼杀了用户的满意度。 在A/B测试中，亚马逊试着以100毫秒为单位逐步增加页面的延迟，结果发现即使是非常小的延迟也会付出高昂的代价，导致收入显著下跌。 于是，你如何把自己跟别人区别开来呢？你得从“快”入手，其他的先放一放。 雅虎的指导原则自2007年以来，构建一个快速的网站的黄金参考准则始终首推雅虎提出的“加速你的网站的13条简单原则”。这里有些不错的建议，但是，其中很多的建议只有在你运营了一个每天有几百万独立用户访问的网站时才有意义。 比如：用户与你网络服务器的邻近程度直接影响着响应时间。从用户的角度来看，把你的内容部署在多个地理位置分散的服务器上会让你的网页加载得更快。作为性能优化的最后一步，stack overflow把他们所有的静态内容都部署到了cnd。结果很令人振奋。 但是，不会推荐大家直接上cdn。因为你的网站流量说不定还没有到达需要使用这一招的时候。因为在雅虎的清单上有一大推改善性能的做法是免费而且很容易实现。自2007年以来，使用cdn的成本降低了很多，用起来也简便了很多。这要归功于有更多的公司参与了这个领域的竞争，它们包括亚马逊，NetDNA，CacheFly等。因此，当时机成熟的时候，而且你已经做完了雅虎清单上要求做的事情，你就可以考虑cdn了。 中国的互联网公司亚马逊是墙外著名的电商互联网公司，而在国内，淘宝是首曲一指的。如果你的服务是缓慢的，遇到了瓶颈，我觉得淘宝技术这十年是很好的参考手册。里面讲述了淘宝从最初的小流量服务，到现在如何成为撑起每天那么大流量的互联网企业。淘宝是如何一步步在遇到困难的时候，革新技术，解决服务瓶颈，在漫长的岁月中往复做着这个事情。最后最好的机器和技术已经无法满足淘宝的业务，技术创新诞生了，并且至今形成了淘宝自己的技术体系。你可以借鉴里面的东西，来优化你自己的服务。 总结对大多数网站而言，性能是一种特性，能克敌制胜。几乎所有的用户更喜欢使用快速的网站。在这里，我想我们还应该学到一个教训——在开放的互联网世界里竞争，最后只会剩下两种网站：要么很快，要么已经死去。而你又将何去何从。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——电梯测试]]></title>
    <url>%2F2018%2F07%2F31%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E7%94%B5%E6%A2%AF%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[何为电梯测试电梯的特点是层层递进的，最终达到你想要到达的目的地。电梯测试也是类似，你可以把它理解为一系列层层递进的为什么，最终得到你想要的答案。接下来笔者会给出例子方便读者理解。 编码的意义软件开发者们是真心喜爱编写代码的。但根据经验，他们当中很少有人可以解释清楚他们为什么在编写代码。如果你不信，你可以从你的团队里找个人来测试一下——这里的测试就是所谓的电梯测试。 电梯测试询问软件开发人员，为什么要做那个，继续问下去，直到你得到一个可以让你的客户理解的原因： 你在做什么？ 我在修复这个数据网格的排序问题。 你为什么要解决这个问题？ 因为它在bug清单上。 它为什么在bug清单上？ 因为有个测试人员把它作为一个bug报出来了。 为什么它被作为一个bug报出来了？ 测试人员认为这个字段应该按照数字顺序而不是字母顺序来排序。 为什么测试人员这么认为？ 很显然，如果把 “条目2“ 排在 “条目19“ 的后面，用户在查找的时候就会有麻烦。 项目远景模型软件开发者认为他们的工作就是编写代码。其实不然。 你团队里的每个人都应该能够通过由陌生人主持的电梯测试——在60秒之内，清晰地解释他们在做什么，以及为什么人们会在意他们正在做的事情。一个项目远景模型可以帮助团队成员通过“电梯测试“。它遵循如下的形式： 为了(目标客户) 他们(关于需求或者机会的说明) 这个(产品名称)是(产品类别) 它的(关键优势，吸引人的购买理由) 不像(主要竞争对手的替代产品) 我们的产品(主要的差异化的特性说明)——《跨越鸿沟》 作用和意义电梯测试和创建项目远景可以帮助团队持续专注于产品的关键方面，否则团队很容易就会被短期开发迭代中的问题缠住，从而失去对整个项目远景的控制。软件开发人员切记的是——所有的短期迭代都是为了实现产品最后的愿景。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——高效编程原则]]></title>
    <url>%2F2018%2F07%2F30%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E9%AB%98%E6%95%88%E7%BC%96%E7%A8%8B%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[永远都是你的错作为一名谦逊的程序员，最基本的要求就是要有意识：你写的代码在任何时候出了问题，那一定都是你的错。 在大多数项目中，你所调试的代码里常常混杂着这些东西：你和项目小组中的其他成员开发的应用代码，第三方的产品，以及平台环境。无论你的软件出现什么样的问题——甚至最开始出错的地方根本就不是你的代码——你也应该总是假定问题出在你的代码里，并根据这个假设采取行动。 说实在，程序员都有一种高傲的性格，一般都是不愿意承认问题出在自己那边。而且出错之后一定会背负领导或者其他各方的压力的。可是事实永远不会因为你的否认而改变，该背的锅还是得背的。 大道至简代码不是什么好东西。代码会随着时间的推移慢慢腐烂。代码需要周期性的维护。并且里面还藏着bug。但是这些问题真正不在于代码，代码不是我们的敌人。你想看看真正的敌人吗？去照一照镜子吧。问题就在那里！作为一个软件开发者，你就是自己最大的敌人。你越早认识到这点，你的境况就会越好。在编码的过程中，你可以从很多维度评价你的代码： 代码的简洁度 功能的完整性 执行速度 编码所花费的时间 健壮性 灵活性我们想要的是一个实用而明智的策略，以缩减一个程序员在想要了解程序的工作原理时所需阅读的代码量。对于一个具体的需求，软件编程向来都是大道至简，然后依据测试的结果按需提升其他的维度。 避免写注释你应该总是专注于编写代码，而忘了还有注释这种东西存在。这里并不是让大家不要注释，否定注释。相反，我认为注释是很重要的东西，用很多注释装饰代码是件好事，但是在代码中加入大片大片的注释反而不是景上添花。注释不是给编译器看的，注释是用来和其他人交流的，你所要做的是在代码核心处与实用方式上添加注释，其他地方你可以使用编程技巧使代码通俗易懂。注释不是那么简单的，好的注释取决于你是否是一个好的作家，是否有良好的沟通能力。作为一个软件开发着，真的是不容易。一些有趣的注释 学会读源代码在沟通这个复杂的领域，写出能让人类领会并理解的连贯的段落比敲出几行不至于让解释器或编译器呕吐的软件代码要难的多。这就是为什么——就软件开发而言——所有的文档大概就是很差劲的，而且，由于为人写作比为机器写作要困难的多，恐怕在可预见的将来，文档还会继续差劲下去。对此，你基本上无能为力的。不管文档上面怎么说，源代码才是最终的事实，是你所能找到的最好的，最确定的，最新的文档。在此，并不想否定文档的作用，相反文档是很重要的，只是想要强调阅读源码的价值。但是，阅读源码是一项费力的活，耐不住寂寞文档就是很好的工具。 向橡皮鸭求助遇到解决不了的困难时，恐怕你做的最多的就是去网上寻求帮助，应该没有人等待并祈祷着吧！在寻求帮助的同时，你应该： 用足够多的细节来描述发生的状况，尽量让别人能了解到底发生了什么事情。 告诉他人你为什么需要知道答案，是闲来无事的好奇，还是在具体的项目中遇到了障碍。 说一说为了解决这个问题你都做过什么研究，以及你自己的发现。别人没有义务花费时间来帮助你，你应该有自己的分析。 适当的组织你的问题，酝酿一个合格的问题可能会帮你更快的找到答案。 如果解决了问题，尽量点赞和喝彩，积累的点赞可以帮助后来者快速注意到解决方法。 创新以人为本上面的标题需要这样断句：创新，以人为本。这一小节讲的是执行力的问题。一个好的创意值多少钱？可能它有点值钱，但是也值不了多少。创意本身不会毫无价值，但是很显然，单有创意只是空头支票。好的执行力才是好创意的倍增器。在软件开发领域，执行意味着专注于构成你的应用程序的所有微小细节。如果你不是始终沉迷于你的应用程序的每个方面，不去持续优化和改进它的每一处细节，那么你就不是在执行。至少，不是在很好的执行。你的团队执行得怎样，决定了他们会把你的创意从黄金变成废铁，还是从废铁变成黄金。创意需要人去执行的，甚至整个团队去执行的，所以最后执行力还是取决于程序员本身，是以人为根本和基础的。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——沟通一直都是问题]]></title>
    <url>%2F2018%2F07%2F27%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E6%B2%9F%E9%80%9A%E4%B8%80%E7%9B%B4%E9%83%BD%E6%98%AF%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[引言沟通，对于程序员来说一直都是不可回避的。但是往往也是容易忽略的，因为我们就是这样的人群，默默地在深夜工作着，似乎根本不需要与他人沟通，但是在软件开发中，沟通一直都是存在的问题。 经典语录杰出的程序员跟勉强过得去的程序员之间的差别，不在于他们掌握了多少种编程语言，也不在于他们谁更擅长python或者java。真正的关键是，他们能不能把他们的想法表达清楚。杰出的程序员通过说服别人来达成协作。通过清晰的注释和技术文档，他们让其他程序员能够读懂他们的代码，这也意味着其他程序员能够重用他们的代码，而不必重新去写。要不然，他们所写的代码价值就大打折扣了。如果你还没有感觉，那么我只能残忍的告诉你，你可能还没有达到勉强的地位。 程序员的素质程序员必备的素质： 扎实的基本功 编程的技巧 坚韧的毅力 良好的沟通技巧 博客编写博客是一个很好的提升沟通能力的方式。对于博客的态度，你需要花费大量的时间去更新或者维护，比如你需要回复提问，复看以前的博客是否存在问题等等。这里推荐几个好的博客站点： github stack overflow 简书 csdn 知乎 使用的场景在软件开发的生涯中，作为程序员你一定会遇到以下的场景： 推行会议的流程 编写手册文档 团队协作…以上所有的场景，都是需要你的沟通技巧，所以沟通一直都是一个问题。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——程序员的8种境界]]></title>
    <url>%2F2018%2F07%2F26%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%848%E7%A7%8D%E5%A2%83%E7%95%8C%2F</url>
    <content type="text"><![CDATA[寄语平心而论，不能为了编程而编程，大多数想要学习编程的人一般都是被盲目的信仰刺激。所以我奉劝看到这篇博客的各位，请下定自己的决心之后，在决定是否仍要踏入这个行业。 什么是程序员？程序员是耗尽毕生精力去编写代码，使他人从代码编写工作中解脱的人，因此，宣扬 “每个人都需要知道如何去编程”，我认为是一种倒退！ 生命中最困难的，是想清楚你真正想要做的事情，而不是学上一堆假设将来会有用的东西。如果说你的研究和探索最终还是引领你走上了编程之路，那就用尽一切方法去学习吧！ 8境界在求职的时候，你也许会被问到这样的问题：你对自己未来5年的职业是怎样规划的？对于这样问题的回答不只是为了应付面试官，而是为了你自己。在心中你究竟想过怎样的生活？作为一个程序员，最完美的职业生涯规划应该是怎样的？ 在此，我通过书籍整理了大致8种境界：1.不朽的程序员这个是最高的境界！总结一句话就是：人不在江湖，但江湖仍然有他的传说。你可能没有见过这些人，但是在书籍中你可能读到过他的名字，你可能正在读他发表的论文，或者你正在使用一些基础的技术，而这些技术正是通过他的论文实现的或者就是由他实现的。他们在生前可能获得过图灵奖，成为计算机博物馆中的一个永久收藏，其他众多的程序员都在学习他的作品。抱歉，虽然很残忍，但是我想说几乎大部分程序员在一生中都无法达到这个境界。 2.成功的程序员总结一句话就是：自己有很强的编程能力，并且具有很强的商业头脑，将代码商业模式化。所以他们运营着一个不错的公司，甚至控制了整个产业链。他们拥有绝对的自由，可以做任何自己想做的事情。这一境界的程序员，相比于编程能力，更多的是需要商业上的才能。 3.知名程序员达到这一境界的程序员也不错。他们可能在一家非常知名的大型技术公司工作，也可能在一家很有影响力的小公司或者是在一个很有希望的创业团队工作。不管怎么样，其他的程序员都或多或少听说过他们，并且他们在自己所在的领域有着积极的影响。 4.胜任的程序员首先，你的能力是你在工作中游刃有余，你从来不会为得到一份满意的工作而发愁。你的同事也非常尊敬你。每一家你曾经工作过的公司都因为你的加盟而在某些方面得到了提升。 5.普通的程序员这个境界了，你基本上能够应付一般的编程工作。由于天资所限，他们很难成为杰出的程序员。很残忍，大部分进入这个领域的人也就到此止步了。但是，天赋跟成功的关系并不大。如果你有敏锐的商业嗅觉和不错的人际交往能力，你依然可以变得很成功。如果你考这一行当过上了不错的生活，这已经说明你很才了。人贵有自知之名。普通你的能力都会比你自认为的要低。缺乏天赋并不是什么大不了的事情。要勇敢一点，发掘自己的特长，并且充分加以发挥。 6.业余程序员这个群体一般是一些很有前途的学生或实习生，也可能正在参与某些开源项目，或者利用个人闲暇时间开发一些好玩的应用程序。他们是一群很有想法，充满激情的人。这个境界的程序员可以通过自我提升，迅速地胜任程序员这个职业。 7.低调的程序员还有一些坊间流传的比较有个性的程序员。他们很有能力但是没什么令世人瞩目的成就。他们可能服务于某家大型公司。写代码仅仅是份工作而已，并非他们生活的全部。 8.烂程序员总结一句话：没有金刚钻，却偏偏拦了瓷器活。这个级别的程序员技能极其匮乏，他们通常是阴差阳错地干上这一行的。他们所做的任何事情都会给他们的同事带来痛苦和灾难。如果你是这样的，你是否有采取行动改善自己，或者远离这个行业。 举例不朽的程序员:Dijkstra这样表示，你一定不知道。如果你学过数据结构，是否记得其中的最短路径算法就是以他命名的？如果你学过操作系统，是否记得有一个调度算法——银行家算法？ Ken ThompsonDennis Ritchie这两位就不用介绍了。大学时代你的第一门语言就是他们发明的。很多语言都是以此为基础在上面进行的扩展。unix系统的发明者，当年火星计划的后继者。 knuth如果你学过算法，你可能知道他。你是否读到过《具体数学》，《研究之美》这些书籍。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql死锁——insert操作死锁]]></title>
    <url>%2F2018%2F05%2F21%2Fmysql%E6%AD%BB%E9%94%81%E2%80%94%E2%80%94insert%E6%93%8D%E4%BD%9C%E6%AD%BB%E9%94%81%2F</url>
    <content type="text"><![CDATA[mysql基础参数不用的mysql版本，加锁的过程不一样；mysql的数据库引擎和事务隔离方式也对分析死锁有帮助。这里涉及的mysql基础参数的查询方式与此次错误无关。1234567891011121314151617181920212223mysql&gt;select version(); # 查询mysql的版本+-----------+| version() |+-----------+| 5.7.9-log |+-----------+# 这个是测试环境mysql版本mysql&gt;show variables like &apos;%engine%&apos;; # 查询mysql的数据库引擎+----------------------------+--------+| Variable_name | Value |+----------------------------+--------+| default_storage_engine | InnoDB || default_tmp_storage_engine | InnoDB || storage_engine | InnoDB |+----------------------------+--------+mysql&gt;select @@global.tx_isolation, @@session.tx_isolation, @@tx_isolation; # 查询mysql的事务隔离方式+-----------------------+------------------------+-----------------+| @@global.tx_isolation | @@session.tx_isolation | @@tx_isolation |+-----------------------+------------------------+-----------------+| REPEATABLE-READ | REPEATABLE-READ | REPEATABLE-READ |+-----------------------+------------------------+-----------------+ InnoDB锁类型InnoDB的锁类型有很多，根据网上的博客，笔者在此进行整理。或者读者可以参考csdn。 1.基本锁1.1共享锁(Shared Locks：S锁) mysql允许持有S锁的事务读取一行，加了S锁的记录允许其他事务再加上S锁，但加不了X锁(参考下方)。1.2排他锁(Exclusive Locks：X锁) mysql允许持有X锁的事务更新或删除一行，加了X锁的记录不允许其他事务再加上S锁或者X锁。 2.记录锁(Record Locks) 记录锁, 仅仅锁住索引记录的一行。 单条索引记录上加锁，Record lock锁住的永远是索引，而非记录本身，即使该表上没有任何索引，那么innodb会在后台创建一个隐藏的聚集主键索引，那么锁住的就是这个隐藏的聚集主键索引。所以说当一条sql没有走任何索引时，那么将会在每一条聚集索引后面加X锁，这个类似于表锁，但原理上和表锁应该是完全不同的。 3.间隙锁(Gap Locks) 区间锁，仅仅锁住一个索引区间(开区间)。 在索引记录之间的间隙中加锁，或者在某一条索引记录的之前或之后加锁，并不包括该索引记录本身。 4.next-key锁(Next-Key Locks) 默认情况下InnoDB使用Next-Key Locks来锁定记录。 但是当查询的索引含有唯一属性的时候，即查找到的记录只有一条，那么Next-Key Locks会进行优化，变成Record Locks。 5.插入意向锁(Insert Intention Locks) Gap Lock中存在一种插入意向锁（Insert Intention Lock），在insert操作时产生。在多事务同时写入不同数据至同一索引间隙的时候，并不需要等待其他事务完成，不会发生锁等待。 假设有一个记录索引包含键值4和7，不同的事务分别插入5和6，每个事务都会产生一个加在4-7之间的插入意向锁，获取在插入行上的排它锁，但是不会被互相锁住，因为数据行并不冲突。 *注：插入意向锁并非意向锁，而是一种特殊的间隙锁。 测试环境问题业务场景：企业信用操作，前端会并发ajax请求获取企业的信用。当数据库有数据时直接显示，没有值时从局端获取数据并插入数据库，锁表就发生在数据插入的时刻。当时排查问题已经定位了表锁了，但是不知道具体的原因，在guox的帮助下进行了相关排查。在此感谢guox大牛。 InnoDB日志mysql死锁脱离了业务层，需要使用命令查看InnoDB状态(包含最近的死锁日志)。1mysql&gt;show engine innodb status; # 数据库执行即可 以下是当时拿到的相关的日志：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162InnoDB &quot;=====================================2018-05-22 08:31:00 0x7f51e4968700 INNODB MONITOR OUTPUT=====================================Per second averages calculated from the last 26 seconds-----------------BACKGROUND THREAD-----------------srv_master_thread loops: 1972057 srv_active, 0 srv_shutdown, 19348138 srv_idlesrv_master_thread log flush and writes: 21320195----------SEMAPHORES----------OS WAIT ARRAY INFO: reservation count 7610437OS WAIT ARRAY INFO: signal count 8000876RW-shared spins 0, rounds 7852374, OS waits 3412636RW-excl spins 0, rounds 19305504, OS waits 392660RW-sx spins 197857, rounds 4219933, OS waits 67445Spin rounds per wait: 7852374.00 RW-shared, 19305504.00 RW-excl, 21.33 RW-sx------------------------LATEST DETECTED DEADLOCK------------------------2018-05-22 08:30:50 0x7f51dd08d700*** (1) TRANSACTION:TRANSACTION 115900879, ACTIVE 0 sec insertingmysql tables in use 2, locked 2LOCK WAIT 6 lock struct(s), heap size 1136, 4 row lock(s), undo log entries 2MySQL thread id 1925809, OS thread handle 139987345471232, query id 161369824 10.199.134.32 rootinsert into tx_credit_info(id, customer_id, year, grade, score, report_id) values ( gen_id(&apos;tx_credit_info&apos;), 127374372, &apos;2017&apos;, &apos;C&apos;, &apos;47.3&apos;, &apos;127374372_20180522083050902_1CE2&apos; )*** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 11344 page no 4 n bits 80 index UNIQUE_INDEX_CUSTOMER_ID_AND_YEAR of table `xqy_test`.`tx_credit_info` trx id 115900879 lock_mode X locks gap before rec insert intention waitingRecord lock, heap no 10 PHYSICAL RECORD: n_fields 3; compact format; info bits 0 0: len 8; hex 8000000007979427; asc &apos;;; 1: len 4; hex 32303137; asc 2017;; 2: len 4; hex 80000bbd; asc ;;*** (2) TRANSACTION:TRANSACTION 115900880, ACTIVE 0 sec starting index readmysql tables in use 2, locked 24 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 1925817, OS thread handle 139989577422592, query id 161369834 10.199.134.32 root statisticsselect next_value into ret_val from sys_sequence where table_name= NAME_CONST(&apos;tableName&apos;,_utf8&apos;tx_credit_info&apos; COLLATE &apos;utf8_general_ci&apos;) for update*** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 11344 page no 4 n bits 80 index UNIQUE_INDEX_CUSTOMER_ID_AND_YEAR of table `xqy_test`.`tx_credit_info` trx id 115900880 lock_mode X locks gap before recRecord lock, heap no 10 PHYSICAL RECORD: n_fields 3; compact format; info bits 0 0: len 8; hex 8000000007979427; asc &apos;;; 1: len 4; hex 32303137; asc 2017;; 2: len 4; hex 80000bbd; asc ;;*** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 10619 page no 4 n bits 424 index idx1_sys_sequence of table `xqy_test`.`sys_sequence` trx id 115900880 lock_mode X locks rec but not gap waitingRecord lock, heap no 315 PHYSICAL RECORD: n_fields 2; compact format; info bits 0 0: len 14; hex 74785f6372656469745f696e666f; asc tx_credit_info;; 1: len 4; hex 80000520; asc ;;*** WE ROLL BACK TRANSACTION (2)...(此处省略) 关于如何读死锁的日志，读者可以参考以下文章。 日志分析12345678910111213*** (1) TRANSACTION:TRANSACTION 115900879, ACTIVE 0 sec insertingmysql tables in use 2, locked 2LOCK WAIT 6 lock struct(s), heap size 1136, 4 row lock(s), undo log entries 2MySQL thread id 1925809, OS thread handle 139987345471232, query id 161369824 10.199.134.32 rootinsert into tx_credit_info(id, customer_id, year, grade, score, report_id) values ( gen_id(&apos;tx_credit_info&apos;), 127374372, &apos;2017&apos;, &apos;C&apos;, &apos;47.3&apos;, &apos;127374372_20180522083050902_1CE2&apos; )*** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 11344 page no 4 n bits 80 index UNIQUE_INDEX_CUSTOMER_ID_AND_YEAR of table `xqy_test`.`tx_credit_info` trx id 115900879 lock_mode X locks gap before rec insert intention waiting 日志中gen_id()是mysql中自定义的方法，可以使用：mysql&gt;show create function gen_id;查看gen_id方法具体干了什么，方法如下：12345678910111213141516171819CREATE DEFINER=`root`@`%` FUNCTION `gen_id`(`tableName` VARCHAR(64)) RETURNS int(11)BEGINDECLARE ret_val INT;#不支持显示事务，mysql默认会开启事务和提交事务#start transaction; select next_value into ret_val from sys_sequence where table_name=tableName for update; update sys_sequence set current_value=current_value+step, next_value=next_value+step where table_name=tableName;#commit;RETURN ret_val;END 从日志中分析，事务1更新了表sys_sequence，持有了sys_sequence相关记录的X锁，然后事务1在等待唯一索引UNIQUE_INDEX_CUSTOMER_ID_AND_YEAR的RECORD LOCKS。然后看事务2的日志：12345678910111213141516*** (2) TRANSACTION:TRANSACTION 115900880, ACTIVE 0 sec starting index readmysql tables in use 2, locked 24 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 1925817, OS thread handle 139989577422592, query id 161369834 10.199.134.32 root statisticsselect next_value into ret_val from sys_sequence where table_name= NAME_CONST(&apos;tableName&apos;,_utf8&apos;tx_credit_info&apos; COLLATE &apos;utf8_general_ci&apos;) for update*** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 11344 page no 4 n bits 80 index UNIQUE_INDEX_CUSTOMER_ID_AND_YEAR of table `xqy_test`.`tx_credit_info` trx id 115900880 lock_mode X locks gap before recRecord lock, heap no 10 PHYSICAL RECORD: n_fields 3; compact format; info bits 0 0: len 8; hex 8000000007979427; asc &apos;;; 1: len 4; hex 32303137; asc 2017;; 2: len 4; hex 80000bbd; asc ;;*** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 10619 page no 4 n bits 424 index idx1_sys_sequence of table `xqy_test`.`sys_sequence` trx id 115900880 lock_mode X locks rec but not gap waitingRecord lock, heap no 315 PHYSICAL RECORD: n_fields 2; compact format; info bits 0 事务2可以看到持有了UNIQUE_INDEX_CUSTOMER_ID_AND_YEAR的RECORD LOCKS，然后事务2在等待sys_sequence相关记录的X锁。 最终循环等待，导致死锁，最后mysql回滚了事务2。 解决方案从日志上看是并发情况下，gen_id方法引起的错误。该方法只有在测试环境下存在，release环境和prod环境使用的阿里云的自增机制，资源不在同一个库里。guox推测线上不会出现这样的问题，release发布之后拭目以待。 长久的解决方案：将测试环境的自增机制的资源放到其他的库中；或者修改交互方案，改变ajax并发的请求，使用循环http请求的方式(以前的业务一般都是这种方式，所以从来没有暴露过死锁的问题)。]]></content>
      <categories>
        <category>DBA</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于jackson转化json的原理]]></title>
    <url>%2F2018%2F04%2F11%2F%E5%85%B3%E4%BA%8Ejackson%E8%BD%AC%E5%8C%96json%E7%9A%84%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[背景web工程中，数据交互是不可避免的，相比xml，json是现在流行的数据交互。在调试接口中，发现返回字段的大小写不是我所期望的，原本应该返回的nNum字段变成了nnum，这样就导致和前端约定的有出入了。 jacksonweb工程中，比较流行的框架是springMVC+spring+mybatis。数据交互由springMVC完成，但是springMVC也不是自己序列化json的，它将这个工作交给了jackson。jackson对object进行序列化的过程中确实存在key大写变小写的问题。看我娓娓道来。 序列化原理这里只对jackson的序列化原理做阐述，其他的序列化工具不一定是相同的原理，不可套用。众所周知，json的数据格式是，key:value的形式，现在的问题就出现在key的大小写这边。下面我们对其进行一定的测试：object中的field都有其对应的get,set方法，一般都会选择是IDE自动生成。如下图所示：其运行的结果当然没有问题： jackson在序列化的时候如何定义key呢？jackson会获取field对应的get方法方法名，比如getXxx，然后进行将get进行截断，变成Xxx，最后将其小写，变成xxx。 如果我们将xxx改成xXx，但是没有更改其get方法，key仍然是xxx，并不是我们期望的xXx： 网上的很多博客基本都只提及大写转小写，其实不然，jackson只会将连续的大写转换成小写，如果中间断了，之后的大写字符也不会处理了，并且jackson是从开头检测的，如果开头就是小写，那么之后的大写字符也不会处理了。测试如下： 如何避免正如网上的博客所说，你需要在field和其对应的get方法上加上对应的标签，然后jackson在序列化的时候就会以你的field名称为key： 总结 spring的序列化工作是由jackson完成(你也可以配置其他的序列化工具) jackson序列化的key定义与field名称无关，反而和其get方法名称有关 jackson的大写转小写从开头检测，并且一定是连续的 jackson的这种序列化机制是可以避免的，这样可以以field名称作为key]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>问题排查</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python网络数据采集——存储数据]]></title>
    <url>%2F2018%2F03%2F25%2Fpython%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E2%80%94%E2%80%94%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[文件下载12345678910111213抛出一个问题：对于爬虫而言，链接中的文件是下载呢，还是只是保存url链接，使用时直接链路？这里我总结我所知道的优缺点：不下载优点：1.爬虫运行更快，耗费的流量更少2.可以节省很多的存储空间3.存储url的代码更容易写，不需要实现文件下载代码4.降低目标主机服务器的负载缺点：1.盗链，每个网站可能会实施防盗链措施2.文件仍然在别人服务器上，跟着别人的节凑走3.盗链的图片容易改变 在python中可以引入 urlretrieve 进行文件的直接下载，在下载之前找到链接是必须的，这就要使用之前的 find 函数了。1234567891011from urllib.request import urlopenfrom urllib.request import urlretrievefrom bs4 import BeautifulSouphtml = urlopen(&quot;http://www.pythonscraping.com/&quot;)bsObject = BeautifulSoup(html, &quot;html.parser&quot;)b = bsObject.find(&quot;a&quot;, &#123;&quot;id&quot;: &quot;logo&quot;&#125;)img = b.find(&quot;img&quot;)location = img[&quot;src&quot;]print(location)urlretrieve(location, &quot;logo.jpg&quot;) 用爬虫下载文件会使你的电脑处于危险的环境当中，因为不知道下载的文件是否恶意软件。如果不能确定下载的文件是否安全，请不要轻易尝试这个.py脚本。 以csv格式存储csv，Comma-Separated Values，逗号分隔值，是存储表格数据的常用格式，如：1234fruit,costapple,1.00banana,0.30pear,1.25 参考以下代码，如何将数据写入到csv格式文件中：12345678910111213141516# 这边在引入的时候要注意，千万不要引入 python3 框架中的模块import _csv as csv# import ospath = &quot;./csv/test.csv&quot;# if not os.path.exists(path):# os.mkdir(path)with open(path, &apos;w+&apos;) as csvFile: try: writer = csv.writer(csvFile) writer.writerow((&apos;number&apos;, &apos;number plus 2&apos;, &apos;number times 2&apos;)) for i in range(10): writer.writerow((i, i+2, i*2)) finally: csvFile.close() python 中新建文件的机制考虑的非常周到，如果文件不存在则会创建。然后注意引入代码上面的注释，python3 框架中也存在csv模块，应该引入内置申明中的包。 csv 的数据形式和html中的表格数据展示相似，所以一般用来存储爬取到的表格中的数据。1234567891011121314151617181920212223import _csv as csvfrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen(&quot;http://en.wikipedia.org/wiki/Comparison_of_text_editors&quot;)bsObj = BeautifulSoup(html, &quot;html.parser&quot;)# 获取页面上的第一个表格table = bsObj.find_all(&quot;table&quot;, &#123;&quot;class&quot;: &quot;wikitable&quot;&#125;)[0]# print(table)rows = table.find_all(&quot;tr&quot;)# print(rows)path = &quot;./csv/table.csv&quot;with open(path, &quot;wt&quot;, newline=&quot;&quot;, encoding=&quot;utf-8&quot;) as csvFile: writer = csv.writer(csvFile) try: for row in rows: csvRow = [] for cell in row.find_all(&#123;&quot;td&quot;, &quot;th&quot;&#125;): csvRow.append(cell.get_text()) writer.writerow(csvRow) finally: csvFile.close() mysql整合之所以需要mysql，因为爬取的数据不能光在内存中处理，还需要进行持久化处理。因为我们可能需要将数据进行展示或者进行后一步的处理。 关于mysql的下载，自行google。macOS系统 可以使用 brew install mysql命令下载。这里使用 pymysql 这个第三方工具整合 mysql ，可以使用pip进行下载。12345678910111213141516import pymysql# 打开数据库db = pymysql.connect(&quot;localhost&quot;, &quot;root&quot;, None, &quot;wyc&quot;)# 创建游标cursor = db.cursor()# 执行sql语句cursor.execute(&quot;select version()&quot;)# 打印信息print(cursor.fetchone())# 关闭数据库db.close() 这段程序有两个对象:连接对象(conn )和光标对象(cur )，一个连接可以有很多个光标。一个光标跟踪一种状态 (state)信息，用完光标和连接之后，千万记得把它们关闭。如果不关闭就会导致连接泄漏 (connection leak)。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python网络爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python网络数据采集——使用API]]></title>
    <url>%2F2018%2F03%2F19%2Fpython%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8API%2F</url>
    <content type="text"><![CDATA[API在网络爬虫的世界里，你可以通过爬取html页面上的信息来收集信息，而使用开放的API接口是另一种方式。这里开放的API接口指的是：服务器可访问的http请求。其实html中的数据也是通过访问http请求获得的，但是这里是直接利用请求返回的数据。为何是可访问的？因为对于http请求，服务器或许是要做验证的，一般的post的请求必须做验证，get请求相对放宽限制。 google API对于API方面的开放，业界没有一家企业做的比google还好了，而且大部分都是免费的，只是每个月需要限制访问的流量。对于API的访问，google是有做校验的，需要相应接口的key，读者可以按照以下的步骤获取(但是首先你必须可以翻墙)： 首先拥有google的账号，没有的话自己去注册 访问api控制台，查询你需要的api 找到对应的api，生成相关的秘钥即可 生成的key，可以去凭证中查看 这里以google的maps接口为例，如下图搜索maps，需要地图相关的API：你可以通过点击API和服务中的库跳转到上图界面：google中你可以在信息中心界面查看你API访问的情况：对于访问时需要的key可以在凭证中查看： 然后你就可以使用key来通过API获取信息了：123456789比如：# 将街道解析成经纬度https://maps.googleapis.com/maps/api/geocode/json?address=1+Science+Park+Boston+MA+02114&amp;key=&lt;你的API key&gt;# 用 Time zone(时区)API 获取任意经纬度的时区信息https://maps.googleapis.com/maps/api/timezone/json?location=42.3677994,-71.0708078&amp;timestamp=1412649030&amp;key=&lt;你的 API key&gt;# 用地点经纬度获取对应的海拔高度https://maps.googleapis.com/maps/api/elevation/json?locations=42.3677994,-71.0708078&amp;key=&lt;你的 API key&gt; 解析json如果使用http访问的API接口，你必然会遇到解析json的问题。python中可以使用自带的json包进行解析。这里以 http://freegeoip.net/json/ 为例，来获取ip地址对应的城市：12345678910import jsonfrom urllib.request import urlopendef getCountry(ipAddress): response = urlopen(&quot;http://freegeoip.net/json/&quot;+ipAddress).read().decode(&apos;utf-8&apos;) responseJson = json.loads(response) return responseJson.get(&quot;country_code&quot;)print(getCountry(&quot;121.97.110.145&quot;)) 对于一些多重复杂的json表达式，loads方法将其解析为key-value的形式：1234567jsonString = &apos;&#123;&quot;arrayOfNums&quot;:[&#123;&quot;number&quot;:0&#125;,&#123;&quot;number&quot;:1&#125;,&#123;&quot;number&quot;:2&#125;], &quot;arrayOfFruits&quot;:&apos; \ &apos;[&#123;&quot;fruit&quot;:&quot;apple&quot;&#125;,&#123;&quot;fruit&quot;:&quot;banana&quot;&#125;,&#123;&quot;fruit&quot;:&quot;pear&quot;&#125;]&#125;&apos;jsonObj = json.loads(jsonString)print(jsonObj.get(&quot;arrayOfNums&quot;))print(jsonObj.get(&quot;arrayOfNums&quot;)[1])print(jsonObj.get(&quot;arrayOfNums&quot;)[1].get(&quot;number&quot;)+jsonObj.get(&quot;arrayOfNums&quot;)[2].get(&quot;number&quot;))print(jsonObj.get(&quot;arrayOfFruits&quot;)[2].get(&quot;fruit&quot;)) wiki百科实践这里我们仍然以wiki作为例子，在wiki百科的历史编辑区中，会记录编辑人员的ip地址，我们用上面的方法再加上网页跳转的形式，来获取相关的地区信息。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#! /usr/local/bin/python3# encoding:utf-8from urllib.request import urlopenfrom urllib.error import HTTPErrorfrom bs4 import BeautifulSoupimport jsonimport datetimeimport randomimport rerandom.seed(datetime.datetime.now())def getLinks(articleUrl): html = urlopen(&quot;http://en.wikipedia.org&quot; + articleUrl) bsObj = BeautifulSoup(html, &quot;html.parser&quot;) return bsObj.find(&quot;div&quot;, &#123;&quot;id&quot;: &quot;bodyContent&quot;&#125;).findAll(&quot;a&quot;, href=re.compile(&quot;^(/wiki/)((?!:).)*$&quot;))def getHistoryIPs(pageUrl): # Format of history pages is: http://en.wikipedia.org/w/index.php?title=Title_in_URL&amp;action=history pageUrl = pageUrl.replace(&quot;/wiki/&quot;, &quot;&quot;) historyUrl = &quot;http://en.wikipedia.org/w/index.php?title=&quot;+pageUrl+&quot;&amp;action=history&quot; print(&quot;history url is: &quot; + historyUrl) html = urlopen(historyUrl) bsObj = BeautifulSoup(html, &quot;html.parser&quot;) # finds only the links with class &quot;mw-anonuserlink&quot; which has IP addresses instead of usernames ipAddresses = bsObj.findAll(&quot;a&quot;, &#123;&quot;class&quot;: &quot;mw-anonuserlink&quot;&#125;) addressList = set() for ipAddress in ipAddresses: addressList.add(ipAddress.get_text()) return addressListdef getCountry(ipAddress): try: response = urlopen(&quot;http://freegeoip.net/json/&quot;+ipAddress).read().decode(&apos;utf-8&apos;) except HTTPError: return None responseJson = json.loads(response) return responseJson.get(&quot;country_code&quot;)links = getLinks(&quot;/wiki/Python_(programming_language)&quot;)while(len(links) &gt; 0): for link in links: print(&quot;-------------------&quot;) historyIPs = getHistoryIPs(link.attrs[&quot;href&quot;]) for historyIP in historyIPs: country = getCountry(historyIP) if country is not None: print(historyIP+&quot; is from &quot; + country) newLink = links[random.randint(0, len(links)-1)].attrs[&quot;href&quot;] links = getLinks(newLink) 其实上面的方法再前面的章节中已经提及过，现在增加的只是将html上爬取的信息，再通过API的加工得到最终的信息。只是想要告诉大家，爬虫不只是解析html那么简单。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python网络爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python网络数据采集——链接爬取和跳转]]></title>
    <url>%2F2018%2F03%2F14%2Fpython%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E7%88%AC%E5%8F%96%E5%92%8C%E8%B7%B3%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[网页跳转在网络数据采集中，如果只对单页面进行操作，那么这个爬虫将毫无意义。因为采集的数据往往是分散在不同的网页的，所以对于单页面而言，爬虫需要采集页面中的链接，并且进行跳转。网页跳转，在正在采集的页面中收集新的链接，然后将新的链接传入到采集程序中，如此之后，采集程序就已经在新的页面采集信息了。 举个例子我们以维基百科的一个页面为例，从主页面开始，采集 id 为 mw-content-text 的段落 p 和 id 为 ca-edit 的 span 中的 a 链接标签。读者可以参考以下代码：12345678910111213141516171819202122232425262728from urllib.request import urlopenfrom bs4 import BeautifulSoupimport repages = set()def getLinks(pageUrl): global pages html = urlopen(&quot;http://en.wikipedia.org&quot;+pageUrl) bsObj = BeautifulSoup(html, &quot;html.parser&quot;) # print(bsObj) try: print(bsObj.h1.get_text()) print(bsObj.find(id=&quot;mw-content-text&quot;).find_all(&quot;p&quot;)[0]) print(bsObj.find(id=&quot;ca-edit&quot;).find(&quot;span&quot;).find(&quot;a&quot;).attrs[&apos;href&apos;]) except AttributeError: print(&quot;页面缺少一些属性!不过不用担心!&quot;) for link in bsObj.find_all(&quot;a&quot;, href=re.compile(&quot;^(/wiki/)&quot;)): if &apos;href&apos; in link.attrs: if link.attrs[&apos;href&apos;] not in pages: # 我们遇到了新页面 newPage = link.attrs[&apos;href&apos;] print(&quot;----------------\n&quot;+newPage) pages.add(newPage) getLinks(newPage)getLinks(&quot;&quot;) 程序先从 http://en.wikipedia.org 网页开始，然后查找以 /wiki/ 开头的链接，之后拼接到 http://en.wikipedia.org 形成新的链接。对于重复的链接程序不进行采集，程序中体现了递归的概念，理论上你很难等到程序的结束。 六度空间理论所谓六度空间理论是数学界中的一种猜想，对于一个陌生人，你可以通过至多6个人认识他，即你与陌生人中间的连接点不会超过6个人。我们可以将这个猜想应用到爬虫程序中，即通过一个人的简介，我们可以爬取到另一个人的简介。这里还是以维基百科为例，从 Kevin_Bacon 开始，爬取其他人的信息：123456789101112131415161718from urllib.request import urlopenfrom bs4 import BeautifulSoupimport datetimeimport randomimport rerandom.seed(datetime.datetime.now())def getLinks(articleUrl): html = urlopen(&quot;http://en.wikipedia.org&quot;+articleUrl) bsObj = BeautifulSoup(html, &quot;html.parser&quot;) return bsObj.find(&quot;div&quot;, &#123;&quot;id&quot;: &quot;bodyContent&quot;&#125;).findAll(&quot;a&quot;, href=re.compile(&quot;^(/wiki/)((?!:).)*$&quot;))links = getLinks(&quot;/wiki/Kevin_Bacon&quot;)while len(links) &gt; 0: newArticle = links[random.randint(0, len(links)-1)].attrs[&quot;href&quot;] print(newArticle) links = getLinks(newArticle) 程序中使用随机数字，从新的链接集合中随机选取，并使用新的链接再次爬取。 scrapy写网络爬虫，你不得不重复一些简单的操作：找出页面上所有的链接，区分内链与外链，跳转到新的页面。像这种重复的工作可以交给第三方工具类来处理，scrapy就是这样一款工具。曾经的 scrapy 是一个傲娇的工具，只支持 python2.7 版本，软件的不普遍支持性会导致软件的不可用。好消息就是，现在的 scrapy 在 python3.x 的环境下也是支持的。$pip3 install scrapy 即可下载。如果是python2.x版本的，务必使用pip命令。scrapy 的使用需要重新创建一个工程(在这里演示如何获取网页的title)： $scrapy startproject wikiSpider 创建新工程 在 spiders 目录下创建 ArticleSpider.py文件，名字也可自取 在 item.py 文件中定义类 Article scrapy crawl article 运行程序(这行命令会用条目名称 article 来调用爬虫(不是类名，也不是文件名，而是由 ArticleSpider 的 name = “article” 决定的)) item.py文件中应如此定义：1234567891011121314151617181920# -*- coding: utf-8 -*-# Define here the models for your scraped items## See documentation in:# https://doc.scrapy.org/en/latest/topics/items.html# import scrapyfrom scrapy import Item, Field# class WikispiderItem(scrapy.Item):# # define the fields for your item here like:# # name = scrapy.Field()# passclass Article(Item): # define the fields for your item here like: # name = scrapy.Field() title = Field() scrapy的每个Item(条目)对象表示网站上的一个页面。当然，你可以根据需要定义不同的条目(比如url、content、header image等)，但是现在我只演示收集每页的title字段 (field)。 ArticleSpider.py 文件中写入如下程序：123456789101112131415161718192021222324#! /usr/local/bin/python3# encoding:utf-8from scrapy import Spider# 这里的引用简直有毒 scrapy crawl article# 像这样 from wikiSpider.wikiSpider.items import Article 编译器是正确的，但是终端执行是错误的# 要如此引用 from wikiSpider.items import Article 但是编译器会报错# 所以建议使用相对路径from .. items import Article# from wikiSpider.wikiSpider.items import Articleclass ArticleSpider(Spider): name = &quot;article&quot; allowed_domains = [&quot;en.wikipedia.org&quot;] start_urls = [&quot;http://en.wikipedia.org/wiki/Main_Page&quot;, &quot;http://en.wikipedia.org/wiki/Python_%28programming_language%29&quot;] def parse(self, response): item = Article() title = response.xpath(&apos;//h1/text()&apos;)[0].extract() print(&quot;Title is: &quot;+title) item[&apos;title&apos;] = title return item 注意文件中引入项目的注释。 如果正确，终端中运行的结果应该是：12Title is: Main PageTitle is: Python (programming language)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python网络爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python网络数据采集——BeautifulSoup]]></title>
    <url>%2F2018%2F03%2F11%2Fpython%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E2%80%94%E2%80%94BeautifulSoup%2F</url>
    <content type="text"><![CDATA[BeautifulSoupBeautifulSoup 是 python 中最受欢迎的 html 解析库。如果 BeautifulSoup 这个第三方库不能解决问题，读者可以考虑另外两个库：12345678xml：可以用来解析 HTML 和 XML 文档，以非常底层的实现而闻名于世，大部分源代码是用 C 语言写的。虽然学习它需要花一些时间(其实学习曲线越陡峭，表明你可以越快地学会它)，但它在处理绝大多数 HTML 文档时速度都非常快。如果遇到的是引用第三方库处理时遇到的性能瓶颈，可以考虑。HTML parser：这是 Python 自带的解析库(https://docs.python.org/3/library/html.parser.html )。因为它不用安装(只要装了 Python 就有)，所以可以很方便地使用。 find 和 find_all在 BeautifulSoup 中，这两个函数是经常使用的，专门用来查找 html 中指定的标签。接下来笔者用链接：http://www.pythonscraping.com/pages/warandpeace.html 做为例子来使用这两个函数。123456789101112131415#! /usr/local/bin/python3# encoding:utf-8from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen(&quot;http://www.pythonscraping.com/pages/warandpeace.html&quot;)bsObject = BeautifulSoup(html, &quot;html.parser&quot;)# 使用 findAll 方法查找 指定标签 指定class属性的nameList = bsObject.find_all(&quot;span&quot;, &#123;&quot;class&quot;: &quot;green&quot;&#125;)for name in nameList: print(name.get_text())# 像这样，你就抓取了html中所有的人名# 因为经过分析，发现 span 标签且 class 为 green 的内容都是小说的人名 通过源码你会发现 find_all 方法拥有以下参数：name=None, attrs={}, recursive=True, text=None, limit=None, **kwargs，在此说明一下 以前的 BeautifulSoup 版本方法名是 findAll。在开发的过程中，95%的情况下你只会使用到 name attars 这两个参数，但是在这里还是要说明一下其他参数的作用： name html中tag的名称 比如span h1 你可以传入多个tag的集合来查找 这个集合是一个set结构 attrs 字典集 用来指定 tag的一些属性 比如class width height recursive 表示在查找时 你是否想要递归 到子标签中查找 默认为True 递归 text 直接查找文本 有相同文本的匹配 limit 指定查找的数量 查找的顺序是按照 html 的顺序来的 默认 全部 kwargs 字典集 查找具有指定属性的标签 业界认为是冗余的设计 不推荐使用 并且存在缺陷 比如无法指定class属性 因为class是python的保留关键字123456789hList = bsObject.find_all(&#123;&quot;h1&quot;, &quot;h2&quot;, &quot;h3&quot;&#125;)print(hList)tList = bsObject.find_all(text=&quot;Anna Pavlovna&quot;)print(tList)nameList = bsObject.find_all(class_=&quot;green&quot;)print(nameList)# 但是你可以使用 class_ 来代替 find 的使用方式和 find_all 差不多，只是缺少了参数 limit 而已。 导航树先抛出一个问题：find_all 函数是通过标签的名称和属性来查找标签的，但是如果需要通过标签在文档中的位置来查找标签，该如何？引入导航树(Navigating Trees)的概念就是为何解决这个问题，比如：bsObject.tag.subTag.anotherSubTag。 在这里我们使用虚拟的购物网站：http://www.pythonscraping.com/pages/page3.html 来作为实例。对于使用导航树，你只需要学会处理三个问题： 处理子标签和后代标签 处理兄弟标签 处理父标签 子标签和后代标签12345678from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen(&quot;http://www.pythonscraping.com/pages/page3.html&quot;)bsObject = BeautifulSoup(html, &quot;html.parser&quot;)for child in bsObject.find(&quot;table&quot;, &#123;&quot;id&quot;: &quot;giftList&quot;&#125;).children: # print(type(child)) print(child) 子标签和后代标签在定义上是不同的：子标签只有一个层级的差距，后代标签则是一个或者多个层级差距。Tag对象 函数 children 和 descendants 分别获取子标签和后代标签。 兄弟标签兄弟标签表示 Tag类型相同且处于同一层级的，Tag对象 函数 next_siblings 和 previous_siblings 分别获取定位标签的上面的所有Tag和下面所有Tag。123456789101112比如：&lt;tr class=&quot;tr1&quot;&gt;&lt;/tr&gt;&lt;tr class=&quot;tr2&quot;&gt;&lt;/tr&gt;&lt;tr class=&quot;tr3&quot;&gt;&lt;/tr&gt;&lt;tr class=&quot;tr4&quot;&gt;&lt;/tr&gt;&lt;tr class=&quot;tr5&quot;&gt;&lt;/tr&gt;现在定位tr3，next_siblings可获取tr4，tr5；previous_siblings可获取tr2，tr1for sibling in bsObject.find(&quot;table&quot;, &#123;&quot;id&quot;: &quot;giftList&quot;&#125;).tr.next_siblings: print(sibling)# 与第一段代码相比，可以发现少了表格标题的tr，因为第一个tr标签不能将自己视为兄弟# 当然也可以使用next_sibling 和 previous_sibling 只不过获取到的是距离最近的那一个 父标签查找父标签在提取信息的过程中是偶尔的情况下才会用到的，如果那个信息比较深入，并且它的子标签有明显的特点方便查找，可以考虑使用。1234567print(bsObject.find(&quot;img&quot;, &#123;&quot;src&quot;: &quot;../img/gifts/img1.jpg&quot;&#125;) .parent.previous_sibling.get_text())# 1.找到Tag类型是 img 的且 src 是 ../img/gifts/img1.jpg# 2.向上寻找到父标签 即 td# 3.找到 td 标签的上一个兄弟标签 还是 td# 4.得到里面的 text 文本，即商品的价格]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python网络爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python网络数据采集——网络连接]]></title>
    <url>%2F2018%2F03%2F09%2Fpython%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[初识urllib要采集信息，首先你需要将html文件抓取到本地，python中使用urllib这个包来进行网络连接，并且进行html的抓取，如果是python2.x版本，使用的是urllib2这个包。下面贴出一段代码，读者可自行运行：123456#! /usr/local/bin/python3# encoding:utf-8from urllib.request import urlopenhtml = urlopen(&quot;http://pythonscraping.com/pages/page1.html&quot;)print(html.read()) 运行BeautifulSoupBeautifulSoup是第三方的python模块，所以需要进行安装，在此只介绍macOS系统的下载方式：12345678910python中一般使用pip来下载第三方的依赖包$sudo easy_install pip 首先确保你的系统中有pip命令$pip install beautifulsoup4 即可下载如果使用的是python3.x版本$pip3 install beautifulsoup4 需要在pip后面加上数字3一般macOS系统自带的是python2.x，在这里推荐使用brew来下载python3$brew search python$brew install python3 即可 BeautifulSoup在代码中的作用相当于html解析器，然后读者可以通过该模块的方法来获取一些html中的标签。在这里贴出BeautifulSoup的代码：123456789#! /usr/local/bin/python3# encoding:utf-8from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen(&quot;http://pythonscraping.com/pages/page1.html&quot;)bsObj = BeautifulSoup(html, &quot;html.parser&quot;)print(bsObj.h1) BeautifulSoup比较老的版本构造时不需要传入”html.parser” 参数，新的版本中，虽然结果仍然打印了出来，但是在运行过程中会报错。html.parser 是为了告诉BeautifulSoup 我使用的是 html 解析器。读者可以参考官方手册了解更过BeautifulSoup的知识点。 可靠的网络连接像上面写的代码，很有可能抛出异常，谁也不知道这个url是否可靠，如果服务器宕机了怎么办？等等问题，都会使得程序终止。想象一下，你的爬虫在晚上执行，你想早上起来查看收集的数据，但是由于中间的错误，导致之后的程序没有执行，你不得不再等待一天的时间。其实说了这么多，就是在代码层面中捕获异常。可靠的网络连接，要保证程序在运行过程中，即使中途遇到了错误，也能保证余下的代码可执行。大家参考下面的代码来发现不同点：12345678910111213141516171819202122from urllib.request import urlopenfrom urllib.request import HTTPErrorfrom bs4 import BeautifulSoupdef get_h1(url): try: html = urlopen(url) except HTTPError as e: return None else: try: bsObj = BeautifulSoup(html, &quot;html.parser&quot;) h1 = bsObj.h1 except AttributeError as e: return None return h1if __name__ == &apos;__main__&apos;: h1 = get_h1(&quot;http://pythonscraping.com/pages/page1.html&quot;) print(h1) 上面这段代码使用了python中经典的try…exception…else的格式，这样保证了即使发生了异常，也能保证else中的代码被执行。在python中获取异常可以使你的代码更加健壮，也是一个好的习惯。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python网络爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python网络数据采集——简介]]></title>
    <url>%2F2018%2F03%2F04%2Fpython%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[简介网络数据采集(网络爬虫)，是一种网络数据的采集方式，又称为网页抓屏，数据挖掘，网络收割或者其他类似的版本。网络数据采集是一种通过多种手段收集网络数据的方式，比如通过api交互，当然如果你有毅力可以选择手动摘抄，人们最常用的估计是使用浏览器进行网页的浏览，这其实也是一种网络数据采集。注：之后相关文章都使用网络爬虫这个术语。 意义已经有浏览器了，为何还需要网络爬虫呢？ 首先浏览器的搜索是主流的，你不一定能从中找到有用的信息。 你需要的数据可能来自不同的网站，为了分析和对比。 网站可能没有api(接口)提供给外部使用。 网络爬虫可以解决这些问题，爬取的数据也可以进行保存和更新，你也可以通过特定的方式进行展示，比如图表。实际上，这些都是手动摘抄都可以办到的，但是谁又会整天看着数据有没有改变，然后去更新手头的数据呢？ 扩展之前笔者是学习过一段时间的python基础的，但是笔者糊口的语言是java，python只是笔者业余的爱好罢了。笔者认为python的几大领域为网络数据采集，自动化和大数据分析，所以读者可以在学习了python的基础上选择一个方向再深入的研究。 关于python的基础知识，笔者的笔记并没有开放在网站上，读者可以自行寻找。 关于网络爬虫，笔者认为基本对现在的业务没有什么帮助，因为网络爬虫对于一个产品的稳定性来说一定是大打折扣的(因为谁有能保证你收集的信息来源是否还存在呢？)。但是可以给你的生活带来一些乐趣，人生在世，技多不压身嘛。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python网络爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络——NAT]]></title>
    <url>%2F2018%2F02%2F20%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94NAT%2F</url>
    <content type="text"><![CDATA[简介NAT(Network Address Translation)，网络地址转换。NAT可以看成一个开放到公网(全球因特网)上的公用路由器，用来代理一个子网下的所有主机的请求和应答。NAT在转换的时候，会将主机的ip地址和端口号替换成公用路由器的ip地址和端口，并且维护在一张表中进行一一对应，为了将返回的数据应答到唯一的主机上。 作用由于ipv4规则的ip地址消耗殆尽，不可能为所有的主机都分配真正的公网ip，所以很多主机ip地址都是虚拟的ip地址，并且有统一的代理服务器进行消息的请求和应答。NAT的作用就是将私有的ip地址转换成全球因特网承认的ip地址，私有的ip地址有三种：①10.0.0.0~10.255.255.255/8 ②172.16.0.0~172.31.255.255/12 ③192.168.0.0~192.168.255.255/16 这些IP地址是用于私有的网络。NAT使得一个组织局域网中的主机都能互相访问，但是想要访问公网就只有一条出路，这样也可以保证局域网中信息的安全性。 例子假设用户王某坐在家庭主机10.0.0.1傍边，请求域名为 www.bilibili.com (ip地址为112.49.19.4)的web服务器(端口为80)上的一个web页面。主机10.0.0.1为其指派了任意的端口2233，并将请求的报文发送到LAN中。NAT路由器收到该数据请求，为其生成了新的端口号5000，并将ip地址替换成广域网的ip地址138.76.29.7，然后继续想目标服务器请求。具体步骤如下： 10.0.0.1:2233 发送报文 —&gt; LAN LAN —&gt; NAT路由器 —&gt; 替换成138.76.29.7:5000 并维护一张映射表 138.76.29.7:5000 发送请求报文 —&gt; 112.49.19.4:80 返回数据到WAN —&gt; NAT路由器 NAT路由器从维护的映射表中获取私有ip地址 返回数据 —&gt; LAN —&gt; 10.0.0.1:2233 读者以前可能发现，百度的ip地址和主机的ip地址不同，因为一个是公网的，主机的只是私有的ip地址，中间就是利用了NAT。代理服务器，即NAT在跳转的当中可能存在很多个，即一个请求可能通过n个NAT。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>网路协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络——DHCP]]></title>
    <url>%2F2018%2F02%2F20%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94DHCP%2F</url>
    <content type="text"><![CDATA[简介DHCP(Dynamic Host Configuration)，即动态主机配置协议。由于DHCP具有能将主机连接进一个网络的网络相关方面的自动能力，故它又常被称为即插即用协议。DHCP协议维护了一张表(存放于DHCP服务器)，用来存放现子网中可以分配的ip，如果有主机离开或者接入子网，则DHCP服务器会更新这张表。 作用一旦一个组织获取了子网的ip地址，网络管理员需要给组织中的主机都配置ip(读者可以参考网络攻防——ip地址了解一个组织内主机ip地址配置过程)。DHCP的问世是为了节省网络管理员手动配置主机ip地址的时间。网络管理员只需要配置DHCP，以使某给定主机每次与网络连接时能得到一个相同的ip地址，或者某主机将被分配一个临时的ip地址，该地址每次与网络连接时也许是不同的。 协议的步骤对于一台新到达的主机而言，DHCP协议是一个4个步骤的过程：1234567891.DHCP服务器发现。一台新的主机接入子网时首要的任务时让DHCP服务器发现它。所以主机会在UDP分组中向端口67发送一个DHCP发现报文，并且这个数据时广播的，因为主机现有的情况是不知道DHCP服务器的具体ip地址的。2.DHCP服务器提供。DHCP服务器收到这个报文时，用一个DHCP提供报文向主机作出应答。这边也是使用的广播的形式，因为在子网中可能存在多个DHCP服务器，主机可以根据各个DHCP服务器返回的信息进行择优选择。3.DHCP请求。主机从一个或多个DHCP服务器中挑选一个，并向选中的服务器提供一个DHCP请求报文进行响应。4.DHCP ACK。服务器用DHCP ACK报文对主机请求报文进行响应，证实所要求的参数。 一旦主机完成了以上的4步，交互便完成了，主机也接入了子网，并拥有了子网中唯一的ip地址。 缺陷从移动性的角度看，DHCP确实有不足之处。因为每当节点连接到一个新的子网时，要从DHCP得到一个新的ip地址，当一个移动节点在子网之间移动时，就不能维持与远程应用之间的TCP连接。之后会研究移动IP(在此记录，以便之后补充)。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>网络协议</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络攻防——ip地址]]></title>
    <url>%2F2018%2F02%2F18%2F%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2%E2%80%94%E2%80%94ip%E5%9C%B0%E5%9D%80%2F</url>
    <content type="text"><![CDATA[这边先预设几个问题 一个组织分配ip地址的过程(涉及到ISP等概念) a,b,c,d,e类的ip地址 如何知道一个主机的子网掩码，它第一跳的路由地址，dns服务器的地址 DHCP动态分配地址 什么是网络地址转换(NAT) ipv4和ipv6的区别 基本概念ip地址目前有ipv4和ipv6两种，这里以ipv4举例。ip地址在整个网络中相当于地址的存在，只有知晓了地址，才能知道向哪里送信，信件又使从哪里发出来的。ipv4地址编码长度为32，即32个比特位，即4个字节，每个比特位只有1或者0，所以ipv4总共有2^32个可能的ip地址。(但是很遗憾，尽管看起来很多，ipv4方式的编码ip已经消耗殆尽，所以ipv6孕育而生) 读者可以使用ipconfig(linux系统使ifconfig)来查看自己的ip地址。读者会发现，显示的ip信息并不是以二进制形式显示，而是十进制。那是为了方便人们阅读，一般使用点分十进制记法书写，然而计算机在交流时仍然只认识二进制记法。1234如：193.32.216.9 的二进制记法是：11000001 00100000 11011000 00001001可以看出以8个比特，即一个字节作为一个分隔 在全球因特网中的每台主机和路由器上的每个接口，必须有一个全球唯一的ip地址(在NAT后面的接口除外，NAT即网络地址转换，之后会提及这个概念)。然而，这些地址不能随意地自由选择，一个接口ip地址的一部分需要由其连接的子网来决定。 标准类别的ip地址ip地址由网络地址和主机地址组成。根据网络地址的比特位数，将ip地址分为A,B或C类网络，网络地址的比特位数分别被限制为8,16或24位，这是一种被称为分类编址的编址方案。 在计算机网络中，读者可能看到形如：233.1.1.0/24的ip地址记法，这是子网地址的写法，其中/24记法有时称为子网掩码，表示32位比特中的最左侧24比特定义了子网地址，即该子网中所有主机分配到的ip地址的前24位都是相同的，即网络地址相同。比如：12一个子网的ip地址为：233.1.1.0/24那么这个子网下的主机的ip地址可能是：233.1.1.1，233.1.1.4 233.1.1.128 ... 在分配子网的时候，不一定使用标准的网络地址。比如一个组织，分类到了一个C类(/24)子网，其仅能容下2^8 - 2 = 254台主机(2^8 = 256，其中的两个地址预留用于特殊的用途)，这对于许多组织来说太小了。然而一个B类(/16)子网可支持多达65534台主机，又太大了。 组织内主机分配ip地址的过程为了获取一块ip地址用于一个组织的子网，网络管理员也许首先会与他的ISP参考百度百科联系，该ISP可能会从已分配给它的更大的地址块中提供一些地址。这里假设ISP自己已被分配了地址块200.23.16.0/20：1234ISP的地址块 200.23.16.0/20 (11001000 00010111 0001)0000 00000000组织0 200.23.16.0/23 (11001000 00010111 000100)0 00000000组织1 200.23.18.0/23 (11001000 00010111 0001001)0 00000000... 加入组织0已经得到了子网地址：200.23.16.0/23，这就表示该组织下的所有主机ip地址都是以11001000 00010111 000100开头的。系统管理员一般手动配置路由器ip地址，主机地址也能手动配置，但是想象一下：如果一个组织下有数以万计的主机，那么系统管理员已哭晕在厕所。所以这里引出另一个概念DHCP(动态主机配置协议，又称即插即用协议)，主机可以自动获取一个临时的ip地址。这也解释了为什么在公司中，主机重启之后主机的ip地址会改变，因为是临时随机分配的。 DHCP笔者认为这个概念可以重新开一个章节，读者可以跳转参考计算机网络——DHCP NAT读者可以跳转参考计算机网络——NAT 如何生动形象解释这里主要解释了ip地址，子网掩码，网关等概念。读者可以参考这篇知乎文章。]]></content>
      <categories>
        <category>网络安全</category>
      </categories>
      <tags>
        <tag>网络协议</tag>
        <tag>ip地址</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产问题排查指南——排查思路]]></title>
    <url>%2F2018%2F02%2F12%2F%E7%94%9F%E4%BA%A7%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E6%8C%87%E5%8D%97%E2%80%94%E2%80%94%E6%8E%92%E6%9F%A5%E6%80%9D%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[业务指标着手需要提出的是，大部分情况还是业务应用本身引发的问题，所以应从业务指标着手。 查看gc日志 查看gc是否变得频繁，是否出现大量gc甚至full gc，这意味着内存可能发生了问题。此时应保留现场，保存内存dump。 一般应用程序中都有配置gc日志，在/usr/local/logs/gc目录下 需要特别注意，个别major gc可能是正常内存回收，并且gc后的dump也不再体现回收前的内存情况，所以不具有参考性。 一般使用MAT对内存dump进行分析，找出异常的对象，从而判断代码或者业务设计是否有需要修改的地方。 查看业务日志如果gc正常，吞吐量下降的原因可能有两个。一,外部依赖阻塞，二,服务内部资源紧张，请求排队等待。以上两种情况都可能产生蝴蝶效应，服务性能下降，请求排队的同时，内存可能堆积大量对象无法释放，这样就导致出现gc甚至full gc。主要关注两类服务，一,错误量最多的服务，二,第一个耗时异常缓慢的服务，根据错误链路找到异常的触发源。最好的做法就是预防入手，对于应用性能有准确的评估以及可能遭遇的流量高峰有所预测，这样及早做资源扩展。 日志查看手册读者可以查看xqy撰写的生产环境日志指导手册。]]></content>
      <categories>
        <category>生产环境</category>
      </categories>
      <tags>
        <tag>问题排查</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产问题排查指南——问题排查方法]]></title>
    <url>%2F2018%2F02%2F12%2F%E7%94%9F%E4%BA%A7%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E6%8C%87%E5%8D%97%E2%80%94%E2%80%94%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[问题排查方法这篇博客主要阐述排查的一些指标，对一些可能造成生产问题的注意点做指导概览。 指标分类基础指标基础指标主要有以下几点： cpu负载：cpu负载高的情况，将直接导致系统吞吐下降。 磁盘：磁盘I/O是否正常，磁盘硬件是否正常。 网络：出网入网流量是否正常，网络是否有波动情况。 中间件负载：中间件负载情况将直接影响调用链路性能。 如果能确保基础指标没有问题，这将大大加快我们排查的效率。例如，当前一段时间内生产网络出现了大幅度的波动，造成了大量服务延迟。如果你没有预先排除网络指标是否正常，而直接排查应用的服务性能问题，那最终将浪费大量的时间和精力。 核心指标线程1234线程数量：多线程并行运行的情况下，存在大量线程切换与上下文信息的同步，cpu主要负责这个。因此线程数量过多，将会直接增加切换与同步的开销，cpu负载增加。top1：无论进程还是线程，都应该 关注cpu占用率最高的那一个，有利于快速排查系统高负载的主要原因。 内存1234567系统内存空闲情况：长时间的低空闲内存，容易在大流量到来时，达到系统警戒阈值，触发OOM Killer，中断应用进程。在其他指标不变的情况下，空闲内存的大小与数据，流量息息相关。大数据与流量高峰都会加大内存开销，遇到服务阻塞，对象会在内存中堆积无法释放，最终导致内存溢出。业务应用内存情况：业务应用会有一块被分配的固定大小的堆内存空间。但配分的大小不一定总是合理的，一个典型的例子是老年代分配内存过大，新生代分配较小，大量的对象在老年代中堆积等待回收，在触发回收前，系统长时间处于空闲内存吃紧状态。 业务指标gc情况直观体现了应用内存当前的健康程度，频繁gc甚至full gc意味着应用内存出现了问题或者没有给应用分配合理的内存空间。 任何时候，日志都是问题排查不可或缺的重要指标。核心指标的异常，最终体现在日志上就是大量的系统或者业务异常，服务错误量激增[调用失败，超时]，执行耗时异常缓慢[一次服务耗时10～100s以上] 扩展读者可能对于OOM Killer这个概念不太了解，可以参考知乎上的这篇形象比喻。]]></content>
      <categories>
        <category>生产环境</category>
      </categories>
      <tags>
        <tag>问题排查</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产问题排查指南——指标查询指令与方法]]></title>
    <url>%2F2018%2F02%2F12%2F%E7%94%9F%E4%BA%A7%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E6%8C%87%E5%8D%97%E2%80%94%E2%80%94%E6%8C%87%E6%A0%87%E6%9F%A5%E8%AF%A2%E6%8C%87%E4%BB%A4%E4%B8%8E%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[生产问题指标cpu中央处理器cpu占用率与负载情况是操作系统繁忙程度的一项总要的指标，它直接影响着应用的吞吐。top命令经常用来监控linux系统状况，也直观展示了系统cpu使用情况。读者可以参考微服务问题定位——top来查看具体的细节。 线程上面的top命令同时展示了各进程cpu使用情况，一般cpu占用量自上而下排序。我们一般关注cpu占用率最高的那个，大部分情况下它就是我们的应用进程。pid列即为进程的id。 $top -Hp pid[十进制进程id] 可以查看指定进程id中各线程的cpu使用情况。 $printf ‘0x%x\n’ nid[十进制线程id] 将线程的id打印为十六进制，目的是给之后的命令提供十六进制的id。 $jstack pid[进程id] | grep ‘nid[十六进制线程id]’ -C5 –colorjstack可以查询jvm进程中的线程栈信息，grep可以从中搜索目标线程信息，可以配合源码分析负载高的原因。 $jstack pid[进程id] &gt; jstack.txt[文件] 当然你可以将当前进程栈信息输出到文件，即我们常说的线程dump，然后进行详细分析。 strace -p nid[线程id] -T strace可以查看操作系统底层的执行情况，包括执行函数和执行耗时等。 内存这里的内存指标分为系统内存和应用内存一般读者可以使用free命令查看系统内存的使用情况，读者可以参考微服务问题定位——free应用内存的查看需要jdk包下的命令： $jmap -heap pid[进程id]jmap可以查看jvm进程的内存分配与使用情况，使用的gc算法等信息。 $jmap -dump:format=b,file=[导出路径] pid[进程id]-dump:format=b,file=可以使用hprof二进制形式输出jvm的heap内容到文件，即我们常说的堆内存dump，然后可以结合MAT[内存分析工具]可以深入分析内存使用情况。注意dump是比较消耗资源的。如果现在系统的内存比较吃紧，磁盘i/o较慢，切忌手动dump，可能成为压死骆驼的最后一根稻草。 -XX:+HeapDumpOnOutOfMemoryError 一般我们会在jvm的启动中添加启动参数，这样发生OOM后jvm能够自动将当时的内存情况dump保留下来。 zip或者gzip 通常dump文件会较大，应该将原dump文件归档到备份地点或者直接移除，以释放这部分磁盘空间占用。在进行下载时也将极大减少带宽开销。 磁盘读者可以参考： 微服务问题定位——df 微服务问题定位——du 网络 dstat 是一个综合的多维度指标命令。视图中有cpu使用情况，磁盘读写，网络情况，分页统计，系统中断与上下文切换统计。这里我们主要关注网络指标net/total- 。recv：入网流量，send：出网流量。当网络传输流量长时间小于实际应用传输数据量大小时，带宽将成为应用性能瓶颈。 ifstat 读者可以参考微服务问题定位——ifstat 日志这里的日志指标分为gc日志和业务日志。 gc日志无论minor gc还是major gc，我们主要关注gc回收实际消耗时间，即为日志中的real，即STW[stop the world]时间，如果实际耗时过长，则严重影响应用性能。 业务日志业务日志一般我们关注调用时间过长或者调用失败的请求，grep ‘[0-9]{3,}ms’ *.log 查看执行耗时3位数以上的服务grep ‘,N’ *.log 查看执行失败的服务]]></content>
      <categories>
        <category>生产环境</category>
      </categories>
      <tags>
        <tag>问题排查</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在mac上搭建python环境并使用docx模块]]></title>
    <url>%2F2018%2F02%2F12%2F%E5%A6%82%E4%BD%95%E5%9C%A8mac%E4%B8%8A%E6%90%AD%E5%BB%BApython%E7%8E%AF%E5%A2%83%E5%B9%B6%E4%BD%BF%E7%94%A8docx%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[背景之前接手到一个处理word文档的项目，需要读取word内容，并对读取的字符串进行一系列的处理。一开始笔者使用java进行处理，需要引入第三方模块——poi。说实话，这个第三方模块对于word的处理不够完善，第一没有很好的写入方式，第二没有很好的样式处理机制，第三没有封装java的io流，仍然需要笔者手动创建和关闭io资源。 在之后的资料中查询到可以使用python的docx模块很快速处理word文件，使用快捷方便，几行代码就可以实现读写，并且很好的支持word的样式。但是docx模块只支持docx后缀的文档，读者可以先将doc转换docx。 搭建环境搭建平台是macOS10.12.6，一般mac上是自带python的，但是普遍版本较低，目录位置为/Library/Python，以后安装的其他版本也在相同的目录下。 终端$python –version 查看python的版本。 这边可以使用pyenv来管理python的各个版本，笔者这边使用IDE(python编译环境)管理的。如果选择pyenv，请参考。即使不使用pyenv，笔者还是要推荐参考博客中提及的brew——mac的统一依赖包管理器。如果选择IDE管理，请到官网下载PyCharm，这种方式不像pyenv可以使用命令下载python的其他版本，读者可以访问官网下载其他python版本。 IDE在栏目PyCharm下选择Pereferences，如图选择可以切换编译器的python版本：不同的版本切换对代码的编译可能会造成影响。 引入docx模块python处理word也需要引入第三方模块，一般使用pip去下载python的第三方模块。mac里面python自带easy_install。$sudo easy_install pip 输入密码下载pip，笔者强烈建议不要在系统自带的python下折腾，因为mac系统下很多软件都依赖python模块，一些操作可能导致系统的软件打开错误。 读者可以先去官网下载其他版本，然后设置环境变量，sudo vim /etc/profile，编辑添加export PATH=xxxx:$PATH，其中xxxx为下载后python的路径，参考搭建环境。 $python –version 查看版本是否更改，之后执行$sudo easy_install pip，然后在执行$pip install docx模块。 然后读者就可以在当前python版本的目录下看到pip模块和docx模块了。之后在PyCharm中切换python的版本，在编写的代码中引入docx模块即可。读者可以参考官网手册]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm——新生代,老年代和持久代]]></title>
    <url>%2F2018%2F02%2F11%2Fjvm%E2%80%94%E2%80%94%E6%96%B0%E7%94%9F%E4%BB%A3-%E8%80%81%E5%B9%B4%E4%BB%A3%E5%92%8C%E6%8C%81%E4%B9%85%E4%BB%A3%2F</url>
    <content type="text"><![CDATA[新生代，老年代和永久代java中堆是jvm所管理的最大的一块内存空间，主要用于存放各种类的实例对象。jvm中内存的分配有如下公式： 堆 = 年轻代 + 老年代 年轻代 = eden space(新生代) + from survivor + to survivor 年轻代默认值保持为堆大小的1/15，特点是对象更新速度快，在短时间内产生大量的死亡对象，并且要产生连续可用的空间。所以使用复制清楚算法和并行收集器进行垃圾回收，对年轻代的垃圾回收称作初级回收(minor gc)。 年轻代的工作机制jvm，年轻代中每次只会使用eden space和其中一块survivor区域来为程序服务，所以无论如何总有一块survivor区域总是空闲的。对象在 eden 中初始化，在经过一次minor gc后，如果对象还存活着，即被引用着，并且能够被另外一块survivor区域所容纳，则使用复制算法将这些仍然还存活的对象复制到另外一块survivor区域中。 年轻代如何变成老年代初始化过程与上面一致，在eden中。在minor gc之后，如果对象还存活着，这些对象的年龄+1，当超过某个值(默认为15)这些对象进入老年代。 老年代的gc堆内存的老年代不同于现实生活，老年代中的对象个个都是从survivor中熬过来的，所以老年代中的类不是那么容易死亡的。因此，full gc(又称major gc)发生的次数没有minor gc那么频繁，并且一次full gc要比minor gc时间要更长。标记-清除算法收集垃圾的时候会产生许多的内存碎片 ( 即不连续的内存空间 )，此后需要为较大的对象分配内存空间时，若无法找到足够的连续的内存空间，就会提前触发一次 GC 的收集动作。 持久代此外还有一个持久代，用于存放静态文件，如java类定义(相当于模版，不是实例对象)、方法、本地方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如hibernate等，在这种时候需要设置较大的持久代空间来存放这些运行过程中新增的类 。持久代大小通过-XX:MaxPermSize=进行设置。 补充有的虚拟机并没有持久代，java8开始持久层也已经被彻底删除了，取代它的是另一个内存区域也被称为元空间。想要参考更多，读者可以参考这篇博客 jvm配置项jvm配置项可以根据程序的实际要求来配置，如各个代的比例等。这边原本应该用表格的形式展示，但是hexo在渲染markdown的表格语法时不支持，待到作者修复这个bug后进行更改。1234567891011121314151617181920212223242526* -Xms 初始堆大小。如：-Xms256m* -Xmx 最大堆大小。如：-Xmx512m * -Xmn 新生代大小。通常为 Xmx 的 1/3 或 1/4。新生代 = Eden + 2 个 Survivor 空间。实际可用空间为 = Eden + 1 个 Survivor，即 90% * -Xss JDK1.5+ 每个线程堆栈大小为 1M，一般来说如果栈不是很深的 话， 1M 是绝对够用了的。 * -XX:NewRatio 新生代与老年代的比例，如 –XX:NewRatio=2， 则新生代占整个堆空间的1/3，老年代占2/3 * -XX:SurvivorRatio 新生代中 Eden 与 Survivor 的比值。默认值为 8。 即 Eden 占新生代空间的 8/10，另外两个 Survivor 各占 1/10 * -XX:PermSize 永久代(方法区)的初始大小 * -XX:MaxPermSize 永久代(方法区)的最大值 * -XX:+PrintGCDetails 打印 GC 信息 * -XX:+HeapDumpOnOutOfMemoryError 让虚拟机在发生内存溢出时 Dump 出当前的内 存堆转储快照，以便分析用]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux编辑器——vim]]></title>
    <url>%2F2018%2F01%2F31%2Flinux%E7%BC%96%E8%BE%91%E5%99%A8%E2%80%94%E2%80%94vim%2F</url>
    <content type="text"><![CDATA[VIM编辑器背景vi在mac下严格意义上讲不是一个简单的命令，而是unix系列下的自带的编译器，笔者没记错发明这编译器的家伙是个大胡子，一直致力于软件开源的维护和倡导$vi filename 这是开始编译一个文件，有时候需要switch user 即sudo 简介这边笔者不做过多的介绍请看维基百科这里的vim编辑器是vi的一种升级版 编辑文件之前说过基本用法:$vim filename，这边还有其他的用法$vim +line filename 编辑文件，并定位到第line行$vim + filename 编辑文件，并定位到最后一行$vim +/pattern filename 编辑文件，并定位到第一个匹配的位置 关闭文件这边需要提及vim的模式(之后补充，这边提及)，打开文件后需要进入输入模式或者其他模式，才能使用一些命令，退出模式后才能关闭文件一般按esc按钮来推出vim的模式，然后输入:来选择退出的方式:q 退出:wq 保存并退出:q! 不保存并退出:w 保存但没有退出，可以继续编辑:w! 强行保存退出后就回到命令窗口界面了，当然也可以直接编辑模式退出如：编辑模式按ZZ即可 模式i 在当前光标所在的字符前面转为输入模式a 在当前光标所在的字符后面转为输入模式o 在当前光标所在行的下方新建一行，并转为输入模式I 在当前光标所在行的行首转为输入模式A 在当前光标所在的行尾转为输入模式O 在当前光标所在的上方新建一行，并转为输入模式以上的命令必须是非编辑状态或者非其他的状态，然后按下相应的按键进入编辑模式 相关命令移动很简单，就是键盘上的上下左右键w 移至下一个单词词首，中文一般以标点为准跳动e 移至当前或下一个单词词尾b 移至当前或者前一个单词词首nw 移动n个单词 跳转0 跳转到光标所在的行首^ 跳转到行首的第一个非空白字符$ 绝对的行尾nG 跳转到第#行gg 跳转到第一行行首G 跳转到最后一行行首 翻页ctrl+f 向下翻一屏ctrl+b 向上翻一屏ctrl+d 向下翻半屏ctrl+u 向上翻半屏 删除x 删除光标所在处的单个字符nx 删除光标所在处及后面共n个单词dd 删除光标所在的行ndd 删除光标所在行及后面共n行 粘贴p: 如果删除或复制为整行内容，则粘贴至光标所在行的下方，如果复制或删除的内容为非整行，则粘贴至光标所在字符的后面P: 如果删除或复制为整行内容，则粘贴至光标所在行的上方，如果复制或删除的内容为非整行，则粘贴至光标所在字符的前面以上命令只作用于vim编辑器内部，外面复制的内容不行 复制用法和d命令的用法相同，将d替换成y 替换r 替换单个字符，但是好像不支持中文nr 光标后n个字符全部替换R 进入替换模式，可直接替换光标所在的字符 撤销编辑操作u 在非模式状态下，撤销前一次的编辑操作nu 直接撤销最近的n次编辑操作 选取v 进入选取模式，按字符选取，最多到光标所在的行V 进入选取模式，但是按矩形进行选取 查找:/pattern 根据匹配的字符查找:?pattern 同上n 下一个N 上一个 查找并替换headline,footline s#PATTERN#string#gn,$s#wyc#王鋆昌#g 替换第n行开始到最后一行中每一行所有wyc为王鋆昌参考 编辑多个文件vim file1 file2:next 切换到下一个文件:prev 切换到上一个文件:last 切换到最后一个文件:first 切换到第一个文件:q 退出当前文件:qa 全部退出 未完待续vim的使用远非及此，之后还有有涉及会及时补充]]></content>
      <categories>
        <category>vim</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令——crontab]]></title>
    <url>%2F2018%2F01%2F31%2Flinux%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94crontab%2F</url>
    <content type="text"><![CDATA[crontab作用自定义定时任务笔者一般用这个命令来提醒事项，适用于基本每天执行的事项 option参数-e 编辑cron脚本文件，可以指定编辑器，默认是vi编辑器-l 列出当前用户下的cron任务-u 指定cron在哪个用户下执行，默认是当前用户 使用实例$brontab -e 执行后跳到编辑器你可以在编辑器中书写cron命令，一般的格式是 command content上面格式中的 从左至右分别表示分，时，日，月份和年份，content就是自定义的命令了如果命令较多，你甚至可以写在一个shell脚本中，执行脚本即可比如：15 9 * command sh xxx.sh 每天早上9:15执行xxx.sh脚本 扩展一般用$cron -e编辑的脚本，笔者也不知道保存到哪里去了，一般笔者都是写完之后直接执行的比如：crontab test.cron即可，test.cron即为编写好的脚本，内容形式与上面的一致 实用价值了解敏捷开发的读者应该知道，部门中可能存在一些管理方式，需要每天执行任务，比如早晨的定时晨会，比如记录一天内的工作时间，等等读者可以使用outlook来提醒，或者手机的定时闹钟，笔者选择用crontab命令来显示高逼格列出了笔者mac中每天执行的任务，将多个cron任务编辑在同一个.cron文件内，执行就能实现多个任务的并存发现cron任务content可以直接命令，也可以shell脚本执行]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令——awk]]></title>
    <url>%2F2018%2F01%2F31%2Flinux%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94awk%2F</url>
    <content type="text"><![CDATA[awk用法awk ‘{pattern + action}’ {filenames} 使用方式awk -F 分隔符 ‘{command操作}’awk -f awk-script-file input-file(s)上面的命令比较有意思，awk-script-file是脚本文件，一般比较复杂的awk命令可以写成一个脚本，然后操作到input-file(s)文件上 内置变量ARGC 命令行参数个数ARGV 命令行参数排列ENVIRON 支持队列中系统环境变量的使用FILENAME awk浏览的文件名FNR 浏览文件的记录数FS 设置输入域分隔符，等价于命令行 -F选项NF 浏览记录的域的个数NR 已读的记录数OFS 输出域分隔符ORS 输出记录分隔符RS 控制记录分隔符 使用实例在xqy的日志使用手册中，有统计所有抛出非业务异常的服务的命令，相信很多开发不知道运行的原理，在此我做一下说明：cat common-service-digest.log | grep ‘N,’ | awk -F ( ‘{print $2}’ | awk -F ) ‘{print $1}’ | awk -F , ‘{!a[$1”.”$2”.”$3]++;}END {for (j in a) print j,a[j]}’这个命令比较长，你可以分开进行执行，比如:1.cat common-service-digest.log | grep ‘N,’ 查看调用失败的日志2.awk -F ( ‘{print $2}’ 以 （作为分隔符分割日志行，并打印分隔片区2比如:日志记录:[(tax,ICustomerSettingManageService,getAreaCodeBatch,Y,12ms)] (traceId=84ec429fbff64c5bbc43347c8cc72257)被分隔为:[(tax,ICustomerSettingManageService,getAreaCodeBatch,Y,12ms)](traceId=84ec429fbff64c5bbc43347c8cc72257)最后输出:tax,ICustomerSettingManageService,getAreaCodeBatch,Y,24ms)]3.awk -F ) ‘{print $1}’ 以 ）作为分隔符分割日志行，并打印分隔片区1最后输出:tax,ICustomerSettingManageService,getAreaCodeBatch,Y,12ms4.awk -F , ‘{!a[$1”.”$2”.”$3]++;} 以 ,作为分隔符分割日志行，并以$1”.”$2”.”$3为键统计5.最后end进行循环输出参考 说明当中涉及到xqy的日志业务和图片，所以不部署到github]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令——sed]]></title>
    <url>%2F2018%2F01%2F31%2Flinux%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94sed%2F</url>
    <content type="text"><![CDATA[sed系统mac系统用的是原生的bsd系列，而一般的linux系统用的是gnu系统，所以两者在某些命令是有区别的，sed命令位列其中 区别mac上sed的添加和插入文本比较奇葩，需要如此:$sed “$line a\ (在\后要加一个空格，然后另起一行，再写需要添加的那一行新的)>$value（需要添加新一行的内容）>“ $filename(文件名)同上，sed i的使用方式亦然 gnu-sed如果无法适应原生bsd系列，可以下载gnu-sed，这个时候brew神器又用到了，直接下载然后配置环境变量即可:$brew install gnu-sed之后的叙述都是基于gnu-sed说明 option参数-n ：使用安静(silent)模式，在一般 sed 的用法中，所有来自 STDIN 的数据一般都会被列出到终端上，但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来-e ：直接在命令列模式上进行 sed 的动作编辑-f ：直接将 sed 的动作写在一个文件内， -f filename 则可以运行 filename 内的 sed 动作-r ：sed 的动作支持的是延伸型正规表示法的语法。(默认是基础正规表示法语法)-i ：直接修改读取的文件内容，而不是输出到终端 方法a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)p ：列印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行s ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法 实例$sed ‘2a 字符串’ filename —— 第二行下一行添加字符串$sed ‘2i 字符串’ filename —— 第二行上一行添加字符串以上的命令只能作用在缓存的文件中，其实真实的文件内容并没有被改变$sed ‘2a 字符串’ -i filename —— 这样可以直接修改文件了$sed ‘1,2d’ -i filename —— 删除文件1-2行的内容sed与nl联合使用可以进行关键字查找:$nl filename | sed -n ‘/关键字/p’sed支持正则表达的搜索和替换:$sed ‘s/正则表达式 or 普通字符串/新字符串/g’ -i filename[其他用法:]https://www.cnblogs.com/ggjucheng/archive/2013/01/13/2856901.html]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令——补充]]></title>
    <url>%2F2018%2F01%2F31%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94%E8%A1%A5%E5%85%85%2F</url>
    <content type="text"><![CDATA[常用命令(补充)grep用法grep [-acinv][–color=auto] ‘查找字符串’ filename option参数-c 统计查找字符在文件中出现的次数-i 忽略大小写-v 反向查找，即显示不在查找内的行，但是根据记录日志的习惯，很少会使用–color=auto 对查找的字符进行颜色显示 联合使用ls -l |grep -i filename 列出指定文件/文件夹的属性ps -ef|grep tomcat/java 列出指定的进程信息 find用法find [PATH] [option] [action] option参数-mtime n 其中n为数字，表示n天以前，查找n天之前修改过的文件-user name 查找指定所有者的文件-size [+-size] 查重比size大/小的文件 使用$find . -mtime 0 来查找当天修改过的文件，一般可以筛选当天有记录的日志 taroption参数-c ：新建打包文件-t ：查看打包文件的内容含有哪些文件名-x ：解打包或解压缩的功能，可以搭配-C（大写）指定解压的目录，注意-c,-t,-x不能同时出现在同一条命令中-j ：通过bzip2的支持进行压缩/解压缩-z ：通过gzip的支持进行压缩/解压缩-v ：在压缩/解压缩过程中，将正在处理的文件名显示出来-f filename ：filename为要处理的文件-C dir ：指定压缩/解压缩的目录dir 使用实例$tar -tzvf xxx.zip 查看指定压缩文件里面的文件内容$tar -zxvf xxx.zip 解压缩指定文件$tar -cvf xxx.tar dir 压缩指定文件夹$tar -czvf xxx.tar.gz dir 以gzip的格式进行压缩如何使用解压缩命令的option操作，最终还是要看压缩文件的格式的，如-z一般用来支持.gz结尾的压缩包 kill用法kill [选项] [参数] option参数-a：当处理当前进程时，不限制命令名和进程号的对应关系-l &lt;信息编号&gt;：若不加&lt;信息编号&gt;选项，则-l参数会列出全部的信息名称 -p：指定kill 命令只打印相关进程的进程号，而不发送任何信号-s &lt;信息名称或编号&gt;：指定要送出的信息-u：指定用户 常用的信号编号HUP 1 终端断线 INT 2 中断（同 Ctrl + C） QUIT 3 退出（同 Ctrl + \） TERM 15 终止 KILL 9 强制终止 CONT 18 继续（与STOP相反， fg/bg命令） STOP 19 暂停（同 Ctrl + Z）在mac中18是STOP，19是CONT]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令文件上传与下载]]></title>
    <url>%2F2018%2F01%2F31%2Flinux%E5%91%BD%E4%BB%A4%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E4%B8%8E%E4%B8%8B%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[文件上传下载搭建实践环境这边使用笔者自己的电脑和windows电脑进行实践。 利用ssh打通windows和mac打开mac的系统偏好设置中的共享，一般情况下没有界面展示的，需要使用搜索查找共享， 然后勾选远程登陆，选择全部用户可访问 在windows上使用ssh连接mac，填写相应的用户名和密码即可 安装要支持sz，rz两个命令，系统需要安装lrzsz，一般的linux系统是自带的，这里使用mac进行实验，需要安装。参考 使用sz filename，即从系统上下载文件到本地，filename是系统中你选择下载的文件rz -b，将本地的文件以二进制的传递方式上传到系统服务器，一般推荐使用二进制传输方式其他参考]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo防止敏感数据部署]]></title>
    <url>%2F2018%2F01%2F30%2Fhexo%E9%98%B2%E6%AD%A2%E6%95%8F%E6%84%9F%E6%95%B0%E6%8D%AE%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[hexo防止敏感数据部署原理熟悉hexo的读者都知道，hexo的部署一般执行如下:$hexo clean$hexo g$hexo d如果不按照顺序执行这三步，读者会发现即使增添了page，部署时git也不会显示增添文件的记录，github上也没有提交的代码记录其实这涉及到hexo的部署原理，hexo clean 会清除之前构建的代码，具体就是public文件夹；hexo g 会重新生成，这样读者添加的page也就构建在其中，此时新的public文件夹会生成；hexo d 就是将hexo g 改变的文件提交到github上所以不执行clean和generate是不会真正意义上修改remote上的代码 敏感数据敏感数据这里指的就是涉及公司业务的文章，因为hexo d 会将文章直接部署到github上，这样就全部公开化了这边最好是本地可以访问，github无法访问你的敏感数据，这篇文章的目的就在此 利用.gitignore熟悉git的读者，应该都知道可以使用.gitignore文件忽略一些提交网上有些说修改.npmignore文件配置(hexo的根目录下)可以起到效果，经过测试发现没有起到作用，所以转而到git寻求解决方案一般执行如下(当前目录为hexo根目录):$cd .deploy_git git push的代码都在此文件夹下$vim .gitignore增加如下配置:page name/ page的名称，hexo n page创建命令中你定义的文章名，如:linux命令性能监控及优化/.gitignore 自己本身但是笔者发现代码虽然没有提交到github上，但是hexo博客上有这个新建的标题，点击访问也是github默认的404页面(因为hexo g过程中一定会将文章标题写入hexo中，.gitignore只能做到忽略提交) 自定义404上面的问题引入了这个小结，github默认的404页面会让读者误以为是网站的问题，其实是不想公开化，所以你可以自定义404页面具体的步骤如下(hexo的根目录下):$hexo n page 404 source目录下生成404目录index.md中增加配置: layout: false //是否使用布局文件 comments: false //是否有评论 permalink: /404 //设置链接然后按照hexo部署步骤上传到github上，然后再访问敏感文章链接会跳转到自定义的404页面其实根本问题是github上没有代码，但是hexo在generate时将目录还是写入了导致，但是笔者找不到解决忽略写入的方法]]></content>
      <categories>
        <category>hexo部署</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令性能监控及优化]]></title>
    <url>%2F2018%2F01%2F30%2Flinux%E5%91%BD%E4%BB%A4%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E5%8F%8A%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[性能监控和优化toptop freefree vmstat用来统计虚拟内存的使用情况，这边涉及到linux物理内存和虚拟内存的知识点，其实之前我们也提到过，虚拟内存即交换区 option参数-a：显示活跃和非活跃内存-f：显示从系统启动至今的fork数量-m：显示slabinfo-n：只在开始时显示一次各字段名称-s：显示内存相关统计信息及多种系统活动数量delay：刷新时间间隔。如果不指定，只显示一条结果count：刷新次数。如果不指定刷新次数，但指定了刷新时间间隔，这时刷新次数为无穷-d：显示磁盘相关统计信息-p：显示指定磁盘分区统计信息-S：使用指定单位显示。参数有 k 、K 、m 、M ，分别代表1000、1024、1000000、1048576字节（byte）默认单位为K（1024 bytes）-V：显示vmstat版本信息 使用实例$vmstat 5 6 在docker上进行试验，在6秒内采集5次数据 说明Procs（进程）：r: 运行队列中进程数量b: 等待IO的进程数量Memory（内存）：swpd: 使用虚拟内存大小free: 可用内存大小buff: 用作缓冲的内存大小cache: 用作缓存的内存大小Swap：si: 每秒从交换区写到内存的大小so: 每秒写入交换区的内存大小IO：（现在的Linux版本块的大小为1024bytes）bi: 每秒读取的块数bo: 每秒写入的块数系统：in: 每秒中断数，包括时钟中断cs: 每秒上下文切换数CPU（以百分比表示）：us: 用户进程执行时间(user time)sy: 系统进程执行时间(system time)id: 空闲时间(包括IO等待时间),中央处理器的空闲时间 以百分比表示wa: 等待IO时间 需要关注的指标如果 r经常大于 4 ，且id经常少于40，表示cpu的负荷很重如果bi，bo 长期不等于0，表示内存不足如果disk 经常不等于0， 且在 b中的队列 大于3， 表示 io性能不好]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令]]></title>
    <url>%2F2018%2F01%2F30%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[常用命令pwd没什么可说的，当前不知道处于那个目录下时，pwd即可 mvmv命令是将一个文件移动到指定的目录，但是在linux可以用来重命名文件 moremore是具有翻页功能的cat命令 lessless是一个可以前后分页浏览文件的命令，相比more更加具有弹性 whichwhich用于查找命令包所在的路径，没有配置在环境变量中的命令是搜索不到的，因为which只会搜索path下的命令，echo $PATH可以查看 lsls命令还是很常用的，不做过多描述ls g* 模糊列出文件列表，即将g开头的文件列出 unzip和gzipunzip主要用于解压.zip压缩文件 unzip xxx.zipgzip file1 file2 … 文件打包压缩为.gz的形式对于文件夹的打包或者其他的压缩格式请使用tar命令读者可以参考这篇博客 whatis和man$whatis command 用于显示命令使用的描述$man command 以使用手册的形式显示]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务问题定位——jmap]]></title>
    <url>%2F2018%2F01%2F29%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D%E2%80%94%E2%80%94jmap%2F</url>
    <content type="text"><![CDATA[jmap使用场景 内存溢出，线上程序运行时内存越来越大，用jmap dump出堆内存的信息，然后进行相关分析 内存的真实使用大于预期，这是因为设计不合理导致冗余的对象存在内存中，用jmap来查看内存中的对象，分析是否有存在的必要 jvm优化，利用jmap来查看整个堆的使用情况，根据老年代和新生代的使用比例来划分jvm的各个区域 使用实例在使用jmap连接进程时，启动的jvm版本和jdk版本要一一对应，毕竟jmap是jdk下的命令包$ps -ef | grep java 查找系统的java进程$jmap -heap pid 打印指定进程堆的摘要信息，包括gc算法 参数说明 Heap Configuration: 堆配置信息 MinHeapFreeRatio 在堆的使用率小于MinHeapFreeRatio(%)的时候进行收缩，当Xmx=Xms的时候此配置无效MaxHeapFreeRatio 在堆使用率大于MaxHeapFreeRatio(%)的时候进行扩展，当Xmx=Xms的时候此配置无效MaxHeapSize 堆的最大空间NewSize 新生代的大小MaxNewSize 最大的新生代的大小OldSize 老年代的大小NewRatio 老年代和新生代的比例SurvivorRatio 新生代中Eden和和Survivor区的比例PermSize 永久代的大小MaxPermSize 久代的最大内存G1HeapRegionSize 使用G1垃圾收集的区间 Heap Usage: 堆的使用信息 New Generation (Eden + 1 Survivor Space): 新生代的大小（Eden区加一个Survivor区的空间信息 capacity 总内存used 已使用内存free 剩余内存13.823827124993642% used 使用内存占比 Eden Space: Eden区的大小 capacityusedfree15.178337946947952% used From Space: 第一个Surivivor区的空间信息 capacityusedfree2.984432830624237% used To Space: 第二个Survivor区的空间信息 capacityusedfree0.0% used concurrent mark-sweep generation: CMS垃圾收集占用的空间信息 capacityusedfree75.36470666527748% used Perm Generation: 永久代的空间信息 capacityusedfree47.5915253162384% used 其他功能参考链接]]></content>
      <categories>
        <category>微服务问题定位</category>
      </categories>
      <tags>
        <tag>jvm</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务问题定位——jstack]]></title>
    <url>%2F2018%2F01%2F29%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D%E2%80%94%E2%80%94jstack%2F</url>
    <content type="text"><![CDATA[jstack功能查看运用程序jvm的堆栈情况，可以找出线程的运行情况，从而排查一些隐患或者服务卡顿的问题 具体说明jstack用于打印出给定的java进程id或者core file或者远程调试服务的java堆栈信息，如果是在64位机器上，需要指定选项-J-d64如果java程序崩溃会生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松知道java程序是如何崩溃和在程序何处发生问题。 option参数-F 当jstack -l pid 没有响应的时候强制打印栈信息-l 长队列，打印关于锁的附加信息-m 打印java和native c/c++框架的所有栈信息 命令格式jstack [option] pidjstack [option] executable corejstack [option] [server-id@]remote-hostname-or-IP 命令格式说明executable core 产生core dump的java可执行文件remote-hostname-or-IP 远程debug服务的主机名或ipserver-id 唯一id,假如一台主机上多个远程debug服务 使用实例一般需要与top命令联用，使用top命令找出异常的进程(一般是cpu使用异常的进程)通过top -Hp pid来定位该进程下各线程的cpu使用情况再通过jstack pid命令打印该线程对应的堆栈情况 扩展 在top命令中，已经获取到了占用cpu资源较高的线程pid，将该pid转成16进制的值(在线转换)，在thread dump中每个线程都有一个nid，找到对应的nid即可 什么是java core和heap dump文件，参考]]></content>
      <categories>
        <category>微服务问题定位</category>
      </categories>
      <tags>
        <tag>jvm</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitbook目录折叠]]></title>
    <url>%2F2018%2F01%2F28%2Fgitbook%E7%9B%AE%E5%BD%95%E6%8A%98%E5%8F%A0%2F</url>
    <content type="text"><![CDATA[背景在编写gitbook的时候发现目录的数量比较庞大的时候显得杂乱无章，因为章节比较多的时候gitbook对于目录默认是全展开，很难有目的地寻找，对于管理人和阅读的读者来说很不友好。(主要是后者，一切都要从用户的角度考虑)如果可以将gitbook的目录折叠，那么查找时会更加清晰，笔者在网上找到了插件，只要让gitbook引入即可。 插件 插件名称：toggle-chapters 效果：默认只在目录导航中显示章的标题，而不会显示小节的标题，点击每一章或者每一节会显示当前章或节的子目录，如果有的话，但是同时会收起其它之前展开的章节。 关于更多的gitbook插件，读者可以参考插件网站。 配置在根目录(即与SUMMARY.md同级的目录)下的配置文件 book.json(如果没有则新建)中添加插件配置，如图：读者可以参考进行配置，同理你可以在里面添加需要使用的插件。配置完成后，可按照一下步骤进行： $ cd gitbook根目录 $ npm install gitbook-plugin-toggle-chapters (此时gitbook的根目录下的node_modules文件夹中已经有了该插件了) $ gitbook build $ gitbook serve访问 http://localhost:4000 看你的插件是否已经生效。]]></content>
      <categories>
        <category>gitbook</category>
      </categories>
      <tags>
        <tag>gitbook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在本地搭建gitbook]]></title>
    <url>%2F2018%2F01%2F28%2F%E5%A6%82%E4%BD%95%E5%9C%A8%E6%9C%AC%E5%9C%B0%E6%90%AD%E5%BB%BAgitbook%2F</url>
    <content type="text"><![CDATA[介绍gitbook更加适用于书写使用手册，不太适合书写一些博客和总结，所以笔者最近在做一些迁移的工作，将之前gitbook上的内容迁移到当前的博客上。但是gitbook对于我们的工作还是有适用的地方的。所以在此介绍一下如何本地安装gitbook。 先决条件安装nodejs是先决条件，因为我们需要npm去下载gitbook。在mac电脑上，有brew工具的可以直接下载：$brew search nodejs 搜索包仓库中是否有nodejs$brew install nodejs 有就可以直接下载你也可以选择去官网上下载最新的nodejs压缩包，解压配置环境变量后即可使用。关于brew，读者可以参考简书进行安装。 搭建这里默认大家的电脑上已经安装好了nodejs。按照步骤执行以下命令：$npm install gitbook-cli -g 下载gitbook客户端包$gitbook –version 查看gitbook的版本，测试是否可以使用命令$cd /users/Desktop 切换到一个目录，这里笔者切换到了桌面$mkdir gitbook 创建gitbook目录，这个目录就是用来初始化的其实上面的两步(切换目录和创建目录)读者可以手动完成。$gitbook init 初始化gitbook目录$gitbook build 构建，一般有内容更新时要构建$gitbook serve 开启服务，默认在4000端口监听访问http://localhost:4000 开启你的gitbook之路吧！ 问题你的系统 4000 端口可能已经被占用了，gitbook可以指定端口启动：gitbook serve –port 2000 另一个问题是笔者在windows上搭建时遇到的：笔者之前在windows上搭建过，出现了一些问题，build的时候总是报错。在github官方issues中，官方表示这个是已知的gitbook的bug，高版本的gitbook会出现这个bug。如果你遇到相同的问题，请参考gitbug上的解决方法。解决方法是点赞和喝彩最多的。]]></content>
      <categories>
        <category>gitbook</category>
      </categories>
      <tags>
        <tag>gitbook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务问题定位——du]]></title>
    <url>%2F2018%2F01%2F28%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D%E2%80%94%E2%80%94du%2F</url>
    <content type="text"><![CDATA[功能du命令也是查看使用空间的，但是与df命令不同的是，du命令是对文件和目录磁盘使用的空间进行查看，而不是针对整个文件系统。 option 参数-a或-all 显示目录中个别文件的大小-b或-bytes 显示目录或文件大小时，以byte为单位-c或–total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和-k或–kilobytes 以KB(1024bytes)为单位输出-m或–megabytes 以MB为单位输出-s或–summarize 仅显示总计，只列出最后加总的值-h或–human-readable 以K，M，G为单位显示，提高信息的可读性-x或–one-file-xystem 以一开始处理时的文件系统为准，若遇上其它不同的文件系统目录则略过-L&lt;符号链接&gt;或–dereference&lt;符号链接&gt; 显示选项中所指定符号链接的源文件大小-S或–separate-dirs 显示个别目录的大小时，并不含其子目录的大小-X&lt;文件&gt;或–exclude-from=&lt;文件&gt; 在&lt;文件&gt;指定目录或文件–exclude=&lt;目录或文件&gt; 略过指定的目录或文件-D或–dereference-args 显示指定符号链接的源文件大小-H或–si 与-h参数相同，但是K，M，G是以1000为换算单位-l或–count-links 重复计算硬件链接的文件读者可以使用man du或者du -help查看完整的参数列表。 使用实例$du 显示当前目录下以及子目录中所有文件的大小，一般都是使用-a参数指定文件的，因为罗列所有的信息读者很难找到相关的。$du dir/filename 显示指定目录或者文件，dir/filename为系统中的路径。$du #1 #2 同时显示多个，空格隔开]]></content>
      <categories>
        <category>微服务问题定位</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
        <tag>微服务</tag>
        <tag>磁盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务问题定位——df]]></title>
    <url>%2F2018%2F01%2F28%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D%E2%80%94%E2%80%94df%2F</url>
    <content type="text"><![CDATA[功能用来检查服务器文件系统的磁盘空间占用情况，可以获取硬盘被占了多少空间，目前还剩下多少空间等信息。 option 参数-a 显示全部文件系统列表-h 以方便查阅的方式显示，即带有单位的形式-H 相当于-h，但是换算的方式变更，这里1k=1000-i 显示node信息-k 区块为1024字节，即1Kb为一个区块-l 只显示本地文件系统-m 区块为1048576字节，即1Mb为一个区块–no-sync 忽略sync命令-P 输出格式为POSIX–sync 在取得磁盘信息前先执行sync命令-T 文件系统类型 选择参数–block-size=&lt;区块大小&gt; 指定区块的大小-t &lt;文件系统类型&gt; 只显示选定文件系统的磁盘信息-x &lt;文件系统类型&gt; 不显示选定文件系统的磁盘信息 使用实例第一列 显示各文件系统第二列 显示文件系统的共有多少块，这里以512B为一块，换算后即可知道文件系统大小第三列 显示已经使用的磁盘大小第四列 显示可用的磁盘大小关于其他的使用方式，读者可以参考cnblogs了解。这边需要说明：已使用的+可使用的 != 总块数，因为缺省的每个分区都预留了少量空间供管理员使用，所以即使普通用户的空间已满，管理员依然可以登录系统解决相关问题。]]></content>
      <categories>
        <category>微服务问题定位</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
        <tag>微服务</tag>
        <tag>磁盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务问题定位——ifstat]]></title>
    <url>%2F2018%2F01%2F28%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D%E2%80%94%E2%80%94ifstat%2F</url>
    <content type="text"><![CDATA[ifstat功能监控系统的流量 安装ifstat不是系统自带的命令，所以需要进行安装。mac系统的读者可以在终端使用命令：$brew install ifstat进行安装。当然系统首先要安装brew(mac系统中的包下载工具)，读者可以参考简书进行安装。 option 参数-l 监测环路网络接口(lo)，缺省情况下不会显示，所谓的环路网络可认为是localhost(127.0.0.1)。读者可以参考知乎了解相关的解释。-a 监测系统所有的网络，比加上-l参数还多一个plip0的接口信息(所谓的并口)-z 隐藏流量是无的接口，排查问题时排除无用端口-i 指定要监测的接口,后面跟网络接口名-s 等于加-d snmp:[comm@][#]host[/nn]] 参数，通过SNMP查询一个远程主机-t 在每一行的开头加一个时间 戳,，告诉我们具体的时间-T 报告所有监测接口的全部带宽，和-i联用来指定端口-S 在同一行更新流量状态，不喜欢屏幕滚动的可以使用读者可以使用man ifstat或者ifstat -help查看完整的参数列表。更多详细的命令，读者可参考其他命令了解。 扩展如何只查看网卡的流量情况，ifstat足矣。详细的流量情况需使用iftop命令，系统依旧不会自带需要下载，mac下的下载方式与ifstat一致。读者可以参考实例了解用法。]]></content>
      <categories>
        <category>微服务问题定位</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
        <tag>微服务</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务问题定位——free]]></title>
    <url>%2F2018%2F01%2F28%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D%E2%80%94%E2%80%94free%2F</url>
    <content type="text"><![CDATA[free功能 显示系统使用和空闲的内存情况，包括物理内存和交互区内存(swap)和内核缓冲区内存 option参数 -b 以Byte为单位显示内存使用情况 -k 以KB为单位显示内存使用情况 -m 以MB为单位显示内存使用情况 -g 以GB为单位显示内存使用情况 -o 不显示缓冲区调节列 -s&lt;间隔秒数&gt; 持续观察内存使用状况 -t 显示内存总和列 -V 显示版本信息 读者可以使用man free或者free -help查看完整参数列表。 样例 这边关注第二行的swap的used指标，如果使用的值较大，则表示系统的内存处于不够使用的情况 为何swap used是一个指标 swap又称为交换分区，当系统的内存小于额定值的时候，内核(OS)会将系统的一部分物理内存释放出来用于当前线程的使用，一般是很久没有操作过的程序会被释放，释放的物理内存被放入到交换区，然后等当前线程执行完毕，内存富余时重新放入物理内存。12345678910例如：物理内存，交换区，当前线程，进程x(目前存在于物理内存中)1.当前线程 向 物理内存 申请执行所需的内存空间2.物理内存 发现没有空间了 查找到 进程x 很久没有操作过3.物理内存 将 进程x 释放到 交换区4.物理内存 说 嘿 当前线程 你可以进来执行了5.当前线程 执行完毕 嘿 我执行完了 让那哥们进来吧6.物理内存 将 进程x 重新载入以上就是简略的过程。]]></content>
      <categories>
        <category>微服务问题定位</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
        <tag>微服务</tag>
        <tag>内存相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务问题定位——strace]]></title>
    <url>%2F2018%2F01%2F28%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D%E2%80%94%E2%80%94strace%2F</url>
    <content type="text"><![CDATA[功能 在linux世界，进程不能直接访问硬件设备，当进程需要访问硬件设备(比如读取磁盘文件，接收网络数据等等)时，必须由用户态模式切换至内核态模式，通过系统调用访问硬件设备。 strace可以跟踪到一个进程产生的系统调用，包括参数，返回值，执行消耗的时间。 option参数 -c 统计每一系统调用的所执行的时间,次数和出错的次数等 -d 输出strace关于标准错误的调试信息 -f 跟踪由fork调用所产生的子进程 -tt 在输出中的每一行前加上时间信息,微秒级， 时间格式：17:22:58.345879 -p pid 跟踪指定的进程pid 以上命令一般都是与-p联用 读者可以使用man strace或者strace -help查看完整的参数列表。 strace使用 strace的使用一般在top命令之后，top命令是用来查看占用cpu异常的进程的。读者可以参考微服务定位——top。 找到异常的进程后，使用命令：top -Hp pid[进程id]进入进程，找到执行异常的那个线程。使用命令：strace -p nid[线程id] -T来查看底层的调用情况。 在此贴出网上的一些参考链接 strace实例 strace实例2 样例贴图 笔者在此贴出strace命令显示的底层执行的信息： 1234567891011$strace cat /dev/nullexecve(&quot;/bin/cat&quot;, [&quot;cat&quot;, &quot;/dev/null&quot;], [/* 22 vars */]) = 0brk(0) = 0xab1000access(&quot;/etc/ld.so.nohwcap&quot;, F_OK) = -1 ENOENT (No such file or directory)mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f29379a7000access(&quot;/etc/ld.so.preload&quot;, R_OK) = -1 ENOENT (No such file or directory)...每一行都是一条系统调用，等号左边是系统调用的函数名及其参数，右边是该调用的返回值。strace 显示这些调用的参数并返回符号形式的值。strace 从内核接收信息，而且不需要以任何特殊的方式来构建内核。]]></content>
      <categories>
        <category>微服务问题定位</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
        <tag>微服务</tag>
        <tag>进程,线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务问题定位——top]]></title>
    <url>%2F2018%2F01%2F26%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%AE%9A%E4%BD%8D%E2%80%94%E2%80%94top%2F</url>
    <content type="text"><![CDATA[功能显示当前系统正在执行的进程的相关信息，包括进程ID、 内存占用率 、CPU占用率等 option 使用参数-b 进入批处理模式 相当于不停地执行top命令显示信息-c 显示完整的command 位于信息的最后一列-i &lt;时间&gt; 设置间隔时间 如-i 5设置5秒钟刷新一次top信息-u &lt;用户名&gt; 显示指定用户的信息 -p &lt;进程号&gt; 显示指定进程的信息 -n &lt;次数&gt; 循环显示的次数 读者可以使用man top或者top -help查看完整的参数列表。 例图 说明 第一行: 13:40:29 — 当前系统时间10 day，2:32 — 机器从开机到目前为止运行的时长20 users — 登陆的用户有20个load average — 分别对应1mins,5mins,15mins的负载情况(根据一定的算法得到的值，load average／cpu数量&gt;=5说明系统超负荷) 第二行 Tasks 系统进程数total — 当前系统进程269个running — 正在运行的1个sleeping — 睡眠的268个stopped — 停止的0个zombie — 僵尸进程0个 第三行 0.1% us — 用户空间占用CPU的百分比0.0% sy — 内核空间占用CPU的百分比0.0% ni — 改变过优先级的进程占用CPU的百分比99.9% id — 空闲CPU百分比0.0% wa — IO等待占用CPU的百分比0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比0.0% si — 软中断（Software Interrupts）占用CPU的百分比 第四行 Mem 物理内存total — 物理内存总量 8GBfree — 空闲的内存used — 正在使用的内存buff/cache — 缓存的内存(used表示现在系统内核控制的内存数，free表示还未进入内核控制的内存数，used还包括了停止使用但可能被重用的内存，所以used使用完的内存不会返还给free，所以free的内存数一定越来越少) 第五行 Swap 交换区total — 交换区总量used — 使用的交换区总量free — 空闲交换区总量cached — 缓冲的交换区总量(swap used经常变化的话说明内存已经不够使用了)(系统可使用的内存近似为第四行的free+buff/cache+第五行的cache)读者可以参考 参考链接了解 进程状态监控各项指标 PID — 进程idUSER — 进程所有者PR — 进程优先级NI — nice值，负值表示高优先级，正值表示低优先级VIRT — 进程使用的虚拟内存总量，单位kb，VIRT=SWAP+RESRES — 进程使用的、未被换出的物理内存大小，单位kb，RES=CODE+DATASHR — 共享内存大小，单位kbS — 进程状态，D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程%CPU — 上次更新到现在的CPU时间占用百分比%MEM — 进程使用的物理内存百分比TIME+ — 进程使用的CPU时间总计，单位1/100秒COMMAND — 进程名称（命令名/命令行） top交互执行top命令之后即进入top信息展示界面，可以使用命令行进行交互 top界面 按1显示多核cpu的使用情况 top界面默认按cpu的使用降序排序 使用shift+&gt;和shift+&lt;来改变排序的指标 top界面 按x高亮排序的指标 top其他使用技巧 h 显示帮助画面，给出一些简短的命令总结说明 k 终止一个进程 i 忽略闲置和僵死进程。这是一个开关式命令 q 退出程序 r 重新安排一个进程的优先级别 S 切换到累计模式 s 改变两次刷新之间的延迟时间（单位为s），如果有小数，就换算成ms。输入0值则系统将不断刷新，默认值是5s f或者F 从当前显示中添加或者删除项目 o或者O 改变显示项目的顺序 l 切换显示平均负载和启动时间信息 m 切换显示内存信息 t 切换显示进程和CPU状态信息 c 切换显示命令名称和完整命令行 M 根据驻留内存大小进行排序 P 根据CPU使用百分比大小进行排序 T 根据时间/累计时间进行排序 W 将当前设置写入~/.toprc文件中 说明在top命令界面中，几乎不使用交互命令，在排查问题时，一般紧盯cpu使用高的线程即可。]]></content>
      <categories>
        <category>微服务问题定位</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
        <tag>微服务</tag>
        <tag>进程,线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F01%2F24%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>宣告全世界</category>
      </categories>
      <tags>
        <tag>welcome</tag>
        <tag>hello</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梁希森林公园]]></title>
    <url>%2F2017%2F04%2F04%2F%E6%A2%81%E5%B8%8C%E6%A3%AE%E6%9E%97%E5%85%AC%E5%9B%AD%2F</url>
    <content type="text"><![CDATA[写博客以来，笔者一直用github作为图床，但当图片的大小超过一定值打开github链接就会直接下载，无法作为url链接，所以将图片暂放微博。 我与梁希先生 人生得意须尽欢 流水潺潺 狗年行大运]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
        <tag>自然环境</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跆拳道]]></title>
    <url>%2F2016%2F06%2F13%2F%E8%B7%86%E6%8B%B3%E9%81%93%2F</url>
    <content type="text"><![CDATA[跆拳道 人不疯狂枉少年]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
        <tag>校园</tag>
      </tags>
  </entry>
</search>
