<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[山水之间]]></title>
    <url>%2F2019%2F02%2F09%2F%E5%B1%B1%E6%B0%B4%E4%B9%8B%E9%97%B4%2F</url>
    <content type="text"><![CDATA[人生不过山水之间申屠古镇 这个古镇名字是我自定义的，之所以取成这样是因为那边的申屠祠堂的事迹给我的印象比较深，然后我已经忘记游览的古镇的名字了，但是申屠姓氏当之无愧。 山水之间 白云之巅景点，具体的名字我已经忘却了，但是与白云相关。松露遍开，漫天飞雪，当世之美景不过如此。]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
        <tag>自然环境</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[征期跨度暴露的浙江爬取数据问题]]></title>
    <url>%2F2018%2F10%2F20%2F%E5%BE%81%E6%9C%9F%E8%B7%A8%E5%BA%A6%E5%BC%95%E8%B5%B7%E7%9A%84%E6%B5%99%E6%B1%9F%E7%88%AC%E5%8F%96%E6%95%B0%E6%8D%AE%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[征期跨度上一期迭代，接触了网厅爬取数据的项目。出现了征期跨度引起的问题，31号发布的功能在本月份没有问题，但是跨越到7月份征期出现了严重的问题。(我们具体发布的时间是：2018-07-31)到了新的征期，税种的未申报都处理成了申报成功，无需缴款。 项目完成过程这个问题我认为是在项目迭代中慢慢积累起来的，最终演化成了一个严重的问题。1.爬取规则问题：浙江的爬取规则，是产品自己研究定的，但是出现了问题，所以最后紧急变更需求。(这里感谢浙江的现场的会计，最终的规则由她们提供)2.时间点问题：研究浙江网厅的时间是在6月末期了，已经过了6月征期，导致，一.提供的账号没有未缴款的数据，无法得知未缴款的数据机构；二.税种列表中都是已申报的状况，没有未申报的数据。3.自测问题：开发的可测试数据太少，原因由问题2引起；开发在进行自测的时候关注点过多集中在已申报缴款的状态，没有考虑到未申报的情况。 针对问题1：我认为如果可行，尽量在自己研究网厅的基础上再询问当地的会计，因为真实使用的客户永远比我们懂的多的多。针对问题2：开发主要责任，完全没有意识到因为征期税种报送完成可能获取不到一些状体的数据(比如未缴款状体)，所以网厅爬取要做好前期工作。针对问题3：在获取到爬取页面的数据结构前提下，开发其实可以模拟其他状态的数据，从而覆盖到更多的场景。 网厅爬取针对这一次的项目，我认为网厅爬取的最大难度在于攻破网厅的登录。浙江的网厅是界面上的国地税合并，登录方式还是分开的，随意本次迭代如何攻破登录确实花费了不少时间。 对于数据的爬取，理想的情况下是请求接口返回json格式的数据，糟糕的情况是返回html页面，后者需要对页面进行解析。此时爬取的规则显得比较重要了，相同的数据可能在不同的界面，返回的格式也有优劣，可以择优选择。 维护成本，爬取的方式维护成本一定是大的。我把他命名为：自己代码的命运掌握在别人的手中。 总结1.关于网厅爬取的任务，前提的工作一定要做好，主要是登录方式的研究和爬取页面数据的收集。2.爬取的规则尽可能找当地的会计进行确认。3.在没有足够数据的前提下，可以进行数据的模拟。4.做好心理准备，随时可能变更代码。 最后还是 心有余悸。如果我们验证的时间点没有跨越到8月份……。]]></content>
      <categories>
        <category>迭代错误</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[drds数据库脚本引发的问题——第一弹]]></title>
    <url>%2F2018%2F10%2F19%2Fdrds%E6%95%B0%E6%8D%AE%E5%BA%93%E8%84%9A%E6%9C%AC%E5%BC%95%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94%E7%AC%AC%E4%B8%80%E5%BC%B9%2F</url>
    <content type="text"><![CDATA[说明这篇文章内容比较敏感，所以不会发布到github上，只会保存在本地。 drds执行错误sql以下的sql执行在dev，test环境没有任何问题，问题暴露在release环境。sql如下：12341.update tx_employee_variation set is_martyr = is_disabled, is_bereaved_gaffer = is_disabled;2.update tx_employee_variation set is_no_domestic_residence = &apos;0&apos; where employee_id in (select id from tx_employee where area = &apos;1&apos;);3.update tx_employee_variation set is_no_domestic_residence = &apos;1&apos; where employee_id in (select id from tx_employee where area = &apos;2&apos;);4.update tx_need_declaration_setting set period_type = 1 where column_code in (&apos;100040&apos;,&apos;100050&apos;) and customer_id in (大概10w的数据量); sql.2和sql.3错误的类型一样，所以并在一起说。 release暴露问题release环境真很重要，至少可以在drds上暴露sql脚本的很多问题，不至于将问题遗留到真正发布的时刻。 关于drds sql执行的一些错误，阿里云有官方的帮助文档，有疑问请参考。上面官方可能会给出一些错误的解决方法。 sql.1第一次执行，报错：1[Err] 3009 - [d550cc4c3000000][10.98.6.41:3306][xqy_testportal]ERR-CODE: [TDDL-4620][ERR_FORBID_EXECUTE_DML_ALL] Forbid execute DELETE ALL or UPDATE ALL sql. 解释：drds对于全表的更改是不予许的。解决： 官方给出了解决方法，在执行的sql前面增加/!TDDL:FORBID_EXECUTE_DML_ALL=false*/，去掉无法执行全表的校验。注意，官方文档中给的惊叹号是中文的，请手动改成英文再执行。 DBA的意见是在sql后面加上where 1=1，增加条件语句即可跳过drds的检查。 sql.2和sql.3第一次执行，报错：1ERR-CODE: [TDDL-4617][ERR_SUBQUERY_LIMIT_PROTECTION] The number of rows returned by the subquery exceeds the maximum number of 100000. 解释：drds对于子查询中量级超过10万就会禁止。解决： 官方给出的解决办法是优化sql。 网络上找到了联表更新方法，将sql优化为： update tx_employee_variation as a,tx_employee as b set a.is_no_domestic_residence = ‘0’ where a.employee_id = b.id and b.area = ‘1’; 优化的sql第一次执行，报错： 1ERR-CODE: [TDDL-4601][ERR_EXECUTOR] not support cross db update. 解释：drds认定以上语句为跨库更新了。 解决： DBA出手解决了这个问题，两张表都是分库分表，在联表更新的情况下，必须加上分库分表健。于是改写成了以下sql：update tx_employee_variation as a,tx_employee as b set a.is_no_domestic_residence = ‘0’ where a.customer_id = b.customer_id and a.employee_id = b.id and b.area = ‘1’; sql.4比较特殊，10万的数据量，来源于cc_servyou这张表，xqy_portal和cc_servyou分属于不同的库，不能进行联动查询更新——DBA。来自DBA的建议： 10w的数据量肯定是不能放在一个in中的，drds执行会报错，支持大概0.5w左右 10w的数据量分成10w条sql跑，执行的速度会很慢 建议将10w的数据拆分成20*0.5的量放在执行语句in中 总结本次迭代的sql脚本也花费了相当一部分时间去解决的。值得好好总结一下。1.全表更新或者删除操作，可以按官方文档解决或者后面添加where 1=1条件，新测第二种方法执行效率更快。2.联表查询更新时，如果表是分库分表的，一定要加上分库分表健。3.in 语句 数据量大时，可以进行适当数量的拆分。 最后感谢，领导的英明神武——搭建了release环境；DBA—— jhh的专业指导；xp，wjn的sql测试和陪伴。 ps：release和线上的cc库是否能赋予我们查询的权利。]]></content>
      <categories>
        <category>drds</category>
      </categories>
      <tags>
        <tag>sql脚本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——低保真的可用性测试]]></title>
    <url>%2F2018%2F10%2F18%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E4%BD%8E%E4%BF%9D%E7%9C%9F%E7%9A%84%E5%8F%AF%E7%94%A8%E6%80%A7%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[可用性测试你怎么知道你的应用程序能正常工作。也许你的程序通过了编译，也许它通过了所有的单元测试。也许它还成功通过了QA的严酷考验。然而所有的这些都不能说明你的程序可以正常工作。如果你不找来真正的用户做可用性测试的话，你无法知道你的程序能否正常工作。 低保真测试演变基本理念：如果你想知道你的软件是否容易使用，那么在一些人尝试使用它们的时候观察他们，记下他们遇到的问题，然后修正这些问题，之后在度测试。 刚开始可用性测试非常昂贵，需要建立一个可用性测试的实验室，招募大量的测试用户，以得到有统计意义的结果，每次测试需要花费2～5万美元。因为成本原因不会经常使用。 1989年，jakob Nieob写了一篇文章，”打折扣的可用性测试”(中文翻译)。文章中指出可用性测试不是非得那样做不可。不需要可用性实验室，减少很多的测试用户，也能得到相同的结果。唯一的问题是聘请一个人主持一次测试仍然得花上5000～15000美元。 这里应该使用更加激进的方法。我们要合理利用客户这个群体资源。可行的话，找几个真实的用户来进行新上线的功能的检验。使用配置白名单的方式来进行小群体的测试也是可行的方案，之后可以逐步扩大开放范围。 关系图减少了测试人员的数量，你也可以获取可观的测试结果。从上图可知，少量的测试人员也可以发现相当的软件问题。(此图来自jakob Nieob的文章) 准则以下是低保真可用性测试的知道原则： 我该在什么时候测试理想情况下，在出现重大变更的时候，需要进行测试。比如更改用户的操作习惯等。 我需要找多少用户从图中的关系来看不需要太多。 要找什么样的用户一般情况下是随机抓一些人，只要会用电脑就行。但是对于一些专业性软件，需要找专职的人员测试。 测试要持续多长时间尽量保持简单，每个用户保持在45分钟～1个小时。 在哪里测试无所谓 测试人员应该具备怎样的特质需要一定的耐心，专业软件专业人才来测试。 我需要什么设备有些客户习惯在线直接测试，但是有些可能希望观看录制的视频，或者观看内部人员的直播。 我要为测试做什么准备提前想好要展示的东西，草拟一些步骤和说明。 测试需要花多少钱理想的情况是最好不要花费一分钱，找一些希望有这个功能的客户进行免费的测试。 我们应该怎样解读测试的结果这个比较重要。向开发团队和任何感兴趣的项目干系人汇报，或者可以以邮件的形式发送到相关的人员。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——结交混世魔猴]]></title>
    <url>%2F2018%2F09%2F13%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E7%BB%93%E4%BA%A4%E6%B7%B7%E4%B8%96%E9%AD%94%E7%8C%B4%2F</url>
    <content type="text"><![CDATA[起源2012年年底，Netflix技术博客上出现了一篇文章，题为”5 Lessons We’ve Learned Using AWS”(转向亚马逊网络服务过程中学到的5个教训)。亚马逊网络服务无疑是所谓”云计算”的杰出代表。因此，这篇文章实际上也可以说是给任何想要转向”云”网站的箴言。 疯狂的猴子下面是文章中的描述，”我们的工程师在AWS里最早创建的一个系统其实是”混世魔猴”。这只猴子的工作就是捣蛋，它要随机杀死我们系统架构里的组件或者服务。” 乍一看，肯定觉得这条建议太疯狂了！但是我们必须面对它。应该没有多少公司认同这样的做法，更别提会有多少公司真的去尝试了。如果在你工作的地方有人部署了一个后台服务，专门用于随机杀死自家服务器集群里的服务或进程，那么可以出门左转财务部。 没得选择“混世魔猴”不会安分守己，即使你不去结交，它也会自己找上门来。谁也无法预料在运行中，程序会不会出现各种诡异的问题。”如果我们不这样持续检验我们在失败面前自我恢复乃至成功的能力，那么这个系统很可能恰恰会在关键时刻掉链子”。 结交“混世魔猴”虽然讨厌，但是它对于服务来说也是有帮助的，它可能使得你的服务韧性十足。每件事情发生的背后总是有原因的。要避免失败，最好的办法就是不断地失败。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——费茨定律和无限宽度]]></title>
    <url>%2F2018%2F09%2F01%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E8%B4%B9%E8%8C%A8%E5%AE%9A%E5%BE%8B%E5%92%8C%E6%97%A0%E9%99%90%E5%AE%BD%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[费茨定律在人机交互领域，“费茨定律”被证明是一个最要的公式。它就是：Time = a + b log2(D/S + 1)其中，这里的 D 表示从光标的起始点到目标的距离，而 S 表示目标物体的宽度。这个理论完全是在运动轴线方向上的一个二维平面内讨论的。 “费茨定律” 在各种各样的条件下生效，这些条件包括肢体(手，脚，头戴式视觉，眼睛注视)，操作特征(输入设备)，物理环境(包括水下！)和用户群体(年轻人，老人，智障者，吸毒者)。注意，公式里的常数a和b是经验值，它们在不同的条件组合下的值各不相同。 Mac菜单“费茨定律” 决定了Mac平台上下拉菜单的获取速度应该比Windows上的下拉菜单获取快大约5倍。而且这已经被证明了。为了让导航变得更加容易一些，你要么把可点击的东西都挨着放在一起，要么就把可点击的区域做得大一些。或者双管齐下。事实上，Mac上的菜单并没有特别大。但Mac上的菜单没有依附在应用程序窗口上——它们永远停留在屏幕的顶端。既然光标停在屏幕的边缘，用”费茨定律“来计算的话，Mac上的菜单就有无穷大！因此，用户也就能更快地导航到Mac上的菜单。 总结“费茨定律”不仅仅是关于“把东西做得更大一点使得点击起来更容易”的理论，它还指导我们要尽最大的可能去利用屏幕边缘的自然边界：*“费茨定律”暗示，在任何电脑显示器上能被最快访问到的目标是屏幕的4个角落，然而，因为它们的图钉动作，设计师们似乎多年来都不惜一切代价地回避它们。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——分页显示该休矣]]></title>
    <url>%2F2018%2F08%2F31%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E5%88%86%E9%A1%B5%E6%98%BE%E7%A4%BA%E8%AF%A5%E4%BC%91%E7%9F%A3%2F</url>
    <content type="text"><![CDATA[并非抛弃当我第一次看到这个标题时，我是怀着好奇的心情去看待的，同时内心也是沉重的，因为我目前参与的项目中没有一个是抛弃分页显示的。这一小节所讲的并不是完全地抛弃分页显示的技术。 快速找到结果没错，我们应该避免没头没脑地生成一个包含有成千上万个条目的列表，然后用“一刀切”的方式把他们分页显示出来。这样就是把所有的负担都扔给了用户，这完全是不合理的！上面的内容用技术的术语来讲就是： 少量的数据，我们可以使用分页，不会阻碍用户寻找他想要知道的答案 数量达到一定的值，我们就需要考虑是否需要增加筛选，搜索和排序的功能 比如好的搜索引擎，返回的结果可能有十几页的数量，但是它一定会做很高效的排序让用户快速地进行选择——google，当然不是所有的搜索引擎都是那么优秀的，在不能翻墙的年代里，很难找到高质量的博客(一般都是无脑地转载)或者对于一个问题的解决方法。 除了排序，筛选和搜索的功能也能让用户在茫茫的数据中找出他想要的结果。 无穷分页法这种分页方式是没有分页按钮的，一般多用于手机App应用。也就是当用户看到页面底部的时候动态地加载更多的内容。传统的分页法对于用户来说不是特别友好(当然是你没有很好解决第二点的情况下)，但无穷分页法也不是完美无暇的——它自身也有缺点和陷阱： 用户永远也不知道下面还有多少的内容，所以在用无穷分页法的时候让用户觉得下面还有多少内容对用户来说不重要——参考头条。 无穷分页法不应该破坏深度链接。用户仍然能干净利落地链接到列表里任何一个特定的条目。 无穷分页的方法不受“网络蜘蛛“的青睐，无法去迎合网络搜索引擎。 当你动态加载新条目的时候，请提供一些用户看得见的反馈信息，这样用户就能自己判断出”内容还没有完，还有新的内容正在被加载，并不是程序死掉了…“。…]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——用户界面代表了软件]]></title>
    <url>%2F2018%2F08%2F27%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E7%94%A8%E6%88%B7%E7%95%8C%E9%9D%A2%E4%BB%A3%E8%A1%A8%E4%BA%86%E8%BD%AF%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[难用即bug一般在项目收尾阶段拒绝任何改动，因为不管改动有多么正当，理性的理由，哪怕最小的改动都会带来更多的bug风险。以后我们可以给一个产品定性一个自带的bug属性，那就是是否受到用户的欢迎。这个bug是区别于功能的，区别于测试阶段发现的任何bug。如果你的系统有一个出色的界面，并且你在资金和时间方面预算充足，你就可以继续在这个系统上工作。即使这个系统有bug或者运行缓慢，你也可以去改进它。但如果你的系统有一个糟糕的界面，你基本上就一无所有了。所以这个自带的bug和界面有不可分割的关系。 用户并不关心当你的产品发布上去让用户使用的时候，你觉得用户关心的是什么？人们往往会更加关心他们所看到的，用户根本不想知道你产品服务端的界面有多么牛逼，你在visio画了多少了不起的架构设图，用户界面就代表了你的软件。系统的界面有多重要，想象一下，最高深的服务架构+超级糟糕的前端界面。有时候提升服务端的性能可能会花上很大的成本，而且客户可能并不买帐(因为客户并不关心他们不可触摸的东西)，然而改变一下你界面的一个小细节带来的成本或许少的多。用很小的成本换回用户超高的体验，我们何乐不为呢？ 纸上原型设计接下来的问题是，在不依赖我们的开发工具的前提下，我们怎么来为用户界面做原型设计呢？有个很简单的方法，就是在纸上做。从某种程度上来说，纸上原型设计是一种永远也不会过时的方法。*纸上的原型设计除了会影响你当前项目的质量以外，它有另外一个好处——它有助于你职业生涯的发展。在你学到的所有知识中，有多少在10年以后仍然有用？20年以后呢？技术的保存期跟香蕉一样。相比之下，纸上原型设计方法的保存期接近于纸张的保存期。一旦你学会了纸上原型设计，就可以在你接下来的职业生涯中参与的每一个项目里都使用它。20年之后什么用户界面技术会比较流行，但我相信，我们还是必须对那些设计做可用性方面的评估。 纸上原型设计的电子化对于软件开发来说是一种革新：原型设计感觉有点交互性了；相比于纸上更加容易修改；电子化更加适合于分布式开发的团队协作模式。Jensen Harris——作为office 2007团队的首席用户界面设计师，他介绍Power Point 做原型设计——呵呵。这个不是唯一的选择，对于一些简易的设计，网上有在线的工具可以进行原型设计。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——你永远不会有足够的奶酪]]></title>
    <url>%2F2018%2F08%2F22%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E4%BD%A0%E6%B0%B8%E8%BF%9C%E4%B8%8D%E4%BC%9A%E6%9C%89%E8%B6%B3%E5%A4%9F%E7%9A%84%E5%A5%B6%E9%85%AA%2F</url>
    <content type="text"><![CDATA[引言令人赏心悦目的产品和让人勉强容忍的产品之间的差别，就在于它们的细节有没有被做好。 哥伦比亚障碍装置哥伦比亚障碍装置，首度在 Human Factors International 公司(一个由商业心理学家和管理咨询师组成的咨询公司)做的一篇演讲文稿里被提及。实验装置如下图所示：实验装置说明：装置的一头是一只老鼠，另一头放置奶酪，中间用电网隔断。来研究奶酪的大小和电击的大小对老鼠是否会跨越电网的影响。 实验结果从上图看出，电击越小，奶酪越大，老鼠跨越电网的可能性越大。其实这个实验结论可以适用于用户是否会使用产品这个领域中。 没有足够的奶酪用户和可用性之间有着很自然的对应关系。要么放上最大的奶酪(让你的应用足够吸引人)，要么把电击降到最低(让你的应用容易被使用)。软件开发人员也许会认为自己的应用程序很吸引人，但是它们在用户眼里真的那么有吸引力吗？我们应该保持怀疑的态度——你的奶酪没有你想象中的那么大。要让用户真正地使用你的软件，如果你对此还抱有一丝希望的话，忘掉奶酪吧——确保你不会“电击”你的用户。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——程序员的高效工作场所]]></title>
    <url>%2F2018%2F08%2F05%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E9%AB%98%E6%95%88%E5%B7%A5%E4%BD%9C%E5%9C%BA%E6%89%80%2F</url>
    <content type="text"><![CDATA[程序员的&lt;&lt;权利法案&gt;&gt;如果一个公司愿意付给一个开发人员6万～10万美元的薪水，却用糟糕的工作条件以及破烂的硬件设施摧残他，这是令我难以置信的。提议我们应该制定一个针对程序员的&lt;&lt;权利法案&gt;&gt;，以防止公司拒绝给程序员提供取得成功所必要的基础条件，最终达到保护程序员权利的目的。 1.每个程序员都应该有两台显示器如果你想把开发者的生产力发挥到极致，请确保每个开发者都有两台显示器。 2.每个程序员都应该有一台快速的电脑首先电脑长时间运行不能卡顿，就这一点而言windows真的可以抛弃了，我觉得mac应该算是标配；有足够的内存支持，你可能需要运行很多的软件，没有足够的内存，运行的越多电脑越慢；电脑速度快，每次编译，调试的周期也就短，开发的效率高。 3.每个程序员都应该自己选择鼠标和键盘一般来说，公司配置的鼠标和键盘是标配，不一定适合每一个人。每个程序员手的手感和大小也都不一，对于键盘按下的感觉也不一致，追随自己的感觉，选择最合适自己的。 4.每个程序员都应该有一把舒适的椅子理论上，程序员靠屁股每天坐上8个小时，为何不在一把舒适的，设计优良的椅子上度过那8小时呢？ 5.每个程序员都应该能快速接入互联网软件开发环境中，网络一定要合格。不能光有有线网络，无限网络是必须的，因为移动办公是很常见的工作方式，做到开机即连。在国内，在补充一条，一定有途径做到翻墙。 6.每个程序员都应该有安静的工作环境编程需要全神贯注。程序员在一个嘈杂的环境下是很难高效工作的。 人体工程学人体工程学很重要，这门学科运用在软件开发中可以减缓程序员身体机能的衰退。如果你关注自己的身体健康，下面有几点注意事项： 显示器的顶端应该与眼睛齐平 桌子的表面应该和肚脐眼基本持平 脚应该在地板上平放，并且膝盖关节成90度(呵呵，不可能) 当你打字的时候，手腕应该和前臂成一条直线，不要弯上弯下，也不要侧弯 背景光的功效程序员有一样东西是无法容忍的，那就是从头顶上直射下来的光线，或者其他方向的光线导致屏幕不方便眼睛的查看，这也是程序员更加喜欢待在黑暗的环境中的原因。在大部分情况下，尽管不开灯的房间更适合阅读电脑显示屏上的东西，但待在这种黑暗的房间里本身也有问题。有研究表明，坐在一间黑屋子里长时间盯着一个超亮的矩形区域，这对眼睛同样有害。有效的解决方法是在电脑的背后制造背景光。背景光是间接光照和光线补偿的完美结合。它能减轻视力疲劳，并且创造一个更加美好，更加舒适的计算机显示体验。请参考mac电脑背后的信仰之灯。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——坏苹果]]></title>
    <url>%2F2018%2F08%2F04%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E5%9D%8F%E8%8B%B9%E6%9E%9C%2F</url>
    <content type="text"><![CDATA[坏苹果效应生活中的小实验，两堆新鲜的苹果，在其中一推中放入一个烂苹果。在等待中你会发现，放入烂苹果的那堆一定烂得更快。你可以把这个效应仿照到团队成员中，一个有问题的团队成员，也就是一个典型的“坏苹果”。 坏苹果的特征一些警报的信号，用以识别你的团队中是否有“坏苹果”存在： 他们掩饰自己的无知，而不是尽力去向他们的团队伙伴学习。 他们对个人的隐私有着过度的渴望，典型就是“我不需要任何人来查看我的代码”。 他们很在意自己的地盘。他们会说：”我代码里的问题没人能修复。但是我现在太忙了，没有时间去管它们。我打算下周处理它们。” 他们抱怨团队所做的决定，并且在团队已经继续前进了很久之后还会重拾旧题。典型就是“我还是认为，我们应该回过头去修改我们上个月讨论的那个设计。我们当初的那个是行不通的”。 所有其他的团队成员都在传说关于同一个人的俏皮话或者抱怨他(简称吐槽)。 他们不会积极投入团队的行动。 团队的毒药当你积极地对项目宣战并以你的团队成员为敌时，你就成为项目的一个负担。毫无疑问，有“坏苹果”的团队会表现得比较差。更加糟糕的是，“坏苹果”的效应，其他团队成员开始呈现“坏苹果“的特征。团队的绩效可以从团队里最差的那位成员身上准确地预测出来。 有趣的实验Will Felps——华盛顿大学的一位教授，曾经组织过一次社会学实验：大学生们每4个人一组，组建几个团队，并指派了同样的任务。为了激励团队，表现最好的那个团队每人都能得到100美元奖励。但是某些团队中安插着一些“坏苹果”，他们具有以下的特征： 沮丧的悲观主义者。他会抱怨任务太无趣，并且公开质疑团队的取胜能力。 混球。他会否定其他人的想法，但是自己从来不拿出想法。 懒鬼。他们会经常说“随便“或者”无所谓“。原本的结果预测是：团队会主导个人，而不是反过来。然而实验的结果表明，团队并没有传统观点上认为的那么强有力，毒害一个团队是多么的容易。他们往往表现为上一节中所描述的行为。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——会议是浪费工作时间的最佳去处]]></title>
    <url>%2F2018%2F08%2F03%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E4%BC%9A%E8%AE%AE%E6%98%AF%E6%B5%AA%E8%B4%B9%E5%B7%A5%E4%BD%9C%E6%97%B6%E9%97%B4%E7%9A%84%E6%9C%80%E4%BD%B3%E5%8E%BB%E5%A4%84%2F</url>
    <content type="text"><![CDATA[疑问现在你在自问一下，那些会议中有多少是值得参加的？如果把相同的时间用在工作上，你又能完成多少事情？这不禁让人想知道，我们究竟为什么要开会？ 会议的准则我们应该以怀疑的态度去看待会议，把它当作是一种降低工作效率的风险。我们举行会议是因为我们认为我们需要它们，但事实上，会议往往只是在浪费宝贵的工作时间。就我而言，我采用以下几个原则，以确保我的会议是真正有用的：1. 会议绝不应该超过一个小时，否则应该判死刑。 对于任何会议，第一个并且重要的是约束会议的时间。因为时间在任何公司都是宝贵的资源；会议的时间太长，参与会议的人员很难记住会议的重点。会议的时间太长说明这个会议可能讨论的范围太宽泛了，或者整体缺少一个必要的焦点。 2.每个会议都应该有一个清晰的目标声明 一个清晰的目标对于缩短会议的时间有很大的帮助，这样能使整体的会议有一个必要的焦点。只要确保每个人都很清楚会议的目的，剩下的事自然会水到渠成。 3.在开会之前预先做好功课 既然你的会议有了一个清晰的目标声明，那么每个与会者都应该提前知道他们将要讨论和分享的内容。主持会议的人需要整理资料文档，参加会议的人需要阅读资料文档。会议邮件的附件中尽量加入会议的资料文档。 4.把会议变成可选的 “强制”的会议是站不住脚的。每一个出现在会议上的人都应该是因为他们想要在那里，或者他们需要在那里。在组织会议的时候考虑团队中哪些成员需要参加，甚至途中哪些成员可以离开。 5.在会议结束时概括一下待办事项 任何一个真正有成效的会议都会做一些决定，然后就直接导致某些事情的发生。作为会议的住持者你需要记录会议中已经达成一致的约定，然后在会议结束的时刻进行总结和再声明。 总结在软件开发中，会议基本是你避无可避的流程(除非你是一个猛人，单核模式)。但是会议千万不要成为我们挥霍时间的地方，我们需要规划我们的流程会议。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——结对编程和代码审查]]></title>
    <url>%2F2018%2F08%2F02%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E7%BB%93%E5%AF%B9%E7%BC%96%E7%A8%8B%E5%92%8C%E4%BB%A3%E7%A0%81%E5%AE%A1%E6%9F%A5%2F</url>
    <content type="text"><![CDATA[结对编程首先来说一下结对编程的表象：两个开发者在同一台机器上工作。他们都有各自的键盘和鼠标。就好像飞机的驾驶员和领航员，驾驶员负责编码，领航员的职责是阅读，核对，拼写检查以及在脑子里测试代码。总结就是一个人在编写代码的同时另一个人在检查核对所写的代码。社区中对结对编程有一些正面的看法，我在这里整理和总结了一下： 缩减后期bug出现的概率，因为两个人多了一双眼睛。 两个人总有不同的技能，技能的传递往往发生在结对中。当一个人向另一个展示一些技巧的时候，实际上他们在进行临时的培训。 保证一个团队当中有超过一个的人熟悉这块代码。 交换的结对编程可以彼此熟悉，加深交流。 结对的成本结对编程固然有很多的好处，但是在软件开发中我们还需要考虑成本的问题。对于同一个业务开发你可能需要投入两倍的人力成本去完成这个结对编程。加入了人力成本的因素之后，结对编程的优点也就不是很突出了。 代码审查经过笔者工作的亲生经历，我可以毫无保留的担保代码评审的价值。其实，结对编程的很多好处都可以通过可靠的同级评审来获得。结对编程的优势在于它的即时性，代码审查是需要编码开发的后期来进行实现的。但是如今有许多的工具可以利用来节省代码审查的人力和时间： sonar扫描——发现编程中一些常见的隐藏bug checkstyle——代码格式检查和拼写检查 gerrit——同级评审制度 喜忧参半代码审查解决了结对编程中需要的高人力成本，同时也保证了代码的检查完成。但是，似乎没有人愿意花时间去真正理解那些并不简单的新代码，所以反馈通常是比较笼统的。而且那些所谓的工具只是发现常见通用的问题，对于具体的业务代码，你仍然需要花费时间去审查。人要在原有的代码基础上添加功能或者修改错误的时候，他们通常会有很多的反馈，甚至推到重来的冲动。所以代码审查真的是一项需要耐心和忍耐力的活。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——性能制胜]]></title>
    <url>%2F2018%2F08%2F01%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E6%80%A7%E8%83%BD%E5%88%B6%E8%83%9C%2F</url>
    <content type="text"><![CDATA[性能的重要性已经有大量的实验数据表明：网站载入和显示的速度越慢，使用它的人就会越少。比如：google发现，显示10个结果的网页需要0.4秒来生成，显示30个结果的网页需要0.9秒来生成。半秒钟的延迟会造成20%的流量下降。半秒钟的延迟就扼杀了用户的满意度。 在A/B测试中，亚马逊试着以100毫秒为单位逐步增加页面的延迟，结果发现即使是非常小的延迟也会付出高昂的代价，导致收入显著下跌。 于是，你如何把自己跟别人区别开来呢？你得从“快”入手，其他的先放一放。 雅虎的指导原则自2007年以来，构建一个快速的网站的黄金参考准则始终首推雅虎提出的“加速你的网站的13条简单原则”。这里有些不错的建议，但是，其中很多的建议只有在你运营了一个每天有几百万独立用户访问的网站时才有意义。 比如：用户与你网络服务器的邻近程度直接影响着响应时间。从用户的角度来看，把你的内容部署在多个地理位置分散的服务器上会让你的网页加载得更快。作为性能优化的最后一步，stack overflow把他们所有的静态内容都部署到了cnd。结果很令人振奋。 但是，不会推荐大家直接上cdn。因为你的网站流量说不定还没有到达需要使用这一招的时候。因为在雅虎的清单上有一大推改善性能的做法是免费而且很容易实现。自2007年以来，使用cdn的成本降低了很多，用起来也简便了很多。这要归功于有更多的公司参与了这个领域的竞争，它们包括亚马逊，NetDNA，CacheFly等。因此，当时机成熟的时候，而且你已经做完了雅虎清单上要求做的事情，你就可以考虑cdn了。 中国的互联网公司亚马逊是墙外著名的电商互联网公司，而在国内，淘宝是首曲一指的。如果你的服务是缓慢的，遇到了瓶颈，我觉得淘宝技术这十年是很好的参考手册。里面讲述了淘宝从最初的小流量服务，到现在如何成为撑起每天那么大流量的互联网企业。淘宝是如何一步步在遇到困难的时候，革新技术，解决服务瓶颈，在漫长的岁月中往复做着这个事情。最后最好的机器和技术已经无法满足淘宝的业务，技术创新诞生了，并且至今形成了淘宝自己的技术体系。你可以借鉴里面的东西，来优化你自己的服务。 总结对大多数网站而言，性能是一种特性，能克敌制胜。几乎所有的用户更喜欢使用快速的网站。在这里，我想我们还应该学到一个教训——在开放的互联网世界里竞争，最后只会剩下两种网站：要么很快，要么已经死去。而你又将何去何从。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——电梯测试]]></title>
    <url>%2F2018%2F07%2F31%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E7%94%B5%E6%A2%AF%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[何为电梯测试电梯的特点是层层递进的，最终达到你想要到达的目的地。电梯测试也是类似，你可以把它理解为一系列层层递进的为什么，最终得到你想要的答案。接下来笔者会给出例子方便读者理解。 编码的意义软件开发者们是真心喜爱编写代码的。但根据经验，他们当中很少有人可以解释清楚他们为什么在编写代码。如果你不信，你可以从你的团队里找个人来测试一下——这里的测试就是所谓的电梯测试。 电梯测试询问软件开发人员，为什么要做那个，继续问下去，直到你得到一个可以让你的客户理解的原因： 你在做什么？ 我在修复这个数据网格的排序问题。 你为什么要解决这个问题？ 因为它在bug清单上。 它为什么在bug清单上？ 因为有个测试人员把它作为一个bug报出来了。 为什么它被作为一个bug报出来了？ 测试人员认为这个字段应该按照数字顺序而不是字母顺序来排序。 为什么测试人员这么认为？ 很显然，如果把 “条目2“ 排在 “条目19“ 的后面，用户在查找的时候就会有麻烦。 项目远景模型软件开发者认为他们的工作就是编写代码。其实不然。 你团队里的每个人都应该能够通过由陌生人主持的电梯测试——在60秒之内，清晰地解释他们在做什么，以及为什么人们会在意他们正在做的事情。一个项目远景模型可以帮助团队成员通过“电梯测试“。它遵循如下的形式： 为了(目标客户) 他们(关于需求或者机会的说明) 这个(产品名称)是(产品类别) 它的(关键优势，吸引人的购买理由) 不像(主要竞争对手的替代产品) 我们的产品(主要的差异化的特性说明)——《跨越鸿沟》 作用和意义电梯测试和创建项目远景可以帮助团队持续专注于产品的关键方面，否则团队很容易就会被短期开发迭代中的问题缠住，从而失去对整个项目远景的控制。软件开发人员切记的是——所有的短期迭代都是为了实现产品最后的愿景。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——高效编程原则]]></title>
    <url>%2F2018%2F07%2F30%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E9%AB%98%E6%95%88%E7%BC%96%E7%A8%8B%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[永远都是你的错作为一名谦逊的程序员，最基本的要求就是要有意识：你写的代码在任何时候出了问题，那一定都是你的错。 在大多数项目中，你所调试的代码里常常混杂着这些东西：你和项目小组中的其他成员开发的应用代码，第三方的产品，以及平台环境。无论你的软件出现什么样的问题——甚至最开始出错的地方根本就不是你的代码——你也应该总是假定问题出在你的代码里，并根据这个假设采取行动。 说实在，程序员都有一种高傲的性格，一般都是不愿意承认问题出在自己那边。而且出错之后一定会背负领导或者其他各方的压力的。可是事实永远不会因为你的否认而改变，该背的锅还是得背的。 大道至简代码不是什么好东西。代码会随着时间的推移慢慢腐烂。代码需要周期性的维护。并且里面还藏着bug。但是这些问题真正不在于代码，代码不是我们的敌人。你想看看真正的敌人吗？去照一照镜子吧。问题就在那里！作为一个软件开发者，你就是自己最大的敌人。你越早认识到这点，你的境况就会越好。在编码的过程中，你可以从很多维度评价你的代码： 代码的简洁度 功能的完整性 执行速度 编码所花费的时间 健壮性 灵活性我们想要的是一个实用而明智的策略，以缩减一个程序员在想要了解程序的工作原理时所需阅读的代码量。对于一个具体的需求，软件编程向来都是大道至简，然后依据测试的结果按需提升其他的维度。 避免写注释你应该总是专注于编写代码，而忘了还有注释这种东西存在。这里并不是让大家不要注释，否定注释。相反，我认为注释是很重要的东西，用很多注释装饰代码是件好事，但是在代码中加入大片大片的注释反而不是景上添花。注释不是给编译器看的，注释是用来和其他人交流的，你所要做的是在代码核心处与实用方式上添加注释，其他地方你可以使用编程技巧使代码通俗易懂。注释不是那么简单的，好的注释取决于你是否是一个好的作家，是否有良好的沟通能力。作为一个软件开发着，真的是不容易。一些有趣的注释 学会读源代码在沟通这个复杂的领域，写出能让人类领会并理解的连贯的段落比敲出几行不至于让解释器或编译器呕吐的软件代码要难的多。这就是为什么——就软件开发而言——所有的文档大概就是很差劲的，而且，由于为人写作比为机器写作要困难的多，恐怕在可预见的将来，文档还会继续差劲下去。对此，你基本上无能为力的。不管文档上面怎么说，源代码才是最终的事实，是你所能找到的最好的，最确定的，最新的文档。在此，并不想否定文档的作用，相反文档是很重要的，只是想要强调阅读源码的价值。但是，阅读源码是一项费力的活，耐不住寂寞文档就是很好的工具。 向橡皮鸭求助遇到解决不了的困难时，恐怕你做的最多的就是去网上寻求帮助，应该没有人等待并祈祷着吧！在寻求帮助的同时，你应该： 用足够多的细节来描述发生的状况，尽量让别人能了解到底发生了什么事情。 告诉他人你为什么需要知道答案，是闲来无事的好奇，还是在具体的项目中遇到了障碍。 说一说为了解决这个问题你都做过什么研究，以及你自己的发现。别人没有义务花费时间来帮助你，你应该有自己的分析。 适当的组织你的问题，酝酿一个合格的问题可能会帮你更快的找到答案。 如果解决了问题，尽量点赞和喝彩，积累的点赞可以帮助后来者快速注意到解决方法。 创新以人为本上面的标题需要这样断句：创新，以人为本。这一小节讲的是执行力的问题。一个好的创意值多少钱？可能它有点值钱，但是也值不了多少。创意本身不会毫无价值，但是很显然，单有创意只是空头支票。好的执行力才是好创意的倍增器。在软件开发领域，执行意味着专注于构成你的应用程序的所有微小细节。如果你不是始终沉迷于你的应用程序的每个方面，不去持续优化和改进它的每一处细节，那么你就不是在执行。至少，不是在很好的执行。你的团队执行得怎样，决定了他们会把你的创意从黄金变成废铁，还是从废铁变成黄金。创意需要人去执行的，甚至整个团队去执行的，所以最后执行力还是取决于程序员本身，是以人为根本和基础的。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——沟通一直都是问题]]></title>
    <url>%2F2018%2F07%2F27%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E6%B2%9F%E9%80%9A%E4%B8%80%E7%9B%B4%E9%83%BD%E6%98%AF%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[引言沟通，对于程序员来说一直都是不可回避的。但是往往也是容易忽略的，因为我们就是这样的人群，默默地在深夜工作着，似乎根本不需要与他人沟通，但是在软件开发中，沟通一直都是存在的问题。 经典语录杰出的程序员跟勉强过得去的程序员之间的差别，不在于他们掌握了多少种编程语言，也不在于他们谁更擅长python或者java。真正的关键是，他们能不能把他们的想法表达清楚。杰出的程序员通过说服别人来达成协作。通过清晰的注释和技术文档，他们让其他程序员能够读懂他们的代码，这也意味着其他程序员能够重用他们的代码，而不必重新去写。要不然，他们所写的代码价值就大打折扣了。如果你还没有感觉，那么我只能残忍的告诉你，你可能还没有达到勉强的地位。 程序员的素质程序员必备的素质： 扎实的基本功 编程的技巧 坚韧的毅力 良好的沟通技巧 博客编写博客是一个很好的提升沟通能力的方式。对于博客的态度，你需要花费大量的时间去更新或者维护，比如你需要回复提问，复看以前的博客是否存在问题等等。这里推荐几个好的博客站点： github stack overflow 简书 csdn 知乎 使用的场景在软件开发的生涯中，作为程序员你一定会遇到以下的场景： 推行会议的流程 编写手册文档 团队协作…以上所有的场景，都是需要你的沟通技巧，所以沟通一直都是一个问题。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件开发不只写代码那么简单——程序员的8种境界]]></title>
    <url>%2F2018%2F07%2F26%2F%E8%BD%AF%E4%BB%B6%E5%BC%80%E5%8F%91%E4%B8%8D%E5%8F%AA%E5%86%99%E4%BB%A3%E7%A0%81%E9%82%A3%E4%B9%88%E7%AE%80%E5%8D%95%E2%80%94%E2%80%94%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%848%E7%A7%8D%E5%A2%83%E7%95%8C%2F</url>
    <content type="text"><![CDATA[寄语平心而论，不能为了编程而编程，大多数想要学习编程的人一般都是被盲目的信仰刺激。所以我奉劝看到这篇博客的各位，请下定自己的决心之后，在决定是否仍要踏入这个行业。 什么是程序员？程序员是耗尽毕生精力去编写代码，使他人从代码编写工作中解脱的人，因此，宣扬 “每个人都需要知道如何去编程”，我认为是一种倒退！ 生命中最困难的，是想清楚你真正想要做的事情，而不是学上一堆假设将来会有用的东西。如果说你的研究和探索最终还是引领你走上了编程之路，那就用尽一切方法去学习吧！ 8境界在求职的时候，你也许会被问到这样的问题：你对自己未来5年的职业是怎样规划的？对于这样问题的回答不只是为了应付面试官，而是为了你自己。在心中你究竟想过怎样的生活？作为一个程序员，最完美的职业生涯规划应该是怎样的？ 在此，我通过书籍整理了大致8种境界：1.不朽的程序员这个是最高的境界！总结一句话就是：人不在江湖，但江湖仍然有他的传说。你可能没有见过这些人，但是在书籍中你可能读到过他的名字，你可能正在读他发表的论文，或者你正在使用一些基础的技术，而这些技术正是通过他的论文实现的或者就是由他实现的。他们在生前可能获得过图灵奖，成为计算机博物馆中的一个永久收藏，其他众多的程序员都在学习他的作品。抱歉，虽然很残忍，但是我想说几乎大部分程序员在一生中都无法达到这个境界。 2.成功的程序员总结一句话就是：自己有很强的编程能力，并且具有很强的商业头脑，将代码商业模式化。所以他们运营着一个不错的公司，甚至控制了整个产业链。他们拥有绝对的自由，可以做任何自己想做的事情。这一境界的程序员，相比于编程能力，更多的是需要商业上的才能。 3.知名程序员达到这一境界的程序员也不错。他们可能在一家非常知名的大型技术公司工作，也可能在一家很有影响力的小公司或者是在一个很有希望的创业团队工作。不管怎么样，其他的程序员都或多或少听说过他们，并且他们在自己所在的领域有着积极的影响。 4.胜任的程序员首先，你的能力是你在工作中游刃有余，你从来不会为得到一份满意的工作而发愁。你的同事也非常尊敬你。每一家你曾经工作过的公司都因为你的加盟而在某些方面得到了提升。 5.普通的程序员这个境界了，你基本上能够应付一般的编程工作。由于天资所限，他们很难成为杰出的程序员。很残忍，大部分进入这个领域的人也就到此止步了。但是，天赋跟成功的关系并不大。如果你有敏锐的商业嗅觉和不错的人际交往能力，你依然可以变得很成功。如果你考这一行当过上了不错的生活，这已经说明你很才了。人贵有自知之名。普通你的能力都会比你自认为的要低。缺乏天赋并不是什么大不了的事情。要勇敢一点，发掘自己的特长，并且充分加以发挥。 6.业余程序员这个群体一般是一些很有前途的学生或实习生，也可能正在参与某些开源项目，或者利用个人闲暇时间开发一些好玩的应用程序。他们是一群很有想法，充满激情的人。这个境界的程序员可以通过自我提升，迅速地胜任程序员这个职业。 7.低调的程序员还有一些坊间流传的比较有个性的程序员。他们很有能力但是没什么令世人瞩目的成就。他们可能服务于某家大型公司。写代码仅仅是份工作而已，并非他们生活的全部。 8.烂程序员总结一句话：没有金刚钻，却偏偏拦了瓷器活。这个级别的程序员技能极其匮乏，他们通常是阴差阳错地干上这一行的。他们所做的任何事情都会给他们的同事带来痛苦和灾难。如果你是这样的，你是否有采取行动改善自己，或者远离这个行业。 举例不朽的程序员:Dijkstra这样表示，你一定不知道。如果你学过数据结构，是否记得其中的最短路径算法就是以他命名的？如果你学过操作系统，是否记得有一个调度算法——银行家算法？ Ken ThompsonDennis Ritchie这两位就不用介绍了。大学时代你的第一门语言就是他们发明的。很多语言都是以此为基础在上面进行的扩展。unix系统的发明者，当年火星计划的后继者。 knuth如果你学过算法，你可能知道他。你是否读到过《具体数学》，《研究之美》这些书籍。]]></content>
      <categories>
        <category>书籍整理</category>
      </categories>
      <tags>
        <tag>非技术总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql死锁——insert操作死锁]]></title>
    <url>%2F2018%2F05%2F21%2Fmysql%E6%AD%BB%E9%94%81%E2%80%94%E2%80%94insert%E6%93%8D%E4%BD%9C%E6%AD%BB%E9%94%81%2F</url>
    <content type="text"><![CDATA[mysql基础参数不用的mysql版本，加锁的过程不一样；mysql的数据库引擎和事务隔离方式也对分析死锁有帮助。这里涉及的mysql基础参数的查询方式与此次错误无关。1234567891011121314151617181920212223mysql&gt;select version(); # 查询mysql的版本+-----------+| version() |+-----------+| 5.7.9-log |+-----------+# 这个是测试环境mysql版本mysql&gt;show variables like &apos;%engine%&apos;; # 查询mysql的数据库引擎+----------------------------+--------+| Variable_name | Value |+----------------------------+--------+| default_storage_engine | InnoDB || default_tmp_storage_engine | InnoDB || storage_engine | InnoDB |+----------------------------+--------+mysql&gt;select @@global.tx_isolation, @@session.tx_isolation, @@tx_isolation; # 查询mysql的事务隔离方式+-----------------------+------------------------+-----------------+| @@global.tx_isolation | @@session.tx_isolation | @@tx_isolation |+-----------------------+------------------------+-----------------+| REPEATABLE-READ | REPEATABLE-READ | REPEATABLE-READ |+-----------------------+------------------------+-----------------+ InnoDB锁类型InnoDB的锁类型有很多，根据网上的博客，笔者在此进行整理。或者读者可以参考csdn。 1.基本锁1.1共享锁(Shared Locks：S锁) mysql允许持有S锁的事务读取一行，加了S锁的记录允许其他事务再加上S锁，但加不了X锁(参考下方)。1.2排他锁(Exclusive Locks：X锁) mysql允许持有X锁的事务更新或删除一行，加了X锁的记录不允许其他事务再加上S锁或者X锁。 2.记录锁(Record Locks) 记录锁, 仅仅锁住索引记录的一行。 单条索引记录上加锁，Record lock锁住的永远是索引，而非记录本身，即使该表上没有任何索引，那么innodb会在后台创建一个隐藏的聚集主键索引，那么锁住的就是这个隐藏的聚集主键索引。所以说当一条sql没有走任何索引时，那么将会在每一条聚集索引后面加X锁，这个类似于表锁，但原理上和表锁应该是完全不同的。 3.间隙锁(Gap Locks) 区间锁，仅仅锁住一个索引区间(开区间)。 在索引记录之间的间隙中加锁，或者在某一条索引记录的之前或之后加锁，并不包括该索引记录本身。 4.next-key锁(Next-Key Locks) 默认情况下InnoDB使用Next-Key Locks来锁定记录。 但是当查询的索引含有唯一属性的时候，即查找到的记录只有一条，那么Next-Key Locks会进行优化，变成Record Locks。 5.插入意向锁(Insert Intention Locks) Gap Lock中存在一种插入意向锁（Insert Intention Lock），在insert操作时产生。在多事务同时写入不同数据至同一索引间隙的时候，并不需要等待其他事务完成，不会发生锁等待。 假设有一个记录索引包含键值4和7，不同的事务分别插入5和6，每个事务都会产生一个加在4-7之间的插入意向锁，获取在插入行上的排它锁，但是不会被互相锁住，因为数据行并不冲突。 *注：插入意向锁并非意向锁，而是一种特殊的间隙锁。 测试环境问题业务场景：企业信用操作，前端会并发ajax请求获取企业的信用。当数据库有数据时直接显示，没有值时从局端获取数据并插入数据库，锁表就发生在数据插入的时刻。当时排查问题已经定位了表锁了，但是不知道具体的原因，在guox的帮助下进行了相关排查。在此感谢guox大牛。 InnoDB日志mysql死锁脱离了业务层，需要使用命令查看InnoDB状态(包含最近的死锁日志)。1mysql&gt;show engine innodb status; # 数据库执行即可 以下是当时拿到的相关的日志：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162InnoDB &quot;=====================================2018-05-22 08:31:00 0x7f51e4968700 INNODB MONITOR OUTPUT=====================================Per second averages calculated from the last 26 seconds-----------------BACKGROUND THREAD-----------------srv_master_thread loops: 1972057 srv_active, 0 srv_shutdown, 19348138 srv_idlesrv_master_thread log flush and writes: 21320195----------SEMAPHORES----------OS WAIT ARRAY INFO: reservation count 7610437OS WAIT ARRAY INFO: signal count 8000876RW-shared spins 0, rounds 7852374, OS waits 3412636RW-excl spins 0, rounds 19305504, OS waits 392660RW-sx spins 197857, rounds 4219933, OS waits 67445Spin rounds per wait: 7852374.00 RW-shared, 19305504.00 RW-excl, 21.33 RW-sx------------------------LATEST DETECTED DEADLOCK------------------------2018-05-22 08:30:50 0x7f51dd08d700*** (1) TRANSACTION:TRANSACTION 115900879, ACTIVE 0 sec insertingmysql tables in use 2, locked 2LOCK WAIT 6 lock struct(s), heap size 1136, 4 row lock(s), undo log entries 2MySQL thread id 1925809, OS thread handle 139987345471232, query id 161369824 10.199.134.32 rootinsert into tx_credit_info(id, customer_id, year, grade, score, report_id) values ( gen_id(&apos;tx_credit_info&apos;), 127374372, &apos;2017&apos;, &apos;C&apos;, &apos;47.3&apos;, &apos;127374372_20180522083050902_1CE2&apos; )*** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 11344 page no 4 n bits 80 index UNIQUE_INDEX_CUSTOMER_ID_AND_YEAR of table `xqy_test`.`tx_credit_info` trx id 115900879 lock_mode X locks gap before rec insert intention waitingRecord lock, heap no 10 PHYSICAL RECORD: n_fields 3; compact format; info bits 0 0: len 8; hex 8000000007979427; asc &apos;;; 1: len 4; hex 32303137; asc 2017;; 2: len 4; hex 80000bbd; asc ;;*** (2) TRANSACTION:TRANSACTION 115900880, ACTIVE 0 sec starting index readmysql tables in use 2, locked 24 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 1925817, OS thread handle 139989577422592, query id 161369834 10.199.134.32 root statisticsselect next_value into ret_val from sys_sequence where table_name= NAME_CONST(&apos;tableName&apos;,_utf8&apos;tx_credit_info&apos; COLLATE &apos;utf8_general_ci&apos;) for update*** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 11344 page no 4 n bits 80 index UNIQUE_INDEX_CUSTOMER_ID_AND_YEAR of table `xqy_test`.`tx_credit_info` trx id 115900880 lock_mode X locks gap before recRecord lock, heap no 10 PHYSICAL RECORD: n_fields 3; compact format; info bits 0 0: len 8; hex 8000000007979427; asc &apos;;; 1: len 4; hex 32303137; asc 2017;; 2: len 4; hex 80000bbd; asc ;;*** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 10619 page no 4 n bits 424 index idx1_sys_sequence of table `xqy_test`.`sys_sequence` trx id 115900880 lock_mode X locks rec but not gap waitingRecord lock, heap no 315 PHYSICAL RECORD: n_fields 2; compact format; info bits 0 0: len 14; hex 74785f6372656469745f696e666f; asc tx_credit_info;; 1: len 4; hex 80000520; asc ;;*** WE ROLL BACK TRANSACTION (2)...(此处省略) 关于如何读死锁的日志，读者可以参考以下文章。 日志分析12345678910111213*** (1) TRANSACTION:TRANSACTION 115900879, ACTIVE 0 sec insertingmysql tables in use 2, locked 2LOCK WAIT 6 lock struct(s), heap size 1136, 4 row lock(s), undo log entries 2MySQL thread id 1925809, OS thread handle 139987345471232, query id 161369824 10.199.134.32 rootinsert into tx_credit_info(id, customer_id, year, grade, score, report_id) values ( gen_id(&apos;tx_credit_info&apos;), 127374372, &apos;2017&apos;, &apos;C&apos;, &apos;47.3&apos;, &apos;127374372_20180522083050902_1CE2&apos; )*** (1) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 11344 page no 4 n bits 80 index UNIQUE_INDEX_CUSTOMER_ID_AND_YEAR of table `xqy_test`.`tx_credit_info` trx id 115900879 lock_mode X locks gap before rec insert intention waiting 日志中gen_id()是mysql中自定义的方法，可以使用：mysql&gt;show create function gen_id;查看gen_id方法具体干了什么，方法如下：12345678910111213141516171819CREATE DEFINER=`root`@`%` FUNCTION `gen_id`(`tableName` VARCHAR(64)) RETURNS int(11)BEGINDECLARE ret_val INT;#不支持显示事务，mysql默认会开启事务和提交事务#start transaction; select next_value into ret_val from sys_sequence where table_name=tableName for update; update sys_sequence set current_value=current_value+step, next_value=next_value+step where table_name=tableName;#commit;RETURN ret_val;END 从日志中分析，事务1更新了表sys_sequence，持有了sys_sequence相关记录的X锁，然后事务1在等待唯一索引UNIQUE_INDEX_CUSTOMER_ID_AND_YEAR的RECORD LOCKS。然后看事务2的日志：12345678910111213141516*** (2) TRANSACTION:TRANSACTION 115900880, ACTIVE 0 sec starting index readmysql tables in use 2, locked 24 lock struct(s), heap size 1136, 2 row lock(s)MySQL thread id 1925817, OS thread handle 139989577422592, query id 161369834 10.199.134.32 root statisticsselect next_value into ret_val from sys_sequence where table_name= NAME_CONST(&apos;tableName&apos;,_utf8&apos;tx_credit_info&apos; COLLATE &apos;utf8_general_ci&apos;) for update*** (2) HOLDS THE LOCK(S):RECORD LOCKS space id 11344 page no 4 n bits 80 index UNIQUE_INDEX_CUSTOMER_ID_AND_YEAR of table `xqy_test`.`tx_credit_info` trx id 115900880 lock_mode X locks gap before recRecord lock, heap no 10 PHYSICAL RECORD: n_fields 3; compact format; info bits 0 0: len 8; hex 8000000007979427; asc &apos;;; 1: len 4; hex 32303137; asc 2017;; 2: len 4; hex 80000bbd; asc ;;*** (2) WAITING FOR THIS LOCK TO BE GRANTED:RECORD LOCKS space id 10619 page no 4 n bits 424 index idx1_sys_sequence of table `xqy_test`.`sys_sequence` trx id 115900880 lock_mode X locks rec but not gap waitingRecord lock, heap no 315 PHYSICAL RECORD: n_fields 2; compact format; info bits 0 事务2可以看到持有了UNIQUE_INDEX_CUSTOMER_ID_AND_YEAR的RECORD LOCKS，然后事务2在等待sys_sequence相关记录的X锁。 最终循环等待，导致死锁，最后mysql回滚了事务2。 解决方案从日志上看是并发情况下，gen_id方法引起的错误。该方法只有在测试环境下存在，release环境和prod环境使用的阿里云的自增机制，资源不在同一个库里。guox推测线上不会出现这样的问题，release发布之后拭目以待。 长久的解决方案：将测试环境的自增机制的资源放到其他的库中；或者修改交互方案，改变ajax并发的请求，使用循环http请求的方式(以前的业务一般都是这种方式，所以从来没有暴露过死锁的问题)。]]></content>
      <categories>
        <category>DBA</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于jackson转化json的原理]]></title>
    <url>%2F2018%2F04%2F11%2F%E5%85%B3%E4%BA%8Ejackson%E8%BD%AC%E5%8C%96json%E7%9A%84%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[背景web工程中，数据交互是不可避免的，相比xml，json是现在流行的数据交互。在调试接口中，发现返回字段的大小写不是我所期望的，原本应该返回的nNum字段变成了nnum，这样就导致和前端约定的有出入了。 jacksonweb工程中，比较流行的框架是springMVC+spring+mybatis。数据交互由springMVC完成，但是springMVC也不是自己序列化json的，它将这个工作交给了jackson。jackson对object进行序列化的过程中确实存在key大写变小写的问题。看我娓娓道来。 序列化原理这里只对jackson的序列化原理做阐述，其他的序列化工具不一定是相同的原理，不可套用。众所周知，json的数据格式是，key:value的形式，现在的问题就出现在key的大小写这边。下面我们对其进行一定的测试：object中的field都有其对应的get,set方法，一般都会选择是IDE自动生成。如下图所示：其运行的结果当然没有问题： jackson在序列化的时候如何定义key呢？jackson会获取field对应的get方法方法名，比如getXxx，然后进行将get进行截断，变成Xxx，最后将其小写，变成xxx。 如果我们将xxx改成xXx，但是没有更改其get方法，key仍然是xxx，并不是我们期望的xXx： 网上的很多博客基本都只提及大写转小写，其实不然，jackson只会将连续的大写转换成小写，如果中间断了，之后的大写字符也不会处理了，并且jackson是从开头检测的，如果开头就是小写，那么之后的大写字符也不会处理了。测试如下： 如何避免正如网上的博客所说，你需要在field和其对应的get方法上加上对应的标签，然后jackson在序列化的时候就会以你的field名称为key： 总结 spring的序列化工作是由jackson完成(你也可以配置其他的序列化工具) jackson序列化的key定义与field名称无关，反而和其get方法名称有关 jackson的大写转小写从开头检测，并且一定是连续的 jackson的这种序列化机制是可以避免的，这样可以以field名称作为key]]></content>
      <categories>
        <category>spring</category>
      </categories>
      <tags>
        <tag>问题排查</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python网络数据采集——存储数据]]></title>
    <url>%2F2018%2F03%2F25%2Fpython%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E2%80%94%E2%80%94%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[文件下载12345678910111213抛出一个问题：对于爬虫而言，链接中的文件是下载呢，还是只是保存url链接，使用时直接链路？这里我总结我所知道的优缺点：不下载优点：1.爬虫运行更快，耗费的流量更少2.可以节省很多的存储空间3.存储url的代码更容易写，不需要实现文件下载代码4.降低目标主机服务器的负载缺点：1.盗链，每个网站可能会实施防盗链措施2.文件仍然在别人服务器上，跟着别人的节凑走3.盗链的图片容易改变 在python中可以引入 urlretrieve 进行文件的直接下载，在下载之前找到链接是必须的，这就要使用之前的 find 函数了。1234567891011from urllib.request import urlopenfrom urllib.request import urlretrievefrom bs4 import BeautifulSouphtml = urlopen(&quot;http://www.pythonscraping.com/&quot;)bsObject = BeautifulSoup(html, &quot;html.parser&quot;)b = bsObject.find(&quot;a&quot;, &#123;&quot;id&quot;: &quot;logo&quot;&#125;)img = b.find(&quot;img&quot;)location = img[&quot;src&quot;]print(location)urlretrieve(location, &quot;logo.jpg&quot;) 用爬虫下载文件会使你的电脑处于危险的环境当中，因为不知道下载的文件是否恶意软件。如果不能确定下载的文件是否安全，请不要轻易尝试这个.py脚本。 以csv格式存储csv，Comma-Separated Values，逗号分隔值，是存储表格数据的常用格式，如：1234fruit,costapple,1.00banana,0.30pear,1.25 参考以下代码，如何将数据写入到csv格式文件中：12345678910111213141516# 这边在引入的时候要注意，千万不要引入 python3 框架中的模块import _csv as csv# import ospath = &quot;./csv/test.csv&quot;# if not os.path.exists(path):# os.mkdir(path)with open(path, &apos;w+&apos;) as csvFile: try: writer = csv.writer(csvFile) writer.writerow((&apos;number&apos;, &apos;number plus 2&apos;, &apos;number times 2&apos;)) for i in range(10): writer.writerow((i, i+2, i*2)) finally: csvFile.close() python 中新建文件的机制考虑的非常周到，如果文件不存在则会创建。然后注意引入代码上面的注释，python3 框架中也存在csv模块，应该引入内置申明中的包。 csv 的数据形式和html中的表格数据展示相似，所以一般用来存储爬取到的表格中的数据。1234567891011121314151617181920212223import _csv as csvfrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen(&quot;http://en.wikipedia.org/wiki/Comparison_of_text_editors&quot;)bsObj = BeautifulSoup(html, &quot;html.parser&quot;)# 获取页面上的第一个表格table = bsObj.find_all(&quot;table&quot;, &#123;&quot;class&quot;: &quot;wikitable&quot;&#125;)[0]# print(table)rows = table.find_all(&quot;tr&quot;)# print(rows)path = &quot;./csv/table.csv&quot;with open(path, &quot;wt&quot;, newline=&quot;&quot;, encoding=&quot;utf-8&quot;) as csvFile: writer = csv.writer(csvFile) try: for row in rows: csvRow = [] for cell in row.find_all(&#123;&quot;td&quot;, &quot;th&quot;&#125;): csvRow.append(cell.get_text()) writer.writerow(csvRow) finally: csvFile.close() mysql整合之所以需要mysql，因为爬取的数据不能光在内存中处理，还需要进行持久化处理。因为我们可能需要将数据进行展示或者进行后一步的处理。 关于mysql的下载，自行google。macOS系统 可以使用 brew install mysql命令下载。这里使用 pymysql 这个第三方工具整合 mysql ，可以使用pip进行下载。12345678910111213141516import pymysql# 打开数据库db = pymysql.connect(&quot;localhost&quot;, &quot;root&quot;, None, &quot;wyc&quot;)# 创建游标cursor = db.cursor()# 执行sql语句cursor.execute(&quot;select version()&quot;)# 打印信息print(cursor.fetchone())# 关闭数据库db.close() 这段程序有两个对象:连接对象(conn )和光标对象(cur )，一个连接可以有很多个光标。一个光标跟踪一种状态 (state)信息，用完光标和连接之后，千万记得把它们关闭。如果不关闭就会导致连接泄漏 (connection leak)。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python网络爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python网络数据采集——使用API]]></title>
    <url>%2F2018%2F03%2F19%2Fpython%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8API%2F</url>
    <content type="text"><![CDATA[API在网络爬虫的世界里，你可以通过爬取html页面上的信息来收集信息，而使用开放的API接口是另一种方式。这里开放的API接口指的是：服务器可访问的http请求。其实html中的数据也是通过访问http请求获得的，但是这里是直接利用请求返回的数据。为何是可访问的？因为对于http请求，服务器或许是要做验证的，一般的post的请求必须做验证，get请求相对放宽限制。 google API对于API方面的开放，业界没有一家企业做的比google还好了，而且大部分都是免费的，只是每个月需要限制访问的流量。对于API的访问，google是有做校验的，需要相应接口的key，读者可以按照以下的步骤获取(但是首先你必须可以翻墙)： 首先拥有google的账号，没有的话自己去注册 访问api控制台，查询你需要的api 找到对应的api，生成相关的秘钥即可 生成的key，可以去凭证中查看 这里以google的maps接口为例，如下图搜索maps，需要地图相关的API：你可以通过点击API和服务中的库跳转到上图界面：google中你可以在信息中心界面查看你API访问的情况：对于访问时需要的key可以在凭证中查看： 然后你就可以使用key来通过API获取信息了：123456789比如：# 将街道解析成经纬度https://maps.googleapis.com/maps/api/geocode/json?address=1+Science+Park+Boston+MA+02114&amp;key=&lt;你的API key&gt;# 用 Time zone(时区)API 获取任意经纬度的时区信息https://maps.googleapis.com/maps/api/timezone/json?location=42.3677994,-71.0708078&amp;timestamp=1412649030&amp;key=&lt;你的 API key&gt;# 用地点经纬度获取对应的海拔高度https://maps.googleapis.com/maps/api/elevation/json?locations=42.3677994,-71.0708078&amp;key=&lt;你的 API key&gt; 解析json如果使用http访问的API接口，你必然会遇到解析json的问题。python中可以使用自带的json包进行解析。这里以 http://freegeoip.net/json/ 为例，来获取ip地址对应的城市：12345678910import jsonfrom urllib.request import urlopendef getCountry(ipAddress): response = urlopen(&quot;http://freegeoip.net/json/&quot;+ipAddress).read().decode(&apos;utf-8&apos;) responseJson = json.loads(response) return responseJson.get(&quot;country_code&quot;)print(getCountry(&quot;121.97.110.145&quot;)) 对于一些多重复杂的json表达式，loads方法将其解析为key-value的形式：1234567jsonString = &apos;&#123;&quot;arrayOfNums&quot;:[&#123;&quot;number&quot;:0&#125;,&#123;&quot;number&quot;:1&#125;,&#123;&quot;number&quot;:2&#125;], &quot;arrayOfFruits&quot;:&apos; \ &apos;[&#123;&quot;fruit&quot;:&quot;apple&quot;&#125;,&#123;&quot;fruit&quot;:&quot;banana&quot;&#125;,&#123;&quot;fruit&quot;:&quot;pear&quot;&#125;]&#125;&apos;jsonObj = json.loads(jsonString)print(jsonObj.get(&quot;arrayOfNums&quot;))print(jsonObj.get(&quot;arrayOfNums&quot;)[1])print(jsonObj.get(&quot;arrayOfNums&quot;)[1].get(&quot;number&quot;)+jsonObj.get(&quot;arrayOfNums&quot;)[2].get(&quot;number&quot;))print(jsonObj.get(&quot;arrayOfFruits&quot;)[2].get(&quot;fruit&quot;)) wiki百科实践这里我们仍然以wiki作为例子，在wiki百科的历史编辑区中，会记录编辑人员的ip地址，我们用上面的方法再加上网页跳转的形式，来获取相关的地区信息。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#! /usr/local/bin/python3# encoding:utf-8from urllib.request import urlopenfrom urllib.error import HTTPErrorfrom bs4 import BeautifulSoupimport jsonimport datetimeimport randomimport rerandom.seed(datetime.datetime.now())def getLinks(articleUrl): html = urlopen(&quot;http://en.wikipedia.org&quot; + articleUrl) bsObj = BeautifulSoup(html, &quot;html.parser&quot;) return bsObj.find(&quot;div&quot;, &#123;&quot;id&quot;: &quot;bodyContent&quot;&#125;).findAll(&quot;a&quot;, href=re.compile(&quot;^(/wiki/)((?!:).)*$&quot;))def getHistoryIPs(pageUrl): # Format of history pages is: http://en.wikipedia.org/w/index.php?title=Title_in_URL&amp;action=history pageUrl = pageUrl.replace(&quot;/wiki/&quot;, &quot;&quot;) historyUrl = &quot;http://en.wikipedia.org/w/index.php?title=&quot;+pageUrl+&quot;&amp;action=history&quot; print(&quot;history url is: &quot; + historyUrl) html = urlopen(historyUrl) bsObj = BeautifulSoup(html, &quot;html.parser&quot;) # finds only the links with class &quot;mw-anonuserlink&quot; which has IP addresses instead of usernames ipAddresses = bsObj.findAll(&quot;a&quot;, &#123;&quot;class&quot;: &quot;mw-anonuserlink&quot;&#125;) addressList = set() for ipAddress in ipAddresses: addressList.add(ipAddress.get_text()) return addressListdef getCountry(ipAddress): try: response = urlopen(&quot;http://freegeoip.net/json/&quot;+ipAddress).read().decode(&apos;utf-8&apos;) except HTTPError: return None responseJson = json.loads(response) return responseJson.get(&quot;country_code&quot;)links = getLinks(&quot;/wiki/Python_(programming_language)&quot;)while(len(links) &gt; 0): for link in links: print(&quot;-------------------&quot;) historyIPs = getHistoryIPs(link.attrs[&quot;href&quot;]) for historyIP in historyIPs: country = getCountry(historyIP) if country is not None: print(historyIP+&quot; is from &quot; + country) newLink = links[random.randint(0, len(links)-1)].attrs[&quot;href&quot;] links = getLinks(newLink) 其实上面的方法再前面的章节中已经提及过，现在增加的只是将html上爬取的信息，再通过API的加工得到最终的信息。只是想要告诉大家，爬虫不只是解析html那么简单。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python网络爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python网络数据采集——链接爬取和跳转]]></title>
    <url>%2F2018%2F03%2F14%2Fpython%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E2%80%94%E2%80%94%E9%93%BE%E6%8E%A5%E7%88%AC%E5%8F%96%E5%92%8C%E8%B7%B3%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[网页跳转在网络数据采集中，如果只对单页面进行操作，那么这个爬虫将毫无意义。因为采集的数据往往是分散在不同的网页的，所以对于单页面而言，爬虫需要采集页面中的链接，并且进行跳转。网页跳转，在正在采集的页面中收集新的链接，然后将新的链接传入到采集程序中，如此之后，采集程序就已经在新的页面采集信息了。 举个例子我们以维基百科的一个页面为例，从主页面开始，采集 id 为 mw-content-text 的段落 p 和 id 为 ca-edit 的 span 中的 a 链接标签。读者可以参考以下代码：12345678910111213141516171819202122232425262728from urllib.request import urlopenfrom bs4 import BeautifulSoupimport repages = set()def getLinks(pageUrl): global pages html = urlopen(&quot;http://en.wikipedia.org&quot;+pageUrl) bsObj = BeautifulSoup(html, &quot;html.parser&quot;) # print(bsObj) try: print(bsObj.h1.get_text()) print(bsObj.find(id=&quot;mw-content-text&quot;).find_all(&quot;p&quot;)[0]) print(bsObj.find(id=&quot;ca-edit&quot;).find(&quot;span&quot;).find(&quot;a&quot;).attrs[&apos;href&apos;]) except AttributeError: print(&quot;页面缺少一些属性!不过不用担心!&quot;) for link in bsObj.find_all(&quot;a&quot;, href=re.compile(&quot;^(/wiki/)&quot;)): if &apos;href&apos; in link.attrs: if link.attrs[&apos;href&apos;] not in pages: # 我们遇到了新页面 newPage = link.attrs[&apos;href&apos;] print(&quot;----------------\n&quot;+newPage) pages.add(newPage) getLinks(newPage)getLinks(&quot;&quot;) 程序先从 http://en.wikipedia.org 网页开始，然后查找以 /wiki/ 开头的链接，之后拼接到 http://en.wikipedia.org 形成新的链接。对于重复的链接程序不进行采集，程序中体现了递归的概念，理论上你很难等到程序的结束。 六度空间理论所谓六度空间理论是数学界中的一种猜想，对于一个陌生人，你可以通过至多6个人认识他，即你与陌生人中间的连接点不会超过6个人。我们可以将这个猜想应用到爬虫程序中，即通过一个人的简介，我们可以爬取到另一个人的简介。这里还是以维基百科为例，从 Kevin_Bacon 开始，爬取其他人的信息：123456789101112131415161718from urllib.request import urlopenfrom bs4 import BeautifulSoupimport datetimeimport randomimport rerandom.seed(datetime.datetime.now())def getLinks(articleUrl): html = urlopen(&quot;http://en.wikipedia.org&quot;+articleUrl) bsObj = BeautifulSoup(html, &quot;html.parser&quot;) return bsObj.find(&quot;div&quot;, &#123;&quot;id&quot;: &quot;bodyContent&quot;&#125;).findAll(&quot;a&quot;, href=re.compile(&quot;^(/wiki/)((?!:).)*$&quot;))links = getLinks(&quot;/wiki/Kevin_Bacon&quot;)while len(links) &gt; 0: newArticle = links[random.randint(0, len(links)-1)].attrs[&quot;href&quot;] print(newArticle) links = getLinks(newArticle) 程序中使用随机数字，从新的链接集合中随机选取，并使用新的链接再次爬取。 scrapy写网络爬虫，你不得不重复一些简单的操作：找出页面上所有的链接，区分内链与外链，跳转到新的页面。像这种重复的工作可以交给第三方工具类来处理，scrapy就是这样一款工具。曾经的 scrapy 是一个傲娇的工具，只支持 python2.7 版本，软件的不普遍支持性会导致软件的不可用。好消息就是，现在的 scrapy 在 python3.x 的环境下也是支持的。$pip3 install scrapy 即可下载。如果是python2.x版本的，务必使用pip命令。scrapy 的使用需要重新创建一个工程(在这里演示如何获取网页的title)： $scrapy startproject wikiSpider 创建新工程 在 spiders 目录下创建 ArticleSpider.py文件，名字也可自取 在 item.py 文件中定义类 Article scrapy crawl article 运行程序(这行命令会用条目名称 article 来调用爬虫(不是类名，也不是文件名，而是由 ArticleSpider 的 name = “article” 决定的)) item.py文件中应如此定义：1234567891011121314151617181920# -*- coding: utf-8 -*-# Define here the models for your scraped items## See documentation in:# https://doc.scrapy.org/en/latest/topics/items.html# import scrapyfrom scrapy import Item, Field# class WikispiderItem(scrapy.Item):# # define the fields for your item here like:# # name = scrapy.Field()# passclass Article(Item): # define the fields for your item here like: # name = scrapy.Field() title = Field() scrapy的每个Item(条目)对象表示网站上的一个页面。当然，你可以根据需要定义不同的条目(比如url、content、header image等)，但是现在我只演示收集每页的title字段 (field)。 ArticleSpider.py 文件中写入如下程序：123456789101112131415161718192021222324#! /usr/local/bin/python3# encoding:utf-8from scrapy import Spider# 这里的引用简直有毒 scrapy crawl article# 像这样 from wikiSpider.wikiSpider.items import Article 编译器是正确的，但是终端执行是错误的# 要如此引用 from wikiSpider.items import Article 但是编译器会报错# 所以建议使用相对路径from .. items import Article# from wikiSpider.wikiSpider.items import Articleclass ArticleSpider(Spider): name = &quot;article&quot; allowed_domains = [&quot;en.wikipedia.org&quot;] start_urls = [&quot;http://en.wikipedia.org/wiki/Main_Page&quot;, &quot;http://en.wikipedia.org/wiki/Python_%28programming_language%29&quot;] def parse(self, response): item = Article() title = response.xpath(&apos;//h1/text()&apos;)[0].extract() print(&quot;Title is: &quot;+title) item[&apos;title&apos;] = title return item 注意文件中引入项目的注释。 如果正确，终端中运行的结果应该是：12Title is: Main PageTitle is: Python (programming language)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python网络爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python网络数据采集——BeautifulSoup]]></title>
    <url>%2F2018%2F03%2F11%2Fpython%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E2%80%94%E2%80%94BeautifulSoup%2F</url>
    <content type="text"><![CDATA[BeautifulSoupBeautifulSoup 是 python 中最受欢迎的 html 解析库。如果 BeautifulSoup 这个第三方库不能解决问题，读者可以考虑另外两个库：12345678xml：可以用来解析 HTML 和 XML 文档，以非常底层的实现而闻名于世，大部分源代码是用 C 语言写的。虽然学习它需要花一些时间(其实学习曲线越陡峭，表明你可以越快地学会它)，但它在处理绝大多数 HTML 文档时速度都非常快。如果遇到的是引用第三方库处理时遇到的性能瓶颈，可以考虑。HTML parser：这是 Python 自带的解析库(https://docs.python.org/3/library/html.parser.html )。因为它不用安装(只要装了 Python 就有)，所以可以很方便地使用。 find 和 find_all在 BeautifulSoup 中，这两个函数是经常使用的，专门用来查找 html 中指定的标签。接下来笔者用链接：http://www.pythonscraping.com/pages/warandpeace.html 做为例子来使用这两个函数。123456789101112131415#! /usr/local/bin/python3# encoding:utf-8from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen(&quot;http://www.pythonscraping.com/pages/warandpeace.html&quot;)bsObject = BeautifulSoup(html, &quot;html.parser&quot;)# 使用 findAll 方法查找 指定标签 指定class属性的nameList = bsObject.find_all(&quot;span&quot;, &#123;&quot;class&quot;: &quot;green&quot;&#125;)for name in nameList: print(name.get_text())# 像这样，你就抓取了html中所有的人名# 因为经过分析，发现 span 标签且 class 为 green 的内容都是小说的人名 通过源码你会发现 find_all 方法拥有以下参数：name=None, attrs={}, recursive=True, text=None, limit=None, **kwargs，在此说明一下 以前的 BeautifulSoup 版本方法名是 findAll。在开发的过程中，95%的情况下你只会使用到 name attars 这两个参数，但是在这里还是要说明一下其他参数的作用： name html中tag的名称 比如span h1 你可以传入多个tag的集合来查找 这个集合是一个set结构 attrs 字典集 用来指定 tag的一些属性 比如class width height recursive 表示在查找时 你是否想要递归 到子标签中查找 默认为True 递归 text 直接查找文本 有相同文本的匹配 limit 指定查找的数量 查找的顺序是按照 html 的顺序来的 默认 全部 kwargs 字典集 查找具有指定属性的标签 业界认为是冗余的设计 不推荐使用 并且存在缺陷 比如无法指定class属性 因为class是python的保留关键字123456789hList = bsObject.find_all(&#123;&quot;h1&quot;, &quot;h2&quot;, &quot;h3&quot;&#125;)print(hList)tList = bsObject.find_all(text=&quot;Anna Pavlovna&quot;)print(tList)nameList = bsObject.find_all(class_=&quot;green&quot;)print(nameList)# 但是你可以使用 class_ 来代替 find 的使用方式和 find_all 差不多，只是缺少了参数 limit 而已。 导航树先抛出一个问题：find_all 函数是通过标签的名称和属性来查找标签的，但是如果需要通过标签在文档中的位置来查找标签，该如何？引入导航树(Navigating Trees)的概念就是为何解决这个问题，比如：bsObject.tag.subTag.anotherSubTag。 在这里我们使用虚拟的购物网站：http://www.pythonscraping.com/pages/page3.html 来作为实例。对于使用导航树，你只需要学会处理三个问题： 处理子标签和后代标签 处理兄弟标签 处理父标签 子标签和后代标签12345678from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen(&quot;http://www.pythonscraping.com/pages/page3.html&quot;)bsObject = BeautifulSoup(html, &quot;html.parser&quot;)for child in bsObject.find(&quot;table&quot;, &#123;&quot;id&quot;: &quot;giftList&quot;&#125;).children: # print(type(child)) print(child) 子标签和后代标签在定义上是不同的：子标签只有一个层级的差距，后代标签则是一个或者多个层级差距。Tag对象 函数 children 和 descendants 分别获取子标签和后代标签。 兄弟标签兄弟标签表示 Tag类型相同且处于同一层级的，Tag对象 函数 next_siblings 和 previous_siblings 分别获取定位标签的上面的所有Tag和下面所有Tag。123456789101112比如：&lt;tr class=&quot;tr1&quot;&gt;&lt;/tr&gt;&lt;tr class=&quot;tr2&quot;&gt;&lt;/tr&gt;&lt;tr class=&quot;tr3&quot;&gt;&lt;/tr&gt;&lt;tr class=&quot;tr4&quot;&gt;&lt;/tr&gt;&lt;tr class=&quot;tr5&quot;&gt;&lt;/tr&gt;现在定位tr3，next_siblings可获取tr4，tr5；previous_siblings可获取tr2，tr1for sibling in bsObject.find(&quot;table&quot;, &#123;&quot;id&quot;: &quot;giftList&quot;&#125;).tr.next_siblings: print(sibling)# 与第一段代码相比，可以发现少了表格标题的tr，因为第一个tr标签不能将自己视为兄弟# 当然也可以使用next_sibling 和 previous_sibling 只不过获取到的是距离最近的那一个 父标签查找父标签在提取信息的过程中是偶尔的情况下才会用到的，如果那个信息比较深入，并且它的子标签有明显的特点方便查找，可以考虑使用。1234567print(bsObject.find(&quot;img&quot;, &#123;&quot;src&quot;: &quot;../img/gifts/img1.jpg&quot;&#125;) .parent.previous_sibling.get_text())# 1.找到Tag类型是 img 的且 src 是 ../img/gifts/img1.jpg# 2.向上寻找到父标签 即 td# 3.找到 td 标签的上一个兄弟标签 还是 td# 4.得到里面的 text 文本，即商品的价格]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python网络爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python网络数据采集——网络连接]]></title>
    <url>%2F2018%2F03%2F09%2Fpython%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[初识urllib要采集信息，首先你需要将html文件抓取到本地，python中使用urllib这个包来进行网络连接，并且进行html的抓取，如果是python2.x版本，使用的是urllib2这个包。下面贴出一段代码，读者可自行运行：123456#! /usr/local/bin/python3# encoding:utf-8from urllib.request import urlopenhtml = urlopen(&quot;http://pythonscraping.com/pages/page1.html&quot;)print(html.read()) 运行BeautifulSoupBeautifulSoup是第三方的python模块，所以需要进行安装，在此只介绍macOS系统的下载方式：12345678910python中一般使用pip来下载第三方的依赖包$sudo easy_install pip 首先确保你的系统中有pip命令$pip install beautifulsoup4 即可下载如果使用的是python3.x版本$pip3 install beautifulsoup4 需要在pip后面加上数字3一般macOS系统自带的是python2.x，在这里推荐使用brew来下载python3$brew search python$brew install python3 即可 BeautifulSoup在代码中的作用相当于html解析器，然后读者可以通过该模块的方法来获取一些html中的标签。在这里贴出BeautifulSoup的代码：123456789#! /usr/local/bin/python3# encoding:utf-8from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen(&quot;http://pythonscraping.com/pages/page1.html&quot;)bsObj = BeautifulSoup(html, &quot;html.parser&quot;)print(bsObj.h1) BeautifulSoup比较老的版本构造时不需要传入”html.parser” 参数，新的版本中，虽然结果仍然打印了出来，但是在运行过程中会报错。html.parser 是为了告诉BeautifulSoup 我使用的是 html 解析器。读者可以参考官方手册了解更过BeautifulSoup的知识点。 可靠的网络连接像上面写的代码，很有可能抛出异常，谁也不知道这个url是否可靠，如果服务器宕机了怎么办？等等问题，都会使得程序终止。想象一下，你的爬虫在晚上执行，你想早上起来查看收集的数据，但是由于中间的错误，导致之后的程序没有执行，你不得不再等待一天的时间。其实说了这么多，就是在代码层面中捕获异常。可靠的网络连接，要保证程序在运行过程中，即使中途遇到了错误，也能保证余下的代码可执行。大家参考下面的代码来发现不同点：12345678910111213141516171819202122from urllib.request import urlopenfrom urllib.request import HTTPErrorfrom bs4 import BeautifulSoupdef get_h1(url): try: html = urlopen(url) except HTTPError as e: return None else: try: bsObj = BeautifulSoup(html, &quot;html.parser&quot;) h1 = bsObj.h1 except AttributeError as e: return None return h1if __name__ == &apos;__main__&apos;: h1 = get_h1(&quot;http://pythonscraping.com/pages/page1.html&quot;) print(h1) 上面这段代码使用了python中经典的try…exception…else的格式，这样保证了即使发生了异常，也能保证else中的代码被执行。在python中获取异常可以使你的代码更加健壮，也是一个好的习惯。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python网络爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python网络数据采集——简介]]></title>
    <url>%2F2018%2F03%2F04%2Fpython%E7%BD%91%E7%BB%9C%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E2%80%94%E2%80%94%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[简介网络数据采集(网络爬虫)，是一种网络数据的采集方式，又称为网页抓屏，数据挖掘，网络收割或者其他类似的版本。网络数据采集是一种通过多种手段收集网络数据的方式，比如通过api交互，当然如果你有毅力可以选择手动摘抄，人们最常用的估计是使用浏览器进行网页的浏览，这其实也是一种网络数据采集。注：之后相关文章都使用网络爬虫这个术语。 意义已经有浏览器了，为何还需要网络爬虫呢？ 首先浏览器的搜索是主流的，你不一定能从中找到有用的信息。 你需要的数据可能来自不同的网站，为了分析和对比。 网站可能没有api(接口)提供给外部使用。 网络爬虫可以解决这些问题，爬取的数据也可以进行保存和更新，你也可以通过特定的方式进行展示，比如图表。实际上，这些都是手动摘抄都可以办到的，但是谁又会整天看着数据有没有改变，然后去更新手头的数据呢？ 扩展之前笔者是学习过一段时间的python基础的，但是笔者糊口的语言是java，python只是笔者业余的爱好罢了。笔者认为python的几大领域为网络数据采集，自动化和大数据分析，所以读者可以在学习了python的基础上选择一个方向再深入的研究。 关于python的基础知识，笔者的笔记并没有开放在网站上，读者可以自行寻找。 关于网络爬虫，笔者认为基本对现在的业务没有什么帮助，因为网络爬虫对于一个产品的稳定性来说一定是大打折扣的(因为谁有能保证你收集的信息来源是否还存在呢？)。但是可以给你的生活带来一些乐趣，人生在世，技多不压身嘛。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python网络爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络——NAT]]></title>
    <url>%2F2018%2F02%2F20%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94NAT%2F</url>
    <content type="text"><![CDATA[简介NAT(Network Address Translation)，网络地址转换。NAT可以看成一个开放到公网(全球因特网)上的公用路由器，用来代理一个子网下的所有主机的请求和应答。NAT在转换的时候，会将主机的ip地址和端口号替换成公用路由器的ip地址和端口，并且维护在一张表中进行一一对应，为了将返回的数据应答到唯一的主机上。 作用由于ipv4规则的ip地址消耗殆尽，不可能为所有的主机都分配真正的公网ip，所以很多主机ip地址都是虚拟的ip地址，并且有统一的代理服务器进行消息的请求和应答。NAT的作用就是将私有的ip地址转换成全球因特网承认的ip地址，私有的ip地址有三种：①10.0.0.0~10.255.255.255/8 ②172.16.0.0~172.31.255.255/12 ③192.168.0.0~192.168.255.255/16 这些IP地址是用于私有的网络。NAT使得一个组织局域网中的主机都能互相访问，但是想要访问公网就只有一条出路，这样也可以保证局域网中信息的安全性。 例子假设用户王某坐在家庭主机10.0.0.1傍边，请求域名为 www.bilibili.com (ip地址为112.49.19.4)的web服务器(端口为80)上的一个web页面。主机10.0.0.1为其指派了任意的端口2233，并将请求的报文发送到LAN中。NAT路由器收到该数据请求，为其生成了新的端口号5000，并将ip地址替换成广域网的ip地址138.76.29.7，然后继续想目标服务器请求。具体步骤如下： 10.0.0.1:2233 发送报文 —&gt; LAN LAN —&gt; NAT路由器 —&gt; 替换成138.76.29.7:5000 并维护一张映射表 138.76.29.7:5000 发送请求报文 —&gt; 112.49.19.4:80 返回数据到WAN —&gt; NAT路由器 NAT路由器从维护的映射表中获取私有ip地址 返回数据 —&gt; LAN —&gt; 10.0.0.1:2233 读者以前可能发现，百度的ip地址和主机的ip地址不同，因为一个是公网的，主机的只是私有的ip地址，中间就是利用了NAT。代理服务器，即NAT在跳转的当中可能存在很多个，即一个请求可能通过n个NAT。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>网路协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络——DHCP]]></title>
    <url>%2F2018%2F02%2F20%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94DHCP%2F</url>
    <content type="text"><![CDATA[简介DHCP(Dynamic Host Configuration)，即动态主机配置协议。由于DHCP具有能将主机连接进一个网络的网络相关方面的自动能力，故它又常被称为即插即用协议。DHCP协议维护了一张表(存放于DHCP服务器)，用来存放现子网中可以分配的ip，如果有主机离开或者接入子网，则DHCP服务器会更新这张表。 作用一旦一个组织获取了子网的ip地址，网络管理员需要给组织中的主机都配置ip(读者可以参考网络攻防——ip地址了解一个组织内主机ip地址配置过程)。DHCP的问世是为了节省网络管理员手动配置主机ip地址的时间。网络管理员只需要配置DHCP，以使某给定主机每次与网络连接时能得到一个相同的ip地址，或者某主机将被分配一个临时的ip地址，该地址每次与网络连接时也许是不同的。 协议的步骤对于一台新到达的主机而言，DHCP协议是一个4个步骤的过程：1234567891.DHCP服务器发现。一台新的主机接入子网时首要的任务时让DHCP服务器发现它。所以主机会在UDP分组中向端口67发送一个DHCP发现报文，并且这个数据时广播的，因为主机现有的情况是不知道DHCP服务器的具体ip地址的。2.DHCP服务器提供。DHCP服务器收到这个报文时，用一个DHCP提供报文向主机作出应答。这边也是使用的广播的形式，因为在子网中可能存在多个DHCP服务器，主机可以根据各个DHCP服务器返回的信息进行择优选择。3.DHCP请求。主机从一个或多个DHCP服务器中挑选一个，并向选中的服务器提供一个DHCP请求报文进行响应。4.DHCP ACK。服务器用DHCP ACK报文对主机请求报文进行响应，证实所要求的参数。 一旦主机完成了以上的4步，交互便完成了，主机也接入了子网，并拥有了子网中唯一的ip地址。 缺陷从移动性的角度看，DHCP确实有不足之处。因为每当节点连接到一个新的子网时，要从DHCP得到一个新的ip地址，当一个移动节点在子网之间移动时，就不能维持与远程应用之间的TCP连接。之后会研究移动IP(在此记录，以便之后补充)。]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>计算机网络</tag>
        <tag>网络协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络攻防——ip地址]]></title>
    <url>%2F2018%2F02%2F18%2F%E7%BD%91%E7%BB%9C%E6%94%BB%E9%98%B2%E2%80%94%E2%80%94ip%E5%9C%B0%E5%9D%80%2F</url>
    <content type="text"><![CDATA[这边先预设几个问题 一个组织分配ip地址的过程(涉及到ISP等概念) a,b,c,d,e类的ip地址 如何知道一个主机的子网掩码，它第一跳的路由地址，dns服务器的地址 DHCP动态分配地址 什么是网络地址转换(NAT) ipv4和ipv6的区别 基本概念ip地址目前有ipv4和ipv6两种，这里以ipv4举例。ip地址在整个网络中相当于地址的存在，只有知晓了地址，才能知道向哪里送信，信件又使从哪里发出来的。ipv4地址编码长度为32，即32个比特位，即4个字节，每个比特位只有1或者0，所以ipv4总共有2^32个可能的ip地址。(但是很遗憾，尽管看起来很多，ipv4方式的编码ip已经消耗殆尽，所以ipv6孕育而生) 读者可以使用ipconfig(linux系统使ifconfig)来查看自己的ip地址。读者会发现，显示的ip信息并不是以二进制形式显示，而是十进制。那是为了方便人们阅读，一般使用点分十进制记法书写，然而计算机在交流时仍然只认识二进制记法。1234如：193.32.216.9 的二进制记法是：11000001 00100000 11011000 00001001可以看出以8个比特，即一个字节作为一个分隔 在全球因特网中的每台主机和路由器上的每个接口，必须有一个全球唯一的ip地址(在NAT后面的接口除外，NAT即网络地址转换，之后会提及这个概念)。然而，这些地址不能随意地自由选择，一个接口ip地址的一部分需要由其连接的子网来决定。 标准类别的ip地址ip地址由网络地址和主机地址组成。根据网络地址的比特位数，将ip地址分为A,B或C类网络，网络地址的比特位数分别被限制为8,16或24位，这是一种被称为分类编址的编址方案。 在计算机网络中，读者可能看到形如：233.1.1.0/24的ip地址记法，这是子网地址的写法，其中/24记法有时称为子网掩码，表示32位比特中的最左侧24比特定义了子网地址，即该子网中所有主机分配到的ip地址的前24位都是相同的，即网络地址相同。比如：12一个子网的ip地址为：233.1.1.0/24那么这个子网下的主机的ip地址可能是：233.1.1.1，233.1.1.4 233.1.1.128 ... 在分配子网的时候，不一定使用标准的网络地址。比如一个组织，分类到了一个C类(/24)子网，其仅能容下2^8 - 2 = 254台主机(2^8 = 256，其中的两个地址预留用于特殊的用途)，这对于许多组织来说太小了。然而一个B类(/16)子网可支持多达65534台主机，又太大了。 组织内主机分配ip地址的过程为了获取一块ip地址用于一个组织的子网，网络管理员也许首先会与他的ISP参考百度百科联系，该ISP可能会从已分配给它的更大的地址块中提供一些地址。这里假设ISP自己已被分配了地址块200.23.16.0/20：1234ISP的地址块 200.23.16.0/20 (11001000 00010111 0001)0000 00000000组织0 200.23.16.0/23 (11001000 00010111 000100)0 00000000组织1 200.23.18.0/23 (11001000 00010111 0001001)0 00000000... 加入组织0已经得到了子网地址：200.23.16.0/23，这就表示该组织下的所有主机ip地址都是以11001000 00010111 000100开头的。系统管理员一般手动配置路由器ip地址，主机地址也能手动配置，但是想象一下：如果一个组织下有数以万计的主机，那么系统管理员已哭晕在厕所。所以这里引出另一个概念DHCP(动态主机配置协议，又称即插即用协议)，主机可以自动获取一个临时的ip地址。这也解释了为什么在公司中，主机重启之后主机的ip地址会改变，因为是临时随机分配的。 DHCP笔者认为这个概念可以重新开一个章节，读者可以跳转参考计算机网络——DHCP NAT读者可以跳转参考计算机网络——NAT 如何生动形象解释这里主要解释了ip地址，子网掩码，网关等概念。读者可以参考这篇知乎文章。]]></content>
      <categories>
        <category>网络安全</category>
      </categories>
      <tags>
        <tag>网络协议</tag>
        <tag>ip地址</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产问题排查指南——排查思路]]></title>
    <url>%2F2018%2F02%2F12%2F%E7%94%9F%E4%BA%A7%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E6%8C%87%E5%8D%97%E2%80%94%E2%80%94%E6%8E%92%E6%9F%A5%E6%80%9D%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[业务指标着手需要提出的是，大部分情况还是业务应用本身引发的问题，所以应从业务指标着手。 查看gc日志 查看gc是否变得频繁，是否出现大量gc甚至full gc，这意味着内存可能发生了问题。此时应保留现场，保存内存dump。 一般应用程序中都有配置gc日志，在/usr/local/logs/gc目录下 需要特别注意，个别major gc可能是正常内存回收，并且gc后的dump也不再体现回收前的内存情况，所以不具有参考性。 一般使用MAT对内存dump进行分析，找出异常的对象，从而判断代码或者业务设计是否有需要修改的地方。 查看业务日志如果gc正常，吞吐量下降的原因可能有两个。一,外部依赖阻塞，二,服务内部资源紧张，请求排队等待。以上两种情况都可能产生蝴蝶效应，服务性能下降，请求排队的同时，内存可能堆积大量对象无法释放，这样就导致出现gc甚至full gc。主要关注两类服务，一,错误量最多的服务，二,第一个耗时异常缓慢的服务，根据错误链路找到异常的触发源。最好的做法就是预防入手，对于应用性能有准确的评估以及可能遭遇的流量高峰有所预测，这样及早做资源扩展。 日志查看手册读者可以查看xqy撰写的生产环境日志指导手册。]]></content>
      <categories>
        <category>生产环境</category>
      </categories>
      <tags>
        <tag>问题排查</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产问题排查指南——问题排查方法]]></title>
    <url>%2F2018%2F02%2F12%2F%E7%94%9F%E4%BA%A7%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E6%8C%87%E5%8D%97%E2%80%94%E2%80%94%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[问题排查方法这篇博客主要阐述排查的一些指标，对一些可能造成生产问题的注意点做指导概览。 指标分类基础指标基础指标主要有以下几点： cpu负载：cpu负载高的情况，将直接导致系统吞吐下降。 磁盘：磁盘I/O是否正常，磁盘硬件是否正常。 网络：出网入网流量是否正常，网络是否有波动情况。 中间件负载：中间件负载情况将直接影响调用链路性能。 如果能确保基础指标没有问题，这将大大加快我们排查的效率。例如，当前一段时间内生产网络出现了大幅度的波动，造成了大量服务延迟。如果你没有预先排除网络指标是否正常，而直接排查应用的服务性能问题，那最终将浪费大量的时间和精力。 核心指标线程1234线程数量：多线程并行运行的情况下，存在大量线程切换与上下文信息的同步，cpu主要负责这个。因此线程数量过多，将会直接增加切换与同步的开销，cpu负载增加。top1：无论进程还是线程，都应该 关注cpu占用率最高的那一个，有利于快速排查系统高负载的主要原因。 内存1234567系统内存空闲情况：长时间的低空闲内存，容易在大流量到来时，达到系统警戒阈值，触发OOM Killer，中断应用进程。在其他指标不变的情况下，空闲内存的大小与数据，流量息息相关。大数据与流量高峰都会加大内存开销，遇到服务阻塞，对象会在内存中堆积无法释放，最终导致内存溢出。业务应用内存情况：业务应用会有一块被分配的固定大小的堆内存空间。但配分的大小不一定总是合理的，一个典型的例子是老年代分配内存过大，新生代分配较小，大量的对象在老年代中堆积等待回收，在触发回收前，系统长时间处于空闲内存吃紧状态。 业务指标gc情况直观体现了应用内存当前的健康程度，频繁gc甚至full gc意味着应用内存出现了问题或者没有给应用分配合理的内存空间。 任何时候，日志都是问题排查不可或缺的重要指标。核心指标的异常，最终体现在日志上就是大量的系统或者业务异常，服务错误量激增[调用失败，超时]，执行耗时异常缓慢[一次服务耗时10～100s以上] 扩展读者可能对于OOM Killer这个概念不太了解，可以参考知乎上的这篇形象比喻。]]></content>
      <categories>
        <category>生产环境</category>
      </categories>
      <tags>
        <tag>问题排查</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生产问题排查指南——指标查询指令与方法]]></title>
    <url>%2F2018%2F02%2F12%2F%E7%94%9F%E4%BA%A7%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E6%8C%87%E5%8D%97%E2%80%94%E2%80%94%E6%8C%87%E6%A0%87%E6%9F%A5%E8%AF%A2%E6%8C%87%E4%BB%A4%E4%B8%8E%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[生产问题指标cpu中央处理器cpu占用率与负载情况是操作系统繁忙程度的一项总要的指标，它直接影响着应用的吞吐。top命令经常用来监控linux系统状况，也直观展示了系统cpu使用情况。读者可以参考微服务问题定位——top来查看具体的细节。 线程上面的top命令同时展示了各进程cpu使用情况，一般cpu占用量自上而下排序。我们一般关注cpu占用率最高的那个，大部分情况下它就是我们的应用进程。pid列即为进程的id。 $top -Hp pid[十进制进程id] 可以查看指定进程id中各线程的cpu使用情况。 $printf ‘0x%x\n’ nid[十进制线程id] 将线程的id打印为十六进制，目的是给之后的命令提供十六进制的id。 $jstack pid[进程id] | grep ‘nid[十六进制线程id]’ -C5 –colorjstack可以查询jvm进程中的线程栈信息，grep可以从中搜索目标线程信息，可以配合源码分析负载高的原因。 $jstack pid[进程id] &gt; jstack.txt[文件] 当然你可以将当前进程栈信息输出到文件，即我们常说的线程dump，然后进行详细分析。 strace -p nid[线程id] -T strace可以查看操作系统底层的执行情况，包括执行函数和执行耗时等。 内存这里的内存指标分为系统内存和应用内存一般读者可以使用free命令查看系统内存的使用情况，读者可以参考微服务问题定位——free应用内存的查看需要jdk包下的命令： $jmap -heap pid[进程id]jmap可以查看jvm进程的内存分配与使用情况，使用的gc算法等信息。 $jmap -dump:format=b,file=[导出路径] pid[进程id]-dump:format=b,file=可以使用hprof二进制形式输出jvm的heap内容到文件，即我们常说的堆内存dump，然后可以结合MAT[内存分析工具]可以深入分析内存使用情况。注意dump是比较消耗资源的。如果现在系统的内存比较吃紧，磁盘i/o较慢，切忌手动dump，可能成为压死骆驼的最后一根稻草。 -XX:+HeapDumpOnOutOfMemoryError 一般我们会在jvm的启动中添加启动参数，这样发生OOM后jvm能够自动将当时的内存情况dump保留下来。 zip或者gzip 通常dump文件会较大，应该将原dump文件归档到备份地点或者直接移除，以释放这部分磁盘空间占用。在进行下载时也将极大减少带宽开销。 磁盘读者可以参考： 微服务问题定位——df 微服务问题定位——du 网络 dstat 是一个综合的多维度指标命令。视图中有cpu使用情况，磁盘读写，网络情况，分页统计，系统中断与上下文切换统计。这里我们主要关注网络指标net/total- 。recv：入网流量，send：出网流量。当网络传输流量长时间小于实际应用传输数据量大小时，带宽将成为应用性能瓶颈。 ifstat 读者可以参考微服务问题定位——ifstat 日志这里的日志指标分为gc日志和业务日志。 gc日志无论minor gc还是major gc，我们主要关注gc回收实际消耗时间，即为日志中的real，即STW[stop the world]时间，如果实际耗时过长，则严重影响应用性能。 业务日志业务日志一般我们关注调用时间过长或者调用失败的请求，grep ‘[0-9]{3,}ms’ *.log 查看执行耗时3位数以上的服务grep ‘,N’ *.log 查看执行失败的服务]]></content>
      <categories>
        <category>生产环境</category>
      </categories>
      <tags>
        <tag>问题排查</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在mac上搭建python环境并使用docx模块]]></title>
    <url>%2F2018%2F02%2F12%2F%E5%A6%82%E4%BD%95%E5%9C%A8mac%E4%B8%8A%E6%90%AD%E5%BB%BApython%E7%8E%AF%E5%A2%83%E5%B9%B6%E4%BD%BF%E7%94%A8docx%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[背景之前接手到一个处理word文档的项目，需要读取word内容，并对读取的字符串进行一系列的处理。一开始笔者使用java进行处理，需要引入第三方模块——poi。说实话，这个第三方模块对于word的处理不够完善，第一没有很好的写入方式，第二没有很好的样式处理机制，第三没有封装java的io流，仍然需要笔者手动创建和关闭io资源。 在之后的资料中查询到可以使用python的docx模块很快速处理word文件，使用快捷方便，几行代码就可以实现读写，并且很好的支持word的样式。但是docx模块只支持docx后缀的文档，读者可以先将doc转换docx。 搭建环境搭建平台是macOS10.12.6，一般mac上是自带python的，但是普遍版本较低，目录位置为/Library/Python，以后安装的其他版本也在相同的目录下。 终端$python –version 查看python的版本。 这边可以使用pyenv来管理python的各个版本，笔者这边使用IDE(python编译环境)管理的。如果选择pyenv，请参考。即使不使用pyenv，笔者还是要推荐参考博客中提及的brew——mac的统一依赖包管理器。如果选择IDE管理，请到官网下载PyCharm，这种方式不像pyenv可以使用命令下载python的其他版本，读者可以访问官网下载其他python版本。 IDE在栏目PyCharm下选择Pereferences，如图选择可以切换编译器的python版本：不同的版本切换对代码的编译可能会造成影响。 引入docx模块python处理word也需要引入第三方模块，一般使用pip去下载python的第三方模块。mac里面python自带easy_install。$sudo easy_install pip 输入密码下载pip，笔者强烈建议不要在系统自带的python下折腾，因为mac系统下很多软件都依赖python模块，一些操作可能导致系统的软件打开错误。 读者可以先去官网下载其他版本，然后设置环境变量，sudo vim /etc/profile，编辑添加export PATH=xxxx:$PATH，其中xxxx为下载后python的路径，参考搭建环境。 $python –version 查看版本是否更改，之后执行$sudo easy_install pip，然后在执行$pip install docx模块。 然后读者就可以在当前python版本的目录下看到pip模块和docx模块了。之后在PyCharm中切换python的版本，在编写的代码中引入docx模块即可。读者可以参考官网手册]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jvm——新生代,老年代和持久代]]></title>
    <url>%2F2018%2F02%2F11%2Fjvm%E2%80%94%E2%80%94%E6%96%B0%E7%94%9F%E4%BB%A3-%E8%80%81%E5%B9%B4%E4%BB%A3%E5%92%8C%E6%8C%81%E4%B9%85%E4%BB%A3%2F</url>
    <content type="text"><![CDATA[新生代，老年代和永久代java中堆是jvm所管理的最大的一块内存空间，主要用于存放各种类的实例对象。jvm中内存的分配有如下公式： 堆 = 年轻代 + 老年代 年轻代 = eden space(新生代) + from survivor + to survivor 年轻代默认值保持为堆大小的1/15，特点是对象更新速度快，在短时间内产生大量的死亡对象，并且要产生连续可用的空间。所以使用复制清楚算法和并行收集器进行垃圾回收，对年轻代的垃圾回收称作初级回收(minor gc)。 年轻代的工作机制jvm，年轻代中每次只会使用eden space和其中一块survivor区域来为程序服务，所以无论如何总有一块survivor区域总是空闲的。对象在 eden 中初始化，在经过一次minor gc后，如果对象还存活着，即被引用着，并且能够被另外一块survivor区域所容纳，则使用复制算法将这些仍然还存活的对象复制到另外一块survivor区域中。 年轻代如何变成老年代初始化过程与上面一致，在eden中。在minor gc之后，如果对象还存活着，这些对象的年龄+1，当超过某个值(默认为15)这些对象进入老年代。 老年代的gc堆内存的老年代不同于现实生活，老年代中的对象个个都是从survivor中熬过来的，所以老年代中的类不是那么容易死亡的。因此，full gc(又称major gc)发生的次数没有minor gc那么频繁，并且一次full gc要比minor gc时间要更长。标记-清除算法收集垃圾的时候会产生许多的内存碎片 ( 即不连续的内存空间 )，此后需要为较大的对象分配内存空间时，若无法找到足够的连续的内存空间，就会提前触发一次 GC 的收集动作。 持久代此外还有一个持久代，用于存放静态文件，如java类定义(相当于模版，不是实例对象)、方法、本地方法等。持久代对垃圾回收没有显著影响，但是有些应用可能动态生成或者调用一些class，例如hibernate等，在这种时候需要设置较大的持久代空间来存放这些运行过程中新增的类 。持久代大小通过-XX:MaxPermSize=进行设置。 补充有的虚拟机并没有持久代，java8开始持久层也已经被彻底删除了，取代它的是另一个内存区域也被称为元空间。想要参考更多，读者可以参考这篇博客 jvm配置项jvm配置项可以根据程序的实际要求来配置，如各个代的比例等。这边原本应该用表格的形式展示，但是hexo在渲染markdown的表格语法时不支持，待到作者修复这个bug后进行更改。1234567891011121314151617181920212223242526* -Xms 初始堆大小。如：-Xms256m* -Xmx 最大堆大小。如：-Xmx512m * -Xmn 新生代大小。通常为 Xmx 的 1/3 或 1/4。新生代 = Eden + 2 个 Survivor 空间。实际可用空间为 = Eden + 1 个 Survivor，即 90% * -Xss JDK1.5+ 每个线程堆栈大小为 1M，一般来说如果栈不是很深的 话， 1M 是绝对够用了的。 * -XX:NewRatio 新生代与老年代的比例，如 –XX:NewRatio=2， 则新生代占整个堆空间的1/3，老年代占2/3 * -XX:SurvivorRatio 新生代中 Eden 与 Survivor 的比值。默认值为 8。 即 Eden 占新生代空间的 8/10，另外两个 Survivor 各占 1/10 * -XX:PermSize 永久代(方法区)的初始大小 * -XX:MaxPermSize 永久代(方法区)的最大值 * -XX:+PrintGCDetails 打印 GC 信息 * -XX:+HeapDumpOnOutOfMemoryError 让虚拟机在发生内存溢出时 Dump 出当前的内 存堆转储快照，以便分析用]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux编辑器——vim]]></title>
    <url>%2F2018%2F01%2F31%2Flinux%E7%BC%96%E8%BE%91%E5%99%A8%E2%80%94%E2%80%94vim%2F</url>
    <content type="text"><![CDATA[VIM编辑器背景vi在mac下严格意义上讲不是一个简单的命令，而是unix系列下的自带的编译器，笔者没记错发明这编译器的家伙是个大胡子，一直致力于软件开源的维护和倡导$vi filename 这是开始编译一个文件，有时候需要switch user 即sudo 简介这边笔者不做过多的介绍请看维基百科这里的vim编辑器是vi的一种升级版 编辑文件之前说过基本用法:$vim filename，这边还有其他的用法$vim +line filename 编辑文件，并定位到第line行$vim + filename 编辑文件，并定位到最后一行$vim +/pattern filename 编辑文件，并定位到第一个匹配的位置 关闭文件这边需要提及vim的模式(之后补充，这边提及)，打开文件后需要进入输入模式或者其他模式，才能使用一些命令，退出模式后才能关闭文件一般按esc按钮来推出vim的模式，然后输入:来选择退出的方式:q 退出:wq 保存并退出:q! 不保存并退出:w 保存但没有退出，可以继续编辑:w! 强行保存退出后就回到命令窗口界面了，当然也可以直接编辑模式退出如：编辑模式按ZZ即可 模式i 在当前光标所在的字符前面转为输入模式a 在当前光标所在的字符后面转为输入模式o 在当前光标所在行的下方新建一行，并转为输入模式I 在当前光标所在行的行首转为输入模式A 在当前光标所在的行尾转为输入模式O 在当前光标所在的上方新建一行，并转为输入模式以上的命令必须是非编辑状态或者非其他的状态，然后按下相应的按键进入编辑模式 相关命令移动很简单，就是键盘上的上下左右键w 移至下一个单词词首，中文一般以标点为准跳动e 移至当前或下一个单词词尾b 移至当前或者前一个单词词首nw 移动n个单词 跳转0 跳转到光标所在的行首^ 跳转到行首的第一个非空白字符$ 绝对的行尾nG 跳转到第#行gg 跳转到第一行行首G 跳转到最后一行行首 翻页ctrl+f 向下翻一屏ctrl+b 向上翻一屏ctrl+d 向下翻半屏ctrl+u 向上翻半屏 删除x 删除光标所在处的单个字符nx 删除光标所在处及后面共n个单词dd 删除光标所在的行ndd 删除光标所在行及后面共n行 粘贴p: 如果删除或复制为整行内容，则粘贴至光标所在行的下方，如果复制或删除的内容为非整行，则粘贴至光标所在字符的后面P: 如果删除或复制为整行内容，则粘贴至光标所在行的上方，如果复制或删除的内容为非整行，则粘贴至光标所在字符的前面以上命令只作用于vim编辑器内部，外面复制的内容不行 复制用法和d命令的用法相同，将d替换成y 替换r 替换单个字符，但是好像不支持中文nr 光标后n个字符全部替换R 进入替换模式，可直接替换光标所在的字符 撤销编辑操作u 在非模式状态下，撤销前一次的编辑操作nu 直接撤销最近的n次编辑操作 选取v 进入选取模式，按字符选取，最多到光标所在的行V 进入选取模式，但是按矩形进行选取 查找:/pattern 根据匹配的字符查找:?pattern 同上n 下一个N 上一个 查找并替换headline,footline s#PATTERN#string#gn,$s#wyc#王鋆昌#g 替换第n行开始到最后一行中每一行所有wyc为王鋆昌参考 编辑多个文件vim file1 file2:next 切换到下一个文件:prev 切换到上一个文件:last 切换到最后一个文件:first 切换到第一个文件:q 退出当前文件:qa 全部退出 未完待续vim的使用远非及此，之后还有有涉及会及时补充]]></content>
      <categories>
        <category>vim</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令——crontab]]></title>
    <url>%2F2018%2F01%2F31%2Flinux%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94crontab%2F</url>
    <content type="text"><![CDATA[crontab作用自定义定时任务笔者一般用这个命令来提醒事项，适用于基本每天执行的事项 option参数-e 编辑cron脚本文件，可以指定编辑器，默认是vi编辑器-l 列出当前用户下的cron任务-u 指定cron在哪个用户下执行，默认是当前用户 使用实例$brontab -e 执行后跳到编辑器你可以在编辑器中书写cron命令，一般的格式是 command content上面格式中的 从左至右分别表示分，时，日，月份和年份，content就是自定义的命令了如果命令较多，你甚至可以写在一个shell脚本中，执行脚本即可比如：15 9 * command sh xxx.sh 每天早上9:15执行xxx.sh脚本 扩展一般用$cron -e编辑的脚本，笔者也不知道保存到哪里去了，一般笔者都是写完之后直接执行的比如：crontab test.cron即可，test.cron即为编写好的脚本，内容形式与上面的一致 实用价值了解敏捷开发的读者应该知道，部门中可能存在一些管理方式，需要每天执行任务，比如早晨的定时晨会，比如记录一天内的工作时间，等等读者可以使用outlook来提醒，或者手机的定时闹钟，笔者选择用crontab命令来显示高逼格列出了笔者mac中每天执行的任务，将多个cron任务编辑在同一个.cron文件内，执行就能实现多个任务的并存发现cron任务content可以直接命令，也可以shell脚本执行]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令——awk]]></title>
    <url>%2F2018%2F01%2F31%2Flinux%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94awk%2F</url>
    <content type="text"><![CDATA[awk用法awk ‘{pattern + action}’ {filenames} 使用方式awk -F 分隔符 ‘{command操作}’awk -f awk-script-file input-file(s)上面的命令比较有意思，awk-script-file是脚本文件，一般比较复杂的awk命令可以写成一个脚本，然后操作到input-file(s)文件上 内置变量ARGC 命令行参数个数ARGV 命令行参数排列ENVIRON 支持队列中系统环境变量的使用FILENAME awk浏览的文件名FNR 浏览文件的记录数FS 设置输入域分隔符，等价于命令行 -F选项NF 浏览记录的域的个数NR 已读的记录数OFS 输出域分隔符ORS 输出记录分隔符RS 控制记录分隔符 使用实例在xqy的日志使用手册中，有统计所有抛出非业务异常的服务的命令，相信很多开发不知道运行的原理，在此我做一下说明：cat common-service-digest.log | grep ‘N,’ | awk -F ( ‘{print $2}’ | awk -F ) ‘{print $1}’ | awk -F , ‘{!a[$1”.”$2”.”$3]++;}END {for (j in a) print j,a[j]}’这个命令比较长，你可以分开进行执行，比如:1.cat common-service-digest.log | grep ‘N,’ 查看调用失败的日志2.awk -F ( ‘{print $2}’ 以 （作为分隔符分割日志行，并打印分隔片区2比如:日志记录:[(tax,ICustomerSettingManageService,getAreaCodeBatch,Y,12ms)] (traceId=84ec429fbff64c5bbc43347c8cc72257)被分隔为:[(tax,ICustomerSettingManageService,getAreaCodeBatch,Y,12ms)](traceId=84ec429fbff64c5bbc43347c8cc72257)最后输出:tax,ICustomerSettingManageService,getAreaCodeBatch,Y,24ms)]3.awk -F ) ‘{print $1}’ 以 ）作为分隔符分割日志行，并打印分隔片区1最后输出:tax,ICustomerSettingManageService,getAreaCodeBatch,Y,12ms4.awk -F , ‘{!a[$1”.”$2”.”$3]++;} 以 ,作为分隔符分割日志行，并以$1”.”$2”.”$3为键统计5.最后end进行循环输出参考 说明当中涉及到xqy的日志业务和图片，所以不部署到github]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令——sed]]></title>
    <url>%2F2018%2F01%2F31%2Flinux%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94sed%2F</url>
    <content type="text"><![CDATA[sed系统mac系统用的是原生的bsd系列，而一般的linux系统用的是gnu系统，所以两者在某些命令是有区别的，sed命令位列其中 区别mac上sed的添加和插入文本比较奇葩，需要如此:$sed “$line a\ (在\后要加一个空格，然后另起一行，再写需要添加的那一行新的)>$value（需要添加新一行的内容）>“ $filename(文件名)同上，sed i的使用方式亦然 gnu-sed如果无法适应原生bsd系列，可以下载gnu-sed，这个时候brew神器又用到了，直接下载然后配置环境变量即可:$brew install gnu-sed之后的叙述都是基于gnu-sed说明 option参数-n ：使用安静(silent)模式，在一般 sed 的用法中，所有来自 STDIN 的数据一般都会被列出到终端上，但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来-e ：直接在命令列模式上进行 sed 的动作编辑-f ：直接将 sed 的动作写在一个文件内， -f filename 则可以运行 filename 内的 sed 动作-r ：sed 的动作支持的是延伸型正规表示法的语法。(默认是基础正规表示法语法)-i ：直接修改读取的文件内容，而不是输出到终端 方法a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)p ：列印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行s ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法 实例$sed ‘2a 字符串’ filename —— 第二行下一行添加字符串$sed ‘2i 字符串’ filename —— 第二行上一行添加字符串以上的命令只能作用在缓存的文件中，其实真实的文件内容并没有被改变$sed ‘2a 字符串’ -i filename —— 这样可以直接修改文件了$sed ‘1,2d’ -i filename —— 删除文件1-2行的内容sed与nl联合使用可以进行关键字查找:$nl filename | sed -n ‘/关键字/p’sed支持正则表达的搜索和替换:$sed ‘s/正则表达式 or 普通字符串/新字符串/g’ -i filename[其他用法:]https://www.cnblogs.com/ggjucheng/archive/2013/01/13/2856901.html]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令——补充]]></title>
    <url>%2F2018%2F01%2F31%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E2%80%94%E2%80%94%E8%A1%A5%E5%85%85%2F</url>
    <content type="text"><![CDATA[常用命令(补充)grep用法grep [-acinv][–color=auto] ‘查找字符串’ filename option参数-c 统计查找字符在文件中出现的次数-i 忽略大小写-v 反向查找，即显示不在查找内的行，但是根据记录日志的习惯，很少会使用–color=auto 对查找的字符进行颜色显示 联合使用ls -l |grep -i filename 列出指定文件/文件夹的属性ps -ef|grep tomcat/java 列出指定的进程信息 find用法find [PATH] [option] [action] option参数-mtime n 其中n为数字，表示n天以前，查找n天之前修改过的文件-user name 查找指定所有者的文件-size [+-size] 查重比size大/小的文件 使用$find . -mtime 0 来查找当天修改过的文件，一般可以筛选当天有记录的日志 taroption参数-c ：新建打包文件-t ：查看打包文件的内容含有哪些文件名-x ：解打包或解压缩的功能，可以搭配-C（大写）指定解压的目录，注意-c,-t,-x不能同时出现在同一条命令中-j ：通过bzip2的支持进行压缩/解压缩-z ：通过gzip的支持进行压缩/解压缩-v ：在压缩/解压缩过程中，将正在处理的文件名显示出来-f filename ：filename为要处理的文件-C dir ：指定压缩/解压缩的目录dir 使用实例$tar -tzvf xxx.zip 查看指定压缩文件里面的文件内容$tar -zxvf xxx.zip 解压缩指定文件$tar -cvf xxx.tar dir 压缩指定文件夹$tar -czvf xxx.tar.gz dir 以gzip的格式进行压缩如何使用解压缩命令的option操作，最终还是要看压缩文件的格式的，如-z一般用来支持.gz结尾的压缩包 kill用法kill [选项] [参数] option参数-a：当处理当前进程时，不限制命令名和进程号的对应关系-l &lt;信息编号&gt;：若不加&lt;信息编号&gt;选项，则-l参数会列出全部的信息名称 -p：指定kill 命令只打印相关进程的进程号，而不发送任何信号-s &lt;信息名称或编号&gt;：指定要送出的信息-u：指定用户 常用的信号编号HUP 1 终端断线 INT 2 中断（同 Ctrl + C） QUIT 3 退出（同 Ctrl + \） TERM 15 终止 KILL 9 强制终止 CONT 18 继续（与STOP相反， fg/bg命令） STOP 19 暂停（同 Ctrl + Z）在mac中18是STOP，19是CONT]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令文件上传与下载]]></title>
    <url>%2F2018%2F01%2F31%2Flinux%E5%91%BD%E4%BB%A4%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E4%B8%8E%E4%B8%8B%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[文件上传下载搭建实践环境这边使用笔者自己的电脑和windows电脑进行实践。 利用ssh打通windows和mac打开mac的系统偏好设置中的共享，一般情况下没有界面展示的，需要使用搜索查找共享， 然后勾选远程登陆，选择全部用户可访问 在windows上使用ssh连接mac，填写相应的用户名和密码即可 安装要支持sz，rz两个命令，系统需要安装lrzsz，一般的linux系统是自带的，这里使用mac进行实验，需要安装。参考 使用sz filename，即从系统上下载文件到本地，filename是系统中你选择下载的文件rz -b，将本地的文件以二进制的传递方式上传到系统服务器，一般推荐使用二进制传输方式其他参考]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo防止敏感数据部署]]></title>
    <url>%2F2018%2F01%2F30%2Fhexo%E9%98%B2%E6%AD%A2%E6%95%8F%E6%84%9F%E6%95%B0%E6%8D%AE%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[hexo防止敏感数据部署原理熟悉hexo的读者都知道，hexo的部署一般执行如下:$hexo clean$hexo g$hexo d如果不按照顺序执行这三步，读者会发现即使增添了page，部署时git也不会显示增添文件的记录，github上也没有提交的代码记录其实这涉及到hexo的部署原理，hexo clean 会清除之前构建的代码，具体就是public文件夹；hexo g 会重新生成，这样读者添加的page也就构建在其中，此时新的public文件夹会生成；hexo d 就是将hexo g 改变的文件提交到github上所以不执行clean和generate是不会真正意义上修改remote上的代码 敏感数据敏感数据这里指的就是涉及公司业务的文章，因为hexo d 会将文章直接部署到github上，这样就全部公开化了这边最好是本地可以访问，github无法访问你的敏感数据，这篇文章的目的就在此 利用.gitignore熟悉git的读者，应该都知道可以使用.gitignore文件忽略一些提交网上有些说修改.npmignore文件配置(hexo的根目录下)可以起到效果，经过测试发现没有起到作用，所以转而到git寻求解决方案一般执行如下(当前目录为hexo根目录):$cd .deploy_git git push的代码都在此文件夹下$vim .gitignore增加如下配置:page name/ page的名称，hexo n page创建命令中你定义的文章名，如:linux命令性能监控及优化/.gitignore 自己本身但是笔者发现代码虽然没有提交到github上，但是hexo博客上有这个新建的标题，点击访问也是github默认的404页面(因为hexo g过程中一定会将文章标题写入hexo中，.gitignore只能做到忽略提交) 自定义404上面的问题引入了这个小结，github默认的404页面会让读者误以为是网站的问题，其实是不想公开化，所以你可以自定义404页面具体的步骤如下(hexo的根目录下):$hexo n page 404 source目录下生成404目录index.md中增加配置: layout: false //是否使用布局文件 comments: false //是否有评论 permalink: /404 //设置链接然后按照hexo部署步骤上传到github上，然后再访问敏感文章链接会跳转到自定义的404页面其实根本问题是github上没有代码，但是hexo在generate时将目录还是写入了导致，但是笔者找不到解决忽略写入的方法]]></content>
      <categories>
        <category>hexo部署</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令性能监控及优化]]></title>
    <url>%2F2018%2F01%2F30%2Flinux%E5%91%BD%E4%BB%A4%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7%E5%8F%8A%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[性能监控和优化toptop freefree vmstat用来统计虚拟内存的使用情况，这边涉及到linux物理内存和虚拟内存的知识点，其实之前我们也提到过，虚拟内存即交换区 option参数-a：显示活跃和非活跃内存-f：显示从系统启动至今的fork数量-m：显示slabinfo-n：只在开始时显示一次各字段名称-s：显示内存相关统计信息及多种系统活动数量delay：刷新时间间隔。如果不指定，只显示一条结果count：刷新次数。如果不指定刷新次数，但指定了刷新时间间隔，这时刷新次数为无穷-d：显示磁盘相关统计信息-p：显示指定磁盘分区统计信息-S：使用指定单位显示。参数有 k 、K 、m 、M ，分别代表1000、1024、1000000、1048576字节（byte）默认单位为K（1024 bytes）-V：显示vmstat版本信息 使用实例$vmstat 5 6 在docker上进行试验，在6秒内采集5次数据 说明Procs（进程）：r: 运行队列中进程数量b: 等待IO的进程数量Memory（内存）：swpd: 使用虚拟内存大小free: 可用内存大小buff: 用作缓冲的内存大小cache: 用作缓存的内存大小Swap：si: 每秒从交换区写到内存的大小so: 每秒写入交换区的内存大小IO：（现在的Linux版本块的大小为1024bytes）bi: 每秒读取的块数bo: 每秒写入的块数系统：in: 每秒中断数，包括时钟中断cs: 每秒上下文切换数CPU（以百分比表示）：us: 用户进程执行时间(user time)sy: 系统进程执行时间(system time)id: 空闲时间(包括IO等待时间),中央处理器的空闲时间 以百分比表示wa: 等待IO时间 需要关注的指标如果 r经常大于 4 ，且id经常少于40，表示cpu的负荷很重如果bi，bo 长期不等于0，表示内存不足如果disk 经常不等于0， 且在 b中的队列 大于3， 表示 io性能不好]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux常用命令]]></title>
    <url>%2F2018%2F01%2F30%2Flinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[常用命令pwd没什么可说的，当前不知道处于那个目录下时，pwd即可 mvmv命令是将一个文件移动到指定的目录，但是在linux可以用来重命名文件 moremore是具有翻页功能的cat命令 lessless是一个可以前后分页浏览文件的命令，相比more更加具有弹性 whichwhich用于查找命令包所在的路径，没有配置在环境变量中的命令是搜索不到的，因为which只会搜索path下的命令，echo $PATH可以查看 lsls命令还是很常用的，不做过多描述ls g* 模糊列出文件列表，即将g开头的文件列出 unzip和gzipunzip主要用于解压.zip压缩文件 unzip xxx.zipgzip file1 file2 … 文件打包压缩为.gz的形式对于文件夹的打包或者其他的压缩格式请使用tar命令读者可以参考这篇博客 whatis和man$whatis command 用于显示命令使用的描述$man command 以使用手册的形式显示]]></content>
      <categories>
        <category>linux命令</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务问题定位——jmap]]></title>
    <url>%2F2018%2F01%2F29%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D%E2%80%94%E2%80%94jmap%2F</url>
    <content type="text"><![CDATA[jmap使用场景 内存溢出，线上程序运行时内存越来越大，用jmap dump出堆内存的信息，然后进行相关分析 内存的真实使用大于预期，这是因为设计不合理导致冗余的对象存在内存中，用jmap来查看内存中的对象，分析是否有存在的必要 jvm优化，利用jmap来查看整个堆的使用情况，根据老年代和新生代的使用比例来划分jvm的各个区域 使用实例在使用jmap连接进程时，启动的jvm版本和jdk版本要一一对应，毕竟jmap是jdk下的命令包$ps -ef | grep java 查找系统的java进程$jmap -heap pid 打印指定进程堆的摘要信息，包括gc算法 参数说明 Heap Configuration: 堆配置信息 MinHeapFreeRatio 在堆的使用率小于MinHeapFreeRatio(%)的时候进行收缩，当Xmx=Xms的时候此配置无效MaxHeapFreeRatio 在堆使用率大于MaxHeapFreeRatio(%)的时候进行扩展，当Xmx=Xms的时候此配置无效MaxHeapSize 堆的最大空间NewSize 新生代的大小MaxNewSize 最大的新生代的大小OldSize 老年代的大小NewRatio 老年代和新生代的比例SurvivorRatio 新生代中Eden和和Survivor区的比例PermSize 永久代的大小MaxPermSize 久代的最大内存G1HeapRegionSize 使用G1垃圾收集的区间 Heap Usage: 堆的使用信息 New Generation (Eden + 1 Survivor Space): 新生代的大小（Eden区加一个Survivor区的空间信息 capacity 总内存used 已使用内存free 剩余内存13.823827124993642% used 使用内存占比 Eden Space: Eden区的大小 capacityusedfree15.178337946947952% used From Space: 第一个Surivivor区的空间信息 capacityusedfree2.984432830624237% used To Space: 第二个Survivor区的空间信息 capacityusedfree0.0% used concurrent mark-sweep generation: CMS垃圾收集占用的空间信息 capacityusedfree75.36470666527748% used Perm Generation: 永久代的空间信息 capacityusedfree47.5915253162384% used 其他功能参考链接]]></content>
      <categories>
        <category>微服务问题定位</category>
      </categories>
      <tags>
        <tag>jvm</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务问题定位——jstack]]></title>
    <url>%2F2018%2F01%2F29%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D%E2%80%94%E2%80%94jstack%2F</url>
    <content type="text"><![CDATA[jstack功能查看运用程序jvm的堆栈情况，可以找出线程的运行情况，从而排查一些隐患或者服务卡顿的问题 具体说明jstack用于打印出给定的java进程id或者core file或者远程调试服务的java堆栈信息，如果是在64位机器上，需要指定选项-J-d64如果java程序崩溃会生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松知道java程序是如何崩溃和在程序何处发生问题。 option参数-F 当jstack -l pid 没有响应的时候强制打印栈信息-l 长队列，打印关于锁的附加信息-m 打印java和native c/c++框架的所有栈信息 命令格式jstack [option] pidjstack [option] executable corejstack [option] [server-id@]remote-hostname-or-IP 命令格式说明executable core 产生core dump的java可执行文件remote-hostname-or-IP 远程debug服务的主机名或ipserver-id 唯一id,假如一台主机上多个远程debug服务 使用实例一般需要与top命令联用，使用top命令找出异常的进程(一般是cpu使用异常的进程)通过top -Hp pid来定位该进程下各线程的cpu使用情况再通过jstack pid命令打印该线程对应的堆栈情况 扩展 在top命令中，已经获取到了占用cpu资源较高的线程pid，将该pid转成16进制的值(在线转换)，在thread dump中每个线程都有一个nid，找到对应的nid即可 什么是java core和heap dump文件，参考]]></content>
      <categories>
        <category>微服务问题定位</category>
      </categories>
      <tags>
        <tag>jvm</tag>
        <tag>微服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gitbook目录折叠]]></title>
    <url>%2F2018%2F01%2F28%2Fgitbook%E7%9B%AE%E5%BD%95%E6%8A%98%E5%8F%A0%2F</url>
    <content type="text"><![CDATA[背景在编写gitbook的时候发现目录的数量比较庞大的时候显得杂乱无章，因为章节比较多的时候gitbook对于目录默认是全展开，很难有目的地寻找，对于管理人和阅读的读者来说很不友好。(主要是后者，一切都要从用户的角度考虑)如果可以将gitbook的目录折叠，那么查找时会更加清晰，笔者在网上找到了插件，只要让gitbook引入即可。 插件 插件名称：toggle-chapters 效果：默认只在目录导航中显示章的标题，而不会显示小节的标题，点击每一章或者每一节会显示当前章或节的子目录，如果有的话，但是同时会收起其它之前展开的章节。 关于更多的gitbook插件，读者可以参考插件网站。 配置在根目录(即与SUMMARY.md同级的目录)下的配置文件 book.json(如果没有则新建)中添加插件配置，如图：读者可以参考进行配置，同理你可以在里面添加需要使用的插件。配置完成后，可按照一下步骤进行： $ cd gitbook根目录 $ npm install gitbook-plugin-toggle-chapters (此时gitbook的根目录下的node_modules文件夹中已经有了该插件了) $ gitbook build $ gitbook serve访问 http://localhost:4000 看你的插件是否已经生效。]]></content>
      <categories>
        <category>gitbook</category>
      </categories>
      <tags>
        <tag>gitbook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在本地搭建gitbook]]></title>
    <url>%2F2018%2F01%2F28%2F%E5%A6%82%E4%BD%95%E5%9C%A8%E6%9C%AC%E5%9C%B0%E6%90%AD%E5%BB%BAgitbook%2F</url>
    <content type="text"><![CDATA[介绍gitbook更加适用于书写使用手册，不太适合书写一些博客和总结，所以笔者最近在做一些迁移的工作，将之前gitbook上的内容迁移到当前的博客上。但是gitbook对于我们的工作还是有适用的地方的。所以在此介绍一下如何本地安装gitbook。 先决条件安装nodejs是先决条件，因为我们需要npm去下载gitbook。在mac电脑上，有brew工具的可以直接下载：$brew search nodejs 搜索包仓库中是否有nodejs$brew install nodejs 有就可以直接下载你也可以选择去官网上下载最新的nodejs压缩包，解压配置环境变量后即可使用。关于brew，读者可以参考简书进行安装。 搭建这里默认大家的电脑上已经安装好了nodejs。按照步骤执行以下命令：$npm install gitbook-cli -g 下载gitbook客户端包$gitbook –version 查看gitbook的版本，测试是否可以使用命令$cd /users/Desktop 切换到一个目录，这里笔者切换到了桌面$mkdir gitbook 创建gitbook目录，这个目录就是用来初始化的其实上面的两步(切换目录和创建目录)读者可以手动完成。$gitbook init 初始化gitbook目录$gitbook build 构建，一般有内容更新时要构建$gitbook serve 开启服务，默认在4000端口监听访问http://localhost:4000 开启你的gitbook之路吧！ 问题你的系统 4000 端口可能已经被占用了，gitbook可以指定端口启动：gitbook serve –port 2000 另一个问题是笔者在windows上搭建时遇到的：笔者之前在windows上搭建过，出现了一些问题，build的时候总是报错。在github官方issues中，官方表示这个是已知的gitbook的bug，高版本的gitbook会出现这个bug。如果你遇到相同的问题，请参考gitbug上的解决方法。解决方法是点赞和喝彩最多的。]]></content>
      <categories>
        <category>gitbook</category>
      </categories>
      <tags>
        <tag>gitbook</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务问题定位——du]]></title>
    <url>%2F2018%2F01%2F28%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D%E2%80%94%E2%80%94du%2F</url>
    <content type="text"><![CDATA[功能du命令也是查看使用空间的，但是与df命令不同的是，du命令是对文件和目录磁盘使用的空间进行查看，而不是针对整个文件系统。 option 参数-a或-all 显示目录中个别文件的大小-b或-bytes 显示目录或文件大小时，以byte为单位-c或–total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和-k或–kilobytes 以KB(1024bytes)为单位输出-m或–megabytes 以MB为单位输出-s或–summarize 仅显示总计，只列出最后加总的值-h或–human-readable 以K，M，G为单位显示，提高信息的可读性-x或–one-file-xystem 以一开始处理时的文件系统为准，若遇上其它不同的文件系统目录则略过-L&lt;符号链接&gt;或–dereference&lt;符号链接&gt; 显示选项中所指定符号链接的源文件大小-S或–separate-dirs 显示个别目录的大小时，并不含其子目录的大小-X&lt;文件&gt;或–exclude-from=&lt;文件&gt; 在&lt;文件&gt;指定目录或文件–exclude=&lt;目录或文件&gt; 略过指定的目录或文件-D或–dereference-args 显示指定符号链接的源文件大小-H或–si 与-h参数相同，但是K，M，G是以1000为换算单位-l或–count-links 重复计算硬件链接的文件读者可以使用man du或者du -help查看完整的参数列表。 使用实例$du 显示当前目录下以及子目录中所有文件的大小，一般都是使用-a参数指定文件的，因为罗列所有的信息读者很难找到相关的。$du dir/filename 显示指定目录或者文件，dir/filename为系统中的路径。$du #1 #2 同时显示多个，空格隔开]]></content>
      <categories>
        <category>微服务问题定位</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
        <tag>微服务</tag>
        <tag>磁盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务问题定位——df]]></title>
    <url>%2F2018%2F01%2F28%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D%E2%80%94%E2%80%94df%2F</url>
    <content type="text"><![CDATA[功能用来检查服务器文件系统的磁盘空间占用情况，可以获取硬盘被占了多少空间，目前还剩下多少空间等信息。 option 参数-a 显示全部文件系统列表-h 以方便查阅的方式显示，即带有单位的形式-H 相当于-h，但是换算的方式变更，这里1k=1000-i 显示node信息-k 区块为1024字节，即1Kb为一个区块-l 只显示本地文件系统-m 区块为1048576字节，即1Mb为一个区块–no-sync 忽略sync命令-P 输出格式为POSIX–sync 在取得磁盘信息前先执行sync命令-T 文件系统类型 选择参数–block-size=&lt;区块大小&gt; 指定区块的大小-t &lt;文件系统类型&gt; 只显示选定文件系统的磁盘信息-x &lt;文件系统类型&gt; 不显示选定文件系统的磁盘信息 使用实例第一列 显示各文件系统第二列 显示文件系统的共有多少块，这里以512B为一块，换算后即可知道文件系统大小第三列 显示已经使用的磁盘大小第四列 显示可用的磁盘大小关于其他的使用方式，读者可以参考cnblogs了解。这边需要说明：已使用的+可使用的 != 总块数，因为缺省的每个分区都预留了少量空间供管理员使用，所以即使普通用户的空间已满，管理员依然可以登录系统解决相关问题。]]></content>
      <categories>
        <category>微服务问题定位</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
        <tag>微服务</tag>
        <tag>磁盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务问题定位——ifstat]]></title>
    <url>%2F2018%2F01%2F28%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D%E2%80%94%E2%80%94ifstat%2F</url>
    <content type="text"><![CDATA[ifstat功能监控系统的流量 安装ifstat不是系统自带的命令，所以需要进行安装。mac系统的读者可以在终端使用命令：$brew install ifstat进行安装。当然系统首先要安装brew(mac系统中的包下载工具)，读者可以参考简书进行安装。 option 参数-l 监测环路网络接口(lo)，缺省情况下不会显示，所谓的环路网络可认为是localhost(127.0.0.1)。读者可以参考知乎了解相关的解释。-a 监测系统所有的网络，比加上-l参数还多一个plip0的接口信息(所谓的并口)-z 隐藏流量是无的接口，排查问题时排除无用端口-i 指定要监测的接口,后面跟网络接口名-s 等于加-d snmp:[comm@][#]host[/nn]] 参数，通过SNMP查询一个远程主机-t 在每一行的开头加一个时间 戳,，告诉我们具体的时间-T 报告所有监测接口的全部带宽，和-i联用来指定端口-S 在同一行更新流量状态，不喜欢屏幕滚动的可以使用读者可以使用man ifstat或者ifstat -help查看完整的参数列表。更多详细的命令，读者可参考其他命令了解。 扩展如何只查看网卡的流量情况，ifstat足矣。详细的流量情况需使用iftop命令，系统依旧不会自带需要下载，mac下的下载方式与ifstat一致。读者可以参考实例了解用法。]]></content>
      <categories>
        <category>微服务问题定位</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
        <tag>微服务</tag>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务问题定位——free]]></title>
    <url>%2F2018%2F01%2F28%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D%E2%80%94%E2%80%94free%2F</url>
    <content type="text"><![CDATA[free功能 显示系统使用和空闲的内存情况，包括物理内存和交互区内存(swap)和内核缓冲区内存 option参数 -b 以Byte为单位显示内存使用情况 -k 以KB为单位显示内存使用情况 -m 以MB为单位显示内存使用情况 -g 以GB为单位显示内存使用情况 -o 不显示缓冲区调节列 -s&lt;间隔秒数&gt; 持续观察内存使用状况 -t 显示内存总和列 -V 显示版本信息 读者可以使用man free或者free -help查看完整参数列表。 样例 这边关注第二行的swap的used指标，如果使用的值较大，则表示系统的内存处于不够使用的情况 为何swap used是一个指标 swap又称为交换分区，当系统的内存小于额定值的时候，内核(OS)会将系统的一部分物理内存释放出来用于当前线程的使用，一般是很久没有操作过的程序会被释放，释放的物理内存被放入到交换区，然后等当前线程执行完毕，内存富余时重新放入物理内存。12345678910例如：物理内存，交换区，当前线程，进程x(目前存在于物理内存中)1.当前线程 向 物理内存 申请执行所需的内存空间2.物理内存 发现没有空间了 查找到 进程x 很久没有操作过3.物理内存 将 进程x 释放到 交换区4.物理内存 说 嘿 当前线程 你可以进来执行了5.当前线程 执行完毕 嘿 我执行完了 让那哥们进来吧6.物理内存 将 进程x 重新载入以上就是简略的过程。]]></content>
      <categories>
        <category>微服务问题定位</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
        <tag>微服务</tag>
        <tag>内存相关</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务问题定位——strace]]></title>
    <url>%2F2018%2F01%2F28%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%97%AE%E9%A2%98%E5%AE%9A%E4%BD%8D%E2%80%94%E2%80%94strace%2F</url>
    <content type="text"><![CDATA[功能 在linux世界，进程不能直接访问硬件设备，当进程需要访问硬件设备(比如读取磁盘文件，接收网络数据等等)时，必须由用户态模式切换至内核态模式，通过系统调用访问硬件设备。 strace可以跟踪到一个进程产生的系统调用，包括参数，返回值，执行消耗的时间。 option参数 -c 统计每一系统调用的所执行的时间,次数和出错的次数等 -d 输出strace关于标准错误的调试信息 -f 跟踪由fork调用所产生的子进程 -tt 在输出中的每一行前加上时间信息,微秒级， 时间格式：17:22:58.345879 -p pid 跟踪指定的进程pid 以上命令一般都是与-p联用 读者可以使用man strace或者strace -help查看完整的参数列表。 strace使用 strace的使用一般在top命令之后，top命令是用来查看占用cpu异常的进程的。读者可以参考微服务定位——top。 找到异常的进程后，使用命令：top -Hp pid[进程id]进入进程，找到执行异常的那个线程。使用命令：strace -p nid[线程id] -T来查看底层的调用情况。 在此贴出网上的一些参考链接 strace实例 strace实例2 样例贴图 笔者在此贴出strace命令显示的底层执行的信息： 1234567891011$strace cat /dev/nullexecve(&quot;/bin/cat&quot;, [&quot;cat&quot;, &quot;/dev/null&quot;], [/* 22 vars */]) = 0brk(0) = 0xab1000access(&quot;/etc/ld.so.nohwcap&quot;, F_OK) = -1 ENOENT (No such file or directory)mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f29379a7000access(&quot;/etc/ld.so.preload&quot;, R_OK) = -1 ENOENT (No such file or directory)...每一行都是一条系统调用，等号左边是系统调用的函数名及其参数，右边是该调用的返回值。strace 显示这些调用的参数并返回符号形式的值。strace 从内核接收信息，而且不需要以任何特殊的方式来构建内核。]]></content>
      <categories>
        <category>微服务问题定位</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
        <tag>微服务</tag>
        <tag>进程,线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务问题定位——top]]></title>
    <url>%2F2018%2F01%2F26%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E5%AE%9A%E4%BD%8D%E2%80%94%E2%80%94top%2F</url>
    <content type="text"><![CDATA[功能显示当前系统正在执行的进程的相关信息，包括进程ID、 内存占用率 、CPU占用率等 option 使用参数-b 进入批处理模式 相当于不停地执行top命令显示信息-c 显示完整的command 位于信息的最后一列-i &lt;时间&gt; 设置间隔时间 如-i 5设置5秒钟刷新一次top信息-u &lt;用户名&gt; 显示指定用户的信息 -p &lt;进程号&gt; 显示指定进程的信息 -n &lt;次数&gt; 循环显示的次数 读者可以使用man top或者top -help查看完整的参数列表。 例图 说明 第一行: 13:40:29 — 当前系统时间10 day，2:32 — 机器从开机到目前为止运行的时长20 users — 登陆的用户有20个load average — 分别对应1mins,5mins,15mins的负载情况(根据一定的算法得到的值，load average／cpu数量&gt;=5说明系统超负荷) 第二行 Tasks 系统进程数total — 当前系统进程269个running — 正在运行的1个sleeping — 睡眠的268个stopped — 停止的0个zombie — 僵尸进程0个 第三行 0.1% us — 用户空间占用CPU的百分比0.0% sy — 内核空间占用CPU的百分比0.0% ni — 改变过优先级的进程占用CPU的百分比99.9% id — 空闲CPU百分比0.0% wa — IO等待占用CPU的百分比0.0% hi — 硬中断（Hardware IRQ）占用CPU的百分比0.0% si — 软中断（Software Interrupts）占用CPU的百分比 第四行 Mem 物理内存total — 物理内存总量 8GBfree — 空闲的内存used — 正在使用的内存buff/cache — 缓存的内存(used表示现在系统内核控制的内存数，free表示还未进入内核控制的内存数，used还包括了停止使用但可能被重用的内存，所以used使用完的内存不会返还给free，所以free的内存数一定越来越少) 第五行 Swap 交换区total — 交换区总量used — 使用的交换区总量free — 空闲交换区总量cached — 缓冲的交换区总量(swap used经常变化的话说明内存已经不够使用了)(系统可使用的内存近似为第四行的free+buff/cache+第五行的cache)读者可以参考 参考链接了解 进程状态监控各项指标 PID — 进程idUSER — 进程所有者PR — 进程优先级NI — nice值，负值表示高优先级，正值表示低优先级VIRT — 进程使用的虚拟内存总量，单位kb，VIRT=SWAP+RESRES — 进程使用的、未被换出的物理内存大小，单位kb，RES=CODE+DATASHR — 共享内存大小，单位kbS — 进程状态，D=不可中断的睡眠状态 R=运行 S=睡眠 T=跟踪/停止 Z=僵尸进程%CPU — 上次更新到现在的CPU时间占用百分比%MEM — 进程使用的物理内存百分比TIME+ — 进程使用的CPU时间总计，单位1/100秒COMMAND — 进程名称（命令名/命令行） top交互执行top命令之后即进入top信息展示界面，可以使用命令行进行交互 top界面 按1显示多核cpu的使用情况 top界面默认按cpu的使用降序排序 使用shift+&gt;和shift+&lt;来改变排序的指标 top界面 按x高亮排序的指标 top其他使用技巧 h 显示帮助画面，给出一些简短的命令总结说明 k 终止一个进程 i 忽略闲置和僵死进程。这是一个开关式命令 q 退出程序 r 重新安排一个进程的优先级别 S 切换到累计模式 s 改变两次刷新之间的延迟时间（单位为s），如果有小数，就换算成ms。输入0值则系统将不断刷新，默认值是5s f或者F 从当前显示中添加或者删除项目 o或者O 改变显示项目的顺序 l 切换显示平均负载和启动时间信息 m 切换显示内存信息 t 切换显示进程和CPU状态信息 c 切换显示命令名称和完整命令行 M 根据驻留内存大小进行排序 P 根据CPU使用百分比大小进行排序 T 根据时间/累计时间进行排序 W 将当前设置写入~/.toprc文件中 说明在top命令界面中，几乎不使用交互命令，在排查问题时，一般紧盯cpu使用高的线程即可。]]></content>
      <categories>
        <category>微服务问题定位</category>
      </categories>
      <tags>
        <tag>linux命令</tag>
        <tag>微服务</tag>
        <tag>进程,线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F01%2F24%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>宣告全世界</category>
      </categories>
      <tags>
        <tag>welcome</tag>
        <tag>hello</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梁希森林公园]]></title>
    <url>%2F2017%2F04%2F04%2F%E6%A2%81%E5%B8%8C%E6%A3%AE%E6%9E%97%E5%85%AC%E5%9B%AD%2F</url>
    <content type="text"><![CDATA[写博客以来，笔者一直用github作为图床，但当图片的大小超过一定值打开github链接就会直接下载，无法作为url链接，所以将图片暂放微博。 我与梁希先生 人生得意须尽欢 流水潺潺 狗年行大运]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
        <tag>自然环境</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[跆拳道]]></title>
    <url>%2F2016%2F06%2F13%2F%E8%B7%86%E6%8B%B3%E9%81%93%2F</url>
    <content type="text"><![CDATA[跆拳道 人不疯狂枉少年]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
        <tag>校园</tag>
      </tags>
  </entry>
</search>
